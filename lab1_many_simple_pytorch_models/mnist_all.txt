iteration 0: loss 2.3803021907806396
iteration 1: loss 2.2754368782043457
iteration 2: loss 2.181302070617676
iteration 3: loss 2.095625162124634
iteration 4: loss 2.017213821411133
iteration 5: loss 1.9453171491622925
iteration 6: loss 1.8793586492538452
iteration 7: loss 1.8188310861587524
iteration 8: loss 1.7632665634155273
iteration 9: loss 1.712231159210205
iteration 10: loss 1.6653202772140503
iteration 11: loss 1.6221601963043213
iteration 12: loss 1.5824095010757446
iteration 13: loss 1.5457537174224854
iteration 14: loss 1.5119075775146484
iteration 15: loss 1.4806137084960938
iteration 16: loss 1.4516372680664062
iteration 17: loss 1.424767255783081
iteration 18: loss 1.399812936782837
iteration 19: loss 1.376604437828064
iteration 20: loss 1.3549864292144775
iteration 21: loss 1.3348205089569092
iteration 22: loss 1.3159819841384888
iteration 23: loss 1.2983582019805908
iteration 24: loss 1.2818481922149658
iteration 25: loss 1.2663604021072388
iteration 26: loss 1.25181245803833
iteration 27: loss 1.2381298542022705
iteration 28: loss 1.2252451181411743
iteration 29: loss 1.2130974531173706
iteration 30: loss 1.2016311883926392
iteration 31: loss 1.190796136856079
iteration 32: loss 1.1805462837219238
iteration 33: loss 1.1708400249481201
iteration 34: loss 1.16163969039917
iteration 35: loss 1.1529096364974976
iteration 36: loss 1.1446189880371094
iteration 37: loss 1.1367379426956177
iteration 38: loss 1.1292401552200317
iteration 39: loss 1.1221013069152832
iteration 40: loss 1.1152981519699097
iteration 41: loss 1.1088099479675293
iteration 42: loss 1.102617621421814
iteration 43: loss 1.0967037677764893
iteration 44: loss 1.0910515785217285
iteration 45: loss 1.085646152496338
iteration 46: loss 1.0804728269577026
iteration 47: loss 1.0755195617675781
iteration 48: loss 1.0707733631134033
iteration 49: loss 1.0662235021591187
iteration 50: loss 1.061859369277954
iteration 51: loss 1.0576703548431396
iteration 52: loss 1.0536483526229858
iteration 53: loss 1.0497846603393555
iteration 54: loss 1.0460706949234009
iteration 55: loss 1.0424994230270386
iteration 56: loss 1.0390640497207642
iteration 57: loss 1.0357577800750732
iteration 58: loss 1.0325744152069092
iteration 59: loss 1.0295078754425049
iteration 60: loss 1.0265529155731201
iteration 61: loss 1.023705005645752
iteration 62: loss 1.0209587812423706
iteration 63: loss 1.018309473991394
iteration 64: loss 1.0157535076141357
iteration 65: loss 1.0132865905761719
iteration 66: loss 1.0109045505523682
iteration 67: loss 1.0086034536361694
iteration 68: loss 1.0063810348510742
iteration 69: loss 1.0042330026626587
iteration 70: loss 1.0021573305130005
iteration 71: loss 1.0001497268676758
iteration 72: loss 0.9982089996337891
iteration 73: loss 0.9963314533233643
iteration 74: loss 0.9945144653320312
iteration 75: loss 0.9927568435668945
iteration 76: loss 0.9910553693771362
iteration 77: loss 0.9894081950187683
iteration 78: loss 0.9878131151199341
iteration 79: loss 0.9862684011459351
iteration 80: loss 0.9847722053527832
iteration 81: loss 0.983322262763977
iteration 82: loss 0.9819178581237793
iteration 83: loss 0.9805572032928467
iteration 84: loss 0.9792379140853882
iteration 85: loss 0.9779590964317322
iteration 86: loss 0.9767192602157593
iteration 87: loss 0.9755176305770874
iteration 88: loss 0.9743521213531494
iteration 89: loss 0.9732214212417603
iteration 90: loss 0.9721254110336304
iteration 91: loss 0.9710615277290344
iteration 92: loss 0.9700301885604858
iteration 93: loss 0.9690296649932861
iteration 94: loss 0.9680591821670532
iteration 95: loss 0.9671173095703125
iteration 96: loss 0.9662034511566162
iteration 97: loss 0.9653162956237793
iteration 98: loss 0.9644560217857361
iteration 99: loss 0.9636205434799194
iteration 100: loss 0.9628103971481323
iteration 101: loss 0.9620240330696106
iteration 102: loss 0.9612604379653931
iteration 103: loss 0.9605199098587036
iteration 104: loss 0.9598004817962646
iteration 105: loss 0.9591031074523926
iteration 106: loss 0.9584257006645203
iteration 107: loss 0.9577686786651611
iteration 108: loss 0.9571309089660645
iteration 109: loss 0.9565122127532959
iteration 110: loss 0.9559116363525391
iteration 111: loss 0.9553289413452148
iteration 112: loss 0.9547640085220337
iteration 113: loss 0.9542151689529419
iteration 114: loss 0.9536832571029663
iteration 115: loss 0.9531673789024353
iteration 116: loss 0.9526668787002563
iteration 117: loss 0.9521812200546265
iteration 118: loss 0.9517104625701904
iteration 119: loss 0.9512544274330139
iteration 120: loss 0.9508119225502014
iteration 121: loss 0.9503828287124634
iteration 122: loss 0.9499672651290894
iteration 123: loss 0.9495643377304077
iteration 124: loss 0.9491739869117737
iteration 125: loss 0.9487959146499634
iteration 126: loss 0.9484299421310425
iteration 127: loss 0.9480752944946289
iteration 128: loss 0.9477314949035645
iteration 129: loss 0.9473999738693237
iteration 130: loss 0.9470783472061157
iteration 131: loss 0.9467673301696777
iteration 132: loss 0.9464667439460754
iteration 133: loss 0.9461760520935059
iteration 134: loss 0.9458950757980347
iteration 135: loss 0.9456238746643066
iteration 136: loss 0.9453617334365845
iteration 137: loss 0.9451087713241577
iteration 138: loss 0.9448643922805786
iteration 139: loss 0.9446287155151367
iteration 140: loss 0.9444018602371216
iteration 141: loss 0.9441834092140198
iteration 142: loss 0.9439725875854492
iteration 143: loss 0.9437698125839233
iteration 144: loss 0.9435751438140869
iteration 145: loss 0.9433867931365967
iteration 146: loss 0.9432064294815063
iteration 147: loss 0.9430333971977234
iteration 148: loss 0.9428668022155762
iteration 149: loss 0.9427073001861572
iteration 150: loss 0.9425551295280457
iteration 151: loss 0.9424088597297668
iteration 152: loss 0.9422695636749268
iteration 153: loss 0.9421364665031433
iteration 154: loss 0.9420090913772583
iteration 155: loss 0.941887378692627
iteration 156: loss 0.9417728185653687
iteration 157: loss 0.9416630268096924
iteration 158: loss 0.9415590763092041
iteration 159: loss 0.9414604306221008
iteration 160: loss 0.941367506980896
iteration 161: loss 0.9412798881530762
iteration 162: loss 0.9411970376968384
iteration 163: loss 0.9411194324493408
iteration 164: loss 0.9410462379455566
iteration 165: loss 0.9409782886505127
iteration 166: loss 0.9409160614013672
iteration 167: loss 0.9408570528030396
iteration 168: loss 0.9408026933670044
iteration 169: loss 0.9407541155815125
iteration 170: loss 0.9407082200050354
iteration 171: loss 0.9406678676605225
iteration 172: loss 0.9406310319900513
iteration 173: loss 0.940598726272583
iteration 174: loss 0.9405703544616699
iteration 175: loss 0.9405460953712463
iteration 176: loss 0.9405248165130615
iteration 177: loss 0.9405078887939453
iteration 178: loss 0.9404951333999634
iteration 179: loss 0.9404851198196411
iteration 180: loss 0.9404792785644531
iteration 181: loss 0.9404765963554382
iteration 182: loss 0.9404774904251099
iteration 183: loss 0.9404815435409546
iteration 184: loss 0.9404892325401306
iteration 185: loss 0.9404999017715454
iteration 186: loss 0.9405138492584229
iteration 187: loss 0.9405313730239868
iteration 188: loss 0.9405511021614075
iteration 189: loss 0.9405739307403564
iteration 190: loss 0.9405999183654785
iteration 191: loss 0.9406285285949707
iteration 192: loss 0.9406603574752808
iteration 193: loss 0.94069504737854
iteration 194: loss 0.9407318830490112
iteration 195: loss 0.940771222114563
iteration 196: loss 0.9408135414123535
iteration 197: loss 0.9408584237098694
iteration 198: loss 0.9409059882164001
iteration 199: loss 0.9409563541412354
iteration 200: loss 0.9410090446472168
iteration 201: loss 0.9410631656646729
iteration 202: loss 0.9411208033561707
iteration 203: loss 0.9411796927452087
iteration 204: loss 0.9412417411804199
iteration 205: loss 0.9413050413131714
iteration 206: loss 0.9413709044456482
iteration 207: loss 0.9414390325546265
iteration 208: loss 0.9415088891983032
iteration 209: loss 0.9415808916091919
iteration 210: loss 0.9416555166244507
iteration 211: loss 0.9417317509651184
iteration 212: loss 0.9418097734451294
iteration 213: loss 0.9418895244598389
iteration 214: loss 0.941970944404602
iteration 215: loss 0.9420547485351562
iteration 216: loss 0.9421403408050537
iteration 217: loss 0.9422275424003601
iteration 218: loss 0.942316472530365
iteration 219: loss 0.9424071311950684
iteration 220: loss 0.9424992799758911
iteration 221: loss 0.942593514919281
iteration 222: loss 0.9426888823509216
iteration 223: loss 0.942786455154419
iteration 224: loss 0.9428855180740356
iteration 225: loss 0.9429858922958374
iteration 226: loss 0.9430872201919556
iteration 227: loss 0.9431901574134827
iteration 228: loss 0.9432950615882874
iteration 229: loss 0.9434007406234741
iteration 230: loss 0.9435094594955444
iteration 231: loss 0.9436172842979431
iteration 232: loss 0.9437275528907776
iteration 233: loss 0.9438393712043762
iteration 234: loss 0.943952202796936
iteration 235: loss 0.9440668821334839
iteration 236: loss 0.9441820383071899
iteration 237: loss 0.9442983865737915
iteration 238: loss 0.9444167613983154
iteration 239: loss 0.9445359706878662
iteration 240: loss 0.9446557760238647
iteration 241: loss 0.9447770118713379
iteration 242: loss 0.944899320602417
iteration 243: loss 0.9450235366821289
iteration 244: loss 0.9451481103897095
iteration 245: loss 0.9452741146087646
iteration 246: loss 0.9454009532928467
iteration 247: loss 0.945528507232666
iteration 248: loss 0.9456573128700256
iteration 249: loss 0.9457875490188599
iteration 250: loss 0.9459182620048523
iteration 251: loss 0.9460504055023193
iteration 252: loss 0.9461824893951416
iteration 253: loss 0.9463165998458862
iteration 254: loss 0.9464507102966309
iteration 255: loss 0.9465862512588501
iteration 256: loss 0.9467235803604126
iteration 257: loss 0.9468605518341064
iteration 258: loss 0.9469982385635376
iteration 259: loss 0.9471378326416016
iteration 260: loss 0.9472774267196655
iteration 261: loss 0.9474178552627563
iteration 262: loss 0.9475594758987427
iteration 263: loss 0.9477017521858215
iteration 264: loss 0.9478447437286377
iteration 265: loss 0.9479881525039673
iteration 266: loss 0.9481323957443237
iteration 267: loss 0.9482778310775757
iteration 268: loss 0.9484236240386963
iteration 269: loss 0.9485700130462646
iteration 270: loss 0.9487178325653076
iteration 271: loss 0.948865532875061
iteration 272: loss 0.9490139484405518
iteration 273: loss 0.9491639137268066
iteration 274: loss 0.9493141174316406
iteration 275: loss 0.9494640827178955
iteration 276: loss 0.9496157169342041
iteration 277: loss 0.9497675895690918
iteration 278: loss 0.9499202966690063
iteration 279: loss 0.9500733017921448
iteration 280: loss 0.9502267837524414
iteration 281: loss 0.950380802154541
iteration 282: loss 0.950535237789154
iteration 283: loss 0.9506908059120178
iteration 284: loss 0.9508468508720398
iteration 285: loss 0.9510030746459961
iteration 286: loss 0.9511598348617554
iteration 287: loss 0.951317548751831
iteration 288: loss 0.9514756202697754
iteration 289: loss 0.9516339302062988
iteration 290: loss 0.9517930746078491
iteration 291: loss 0.9519517421722412
iteration 292: loss 0.9521117210388184
iteration 293: loss 0.9522722959518433
iteration 294: loss 0.9524323344230652
iteration 295: loss 0.9525939226150513
iteration 296: loss 0.9527548551559448
iteration 297: loss 0.9529174566268921
iteration 298: loss 0.953079879283905
iteration 299: loss 0.9532425403594971
iteration 300: loss 0.9534064531326294
iteration 301: loss 0.9535697102546692
iteration 302: loss 0.9537338018417358
iteration 303: loss 0.9538983106613159
iteration 304: loss 0.954062819480896
iteration 305: loss 0.9542281031608582
iteration 306: loss 0.9543938636779785
iteration 307: loss 0.9545599818229675
iteration 308: loss 0.9547262191772461
iteration 309: loss 0.9548929333686829
iteration 310: loss 0.9550598859786987
iteration 311: loss 0.9552267789840698
iteration 312: loss 0.9553946256637573
iteration 313: loss 0.9555624127388
iteration 314: loss 0.9557305574417114
iteration 315: loss 0.9558989405632019
iteration 316: loss 0.9560679197311401
iteration 317: loss 0.9562371969223022
iteration 318: loss 0.9564067125320435
iteration 319: loss 0.9565762281417847
iteration 320: loss 0.9567457437515259
iteration 321: loss 0.9569158554077148
iteration 322: loss 0.9570873975753784
iteration 323: loss 0.9572570323944092
iteration 324: loss 0.9574285745620728
iteration 325: loss 0.9575998783111572
iteration 326: loss 0.9577714800834656
iteration 327: loss 0.9579427242279053
iteration 328: loss 0.958114743232727
iteration 329: loss 0.9582866430282593
iteration 330: loss 0.9584595561027527
iteration 331: loss 0.9586325883865356
iteration 332: loss 0.9588055610656738
iteration 333: loss 0.958978533744812
iteration 334: loss 0.9591517448425293
iteration 335: loss 0.9593245983123779
iteration 336: loss 0.9594985842704773
iteration 337: loss 0.9596725702285767
iteration 338: loss 0.9598461389541626
iteration 339: loss 0.9600200653076172
iteration 340: loss 0.9601947069168091
iteration 341: loss 0.9603692293167114
iteration 342: loss 0.9605446457862854
iteration 343: loss 0.9607197046279907
iteration 344: loss 0.9608949422836304
iteration 345: loss 0.9610695838928223
iteration 346: loss 0.9612453579902649
iteration 347: loss 0.9614205360412598
iteration 348: loss 0.9615966081619263
iteration 349: loss 0.9617719650268555
iteration 350: loss 0.9619481563568115
iteration 351: loss 0.9621243476867676
iteration 352: loss 0.9623007774353027
iteration 353: loss 0.9624767303466797
iteration 354: loss 0.9626538753509521
iteration 355: loss 0.9628303050994873
iteration 356: loss 0.9630075097084045
iteration 357: loss 0.9631844162940979
iteration 358: loss 0.9633610248565674
iteration 359: loss 0.9635380506515503
iteration 360: loss 0.9637159109115601
iteration 361: loss 0.9638926982879639
iteration 362: loss 0.9640700817108154
iteration 363: loss 0.9642477035522461
iteration 364: loss 0.9644256830215454
iteration 365: loss 0.9646037817001343
iteration 366: loss 0.9647811651229858
iteration 367: loss 0.9649587869644165
iteration 368: loss 0.9651370048522949
iteration 369: loss 0.9653154015541077
iteration 370: loss 0.9654932022094727
iteration 371: loss 0.965671718120575
iteration 372: loss 0.9658500552177429
iteration 373: loss 0.9660282135009766
iteration 374: loss 0.9662073850631714
iteration 375: loss 0.9663856029510498
iteration 376: loss 0.9665641784667969
iteration 377: loss 0.9667432308197021
iteration 378: loss 0.9669216275215149
iteration 379: loss 0.9671007394790649
iteration 380: loss 0.967279314994812
iteration 381: loss 0.9674581289291382
iteration 382: loss 0.9676380753517151
iteration 383: loss 0.9678167700767517
iteration 384: loss 0.9679959416389465
iteration 385: loss 0.9681751728057861
iteration 386: loss 0.9683542251586914
iteration 387: loss 0.9685335159301758
iteration 388: loss 0.968712329864502
iteration 389: loss 0.9688917398452759
iteration 390: loss 0.9690713286399841
iteration 391: loss 0.9692506790161133
iteration 392: loss 0.9694293141365051
iteration 393: loss 0.9696093797683716
iteration 394: loss 0.9697887301445007
iteration 395: loss 0.969968318939209
iteration 396: loss 0.9701476097106934
iteration 397: loss 0.9703273773193359
iteration 398: loss 0.9705066680908203
iteration 399: loss 0.9706863164901733
iteration 400: loss 0.9708651304244995
iteration 401: loss 0.9710454940795898
iteration 402: loss 0.9712249040603638
iteration 403: loss 0.9714045524597168
iteration 404: loss 0.9715843796730042
iteration 405: loss 0.971764087677002
iteration 406: loss 0.971943736076355
iteration 407: loss 0.9721235036849976
iteration 408: loss 0.9723027944564819
iteration 409: loss 0.972482442855835
iteration 410: loss 0.9726618528366089
iteration 411: loss 0.9728416204452515
iteration 412: loss 0.9730207920074463
iteration 413: loss 0.9732006192207336
iteration 414: loss 0.973380446434021
iteration 415: loss 0.9735601544380188
iteration 416: loss 0.973739743232727
iteration 417: loss 0.973919153213501
iteration 418: loss 0.9740989208221436
iteration 419: loss 0.9742785692214966
iteration 420: loss 0.9744581580162048
iteration 421: loss 0.9746372103691101
iteration 422: loss 0.9748178124427795
iteration 423: loss 0.9749971032142639
iteration 424: loss 0.9751758575439453
iteration 425: loss 0.9753557443618774
iteration 426: loss 0.9755356311798096
iteration 427: loss 0.9757148027420044
iteration 428: loss 0.9758937954902649
iteration 429: loss 0.9760736227035522
iteration 430: loss 0.9762532711029053
iteration 431: loss 0.9764319658279419
iteration 432: loss 0.9766113758087158
iteration 433: loss 0.9767903685569763
iteration 434: loss 0.9769700169563293
iteration 435: loss 0.9771495461463928
iteration 436: loss 0.9773285388946533
iteration 437: loss 0.9775071144104004
iteration 438: loss 0.9776866436004639
iteration 439: loss 0.9778655171394348
iteration 440: loss 0.9780445098876953
iteration 441: loss 0.9782238006591797
iteration 442: loss 0.9784031510353088
iteration 443: loss 0.9785813689231873
iteration 444: loss 0.9787603616714478
iteration 445: loss 0.9789389371871948
iteration 446: loss 0.9791182279586792
iteration 447: loss 0.9792969226837158
iteration 448: loss 0.9794752597808838
iteration 449: loss 0.9796539545059204
iteration 450: loss 0.9798325896263123
iteration 451: loss 0.9800112843513489
iteration 452: loss 0.980189323425293
iteration 453: loss 0.9803686141967773
iteration 454: loss 0.980546236038208
iteration 455: loss 0.9807246923446655
iteration 456: loss 0.9809033870697021
iteration 457: loss 0.9810816645622253
iteration 458: loss 0.9812595844268799
iteration 459: loss 0.9814372062683105
iteration 460: loss 0.9816159009933472
iteration 461: loss 0.9817933440208435
iteration 462: loss 0.9819718599319458
iteration 463: loss 0.9821488857269287
iteration 464: loss 0.9823274612426758
iteration 465: loss 0.9825050234794617
iteration 466: loss 0.9826827049255371
iteration 467: loss 0.9828606843948364
iteration 468: loss 0.9830386638641357
iteration 469: loss 0.9832159280776978
iteration 470: loss 0.9833927750587463
iteration 471: loss 0.9835708141326904
iteration 472: loss 0.9837484359741211
iteration 473: loss 0.9839258790016174
iteration 474: loss 0.984103262424469
iteration 475: loss 0.9842798113822937
iteration 476: loss 0.9844568967819214
iteration 477: loss 0.9846343994140625
iteration 478: loss 0.9848111867904663
iteration 479: loss 0.9849882125854492
iteration 480: loss 0.9851651787757874
iteration 481: loss 0.9853414297103882
iteration 482: loss 0.9855182766914368
iteration 483: loss 0.9856954216957092
iteration 484: loss 0.9858716726303101
iteration 485: loss 0.9860483407974243
iteration 486: loss 0.9862242937088013
iteration 487: loss 0.9864010810852051
iteration 488: loss 0.9865776300430298
iteration 489: loss 0.9867538809776306
iteration 490: loss 0.9869296550750732
iteration 491: loss 0.9871061444282532
iteration 492: loss 0.987281858921051
iteration 493: loss 0.9874575138092041
iteration 494: loss 0.9876331090927124
iteration 495: loss 0.9878089427947998
iteration 496: loss 0.9879845380783081
iteration 497: loss 0.9881603717803955
iteration 498: loss 0.9883357882499695
iteration 499: loss 0.9885115027427673
Accuracy:  0.9007 
Recall:  [(0.9785714285714285, 0.9283639883833494), (0.9718061674008811, 0.9500430663221361), (0.8585271317829457, 0.9124613800205973), (0.8891089108910891, 0.8829891838741396), (0.9175152749490835, 0.8876847290640394), (0.8195067264573991, 0.8849878934624698), (0.9331941544885177, 0.9226006191950464), (0.892023346303502, 0.9133466135458167), (0.8593429158110883, 0.8480243161094225), (0.8731417244796829, 0.8662733529990168)] 
Matrix:
 [[ 959    0    2    3    0    2    7    1    6    0]
 [   0 1103    2    4    1    2    4    0   19    0]
 [  12    7  886   20   16    0   18   21   43    9]
 [   6    1   18  898    1   30    6   15   23   12]
 [   1    6    5    1  901    1   11    1    8   47]
 [  14    6    5   44   16  731   17   11   37   11]
 [  16    3    6    3   14   17  894    1    4    0]
 [   3   19   30    4   11    0    0  917    4   40]
 [  10    8   11   31    8   27   12   13  837   17]
 [  12    8    6    9   47   16    0   24    6  881]]
Architecture: [784, 10]
Experiment done!

iteration 0: loss 2.3164422512054443
iteration 1: loss 2.2086470127105713
iteration 2: loss 2.1112747192382812
iteration 3: loss 2.0216169357299805
iteration 4: loss 1.9385968446731567
iteration 5: loss 1.861620545387268
iteration 6: loss 1.7902458906173706
iteration 7: loss 1.7240796089172363
iteration 8: loss 1.6627485752105713
iteration 9: loss 1.6058909893035889
iteration 10: loss 1.5531610250473022
iteration 11: loss 1.5042279958724976
iteration 12: loss 1.4587808847427368
iteration 13: loss 1.4165295362472534
iteration 14: loss 1.3772027492523193
iteration 15: loss 1.3405520915985107
iteration 16: loss 1.3063489198684692
iteration 17: loss 1.2743847370147705
iteration 18: loss 1.244469404220581
iteration 19: loss 1.2164303064346313
iteration 20: loss 1.190110206604004
iteration 21: loss 1.165367841720581
iteration 22: loss 1.142073631286621
iteration 23: loss 1.1201117038726807
iteration 24: loss 1.0993763208389282
iteration 25: loss 1.0797721147537231
iteration 26: loss 1.0612118244171143
iteration 27: loss 1.0436170101165771
iteration 28: loss 1.026916265487671
iteration 29: loss 1.0110446214675903
iteration 30: loss 0.9959423542022705
iteration 31: loss 0.9815560579299927
iteration 32: loss 0.9678361415863037
iteration 33: loss 0.9547377824783325
iteration 34: loss 0.9422197341918945
iteration 35: loss 0.9302440285682678
iteration 36: loss 0.9187763333320618
iteration 37: loss 0.9077844023704529
iteration 38: loss 0.8972392082214355
iteration 39: loss 0.8871135115623474
iteration 40: loss 0.8773826360702515
iteration 41: loss 0.8680235743522644
iteration 42: loss 0.8590146899223328
iteration 43: loss 0.8503366708755493
iteration 44: loss 0.8419710397720337
iteration 45: loss 0.8339007496833801
iteration 46: loss 0.8261102437973022
iteration 47: loss 0.8185846209526062
iteration 48: loss 0.8113101720809937
iteration 49: loss 0.8042742609977722
iteration 50: loss 0.7974649667739868
iteration 51: loss 0.7908712029457092
iteration 52: loss 0.784482479095459
iteration 53: loss 0.7782889604568481
iteration 54: loss 0.77228182554245
iteration 55: loss 0.7664520144462585
iteration 56: loss 0.7607915997505188
iteration 57: loss 0.7552934288978577
iteration 58: loss 0.7499496340751648
iteration 59: loss 0.7447540760040283
iteration 60: loss 0.7397002577781677
iteration 61: loss 0.7347823977470398
iteration 62: loss 0.7299942970275879
iteration 63: loss 0.7253313660621643
iteration 64: loss 0.720787763595581
iteration 65: loss 0.7163594365119934
iteration 66: loss 0.7120412588119507
iteration 67: loss 0.7078296542167664
iteration 68: loss 0.7037197947502136
iteration 69: loss 0.6997083425521851
iteration 70: loss 0.6957916021347046
iteration 71: loss 0.691965639591217
iteration 72: loss 0.6882277727127075
iteration 73: loss 0.6845746040344238
iteration 74: loss 0.6810030341148376
iteration 75: loss 0.6775102615356445
iteration 76: loss 0.67409348487854
iteration 77: loss 0.6707504391670227
iteration 78: loss 0.6674782037734985
iteration 79: loss 0.6642745733261108
iteration 80: loss 0.6611374616622925
iteration 81: loss 0.6580646634101868
iteration 82: loss 0.6550539135932922
iteration 83: loss 0.6521033644676208
iteration 84: loss 0.6492112278938293
iteration 85: loss 0.6463755369186401
iteration 86: loss 0.64359450340271
iteration 87: loss 0.6408665776252747
iteration 88: loss 0.6381901502609253
iteration 89: loss 0.6355637311935425
iteration 90: loss 0.6329855918884277
iteration 91: loss 0.6304546594619751
iteration 92: loss 0.6279695630073547
iteration 93: loss 0.6255289912223816
iteration 94: loss 0.6231312155723572
iteration 95: loss 0.6207754611968994
iteration 96: loss 0.6184607148170471
iteration 97: loss 0.6161854267120361
iteration 98: loss 0.6139487624168396
iteration 99: loss 0.6117497086524963
iteration 100: loss 0.6095874309539795
iteration 101: loss 0.6074603199958801
iteration 102: loss 0.6053681373596191
iteration 103: loss 0.6033095717430115
iteration 104: loss 0.6012840867042542
iteration 105: loss 0.5992905497550964
iteration 106: loss 0.5973281860351562
iteration 107: loss 0.5953965783119202
iteration 108: loss 0.5934944152832031
iteration 109: loss 0.591621458530426
iteration 110: loss 0.5897765755653381
iteration 111: loss 0.5879595279693604
iteration 112: loss 0.5861693620681763
iteration 113: loss 0.5844053626060486
iteration 114: loss 0.582667350769043
iteration 115: loss 0.580954372882843
iteration 116: loss 0.5792657136917114
iteration 117: loss 0.5776011943817139
iteration 118: loss 0.5759599804878235
iteration 119: loss 0.574341893196106
iteration 120: loss 0.5727459788322449
iteration 121: loss 0.5711719393730164
iteration 122: loss 0.5696194767951965
iteration 123: loss 0.5680878162384033
iteration 124: loss 0.5665766596794128
iteration 125: loss 0.565085768699646
iteration 126: loss 0.5636141300201416
iteration 127: loss 0.5621620416641235
iteration 128: loss 0.5607286691665649
iteration 129: loss 0.5593135952949524
iteration 130: loss 0.5579167008399963
iteration 131: loss 0.5565373301506042
iteration 132: loss 0.5551755428314209
iteration 133: loss 0.5538305640220642
iteration 134: loss 0.5525020956993103
iteration 135: loss 0.5511900186538696
iteration 136: loss 0.549893856048584
iteration 137: loss 0.5486134886741638
iteration 138: loss 0.5473483800888062
iteration 139: loss 0.5460984110832214
iteration 140: loss 0.544863224029541
iteration 141: loss 0.5436424016952515
iteration 142: loss 0.5424359440803528
iteration 143: loss 0.541243314743042
iteration 144: loss 0.5400645136833191
iteration 145: loss 0.5388989448547363
iteration 146: loss 0.537746787071228
iteration 147: loss 0.536607563495636
iteration 148: loss 0.5354808568954468
iteration 149: loss 0.5343667268753052
iteration 150: loss 0.5332649946212769
iteration 151: loss 0.5321752429008484
iteration 152: loss 0.5310972929000854
iteration 153: loss 0.5300309062004089
iteration 154: loss 0.5289760231971741
iteration 155: loss 0.5279324054718018
iteration 156: loss 0.5268997550010681
iteration 157: loss 0.5258779525756836
iteration 158: loss 0.5248668193817139
iteration 159: loss 0.5238661766052246
iteration 160: loss 0.5228758454322815
iteration 161: loss 0.521895706653595
iteration 162: loss 0.5209254622459412
iteration 163: loss 0.5199649333953857
iteration 164: loss 0.5190142393112183
iteration 165: loss 0.5180728435516357
iteration 166: loss 0.5171408653259277
iteration 167: loss 0.5162181258201599
iteration 168: loss 0.5153043270111084
iteration 169: loss 0.514399528503418
iteration 170: loss 0.51350337266922
iteration 171: loss 0.5126158595085144
iteration 172: loss 0.5117368102073669
iteration 173: loss 0.5108661651611328
iteration 174: loss 0.5100036263465881
iteration 175: loss 0.5091493129730225
iteration 176: loss 0.5083029866218567
iteration 177: loss 0.5074644684791565
iteration 178: loss 0.5066336393356323
iteration 179: loss 0.5058103799819946
iteration 180: loss 0.5049947500228882
iteration 181: loss 0.5041864514350891
iteration 182: loss 0.5033854842185974
iteration 183: loss 0.5025916695594788
iteration 184: loss 0.5018048882484436
iteration 185: loss 0.5010250210762024
iteration 186: loss 0.5002520680427551
iteration 187: loss 0.4994858503341675
iteration 188: loss 0.4987264573574066
iteration 189: loss 0.4979735314846039
iteration 190: loss 0.49722713232040405
iteration 191: loss 0.4964871406555176
iteration 192: loss 0.49575331807136536
iteration 193: loss 0.4950259029865265
iteration 194: loss 0.49430447816848755
iteration 195: loss 0.4935891032218933
iteration 196: loss 0.49287986755371094
iteration 197: loss 0.4921763241291046
iteration 198: loss 0.49147865176200867
iteration 199: loss 0.4907866418361664
iteration 200: loss 0.4901004433631897
iteration 201: loss 0.4894196689128876
iteration 202: loss 0.4887444078922272
iteration 203: loss 0.4880746603012085
iteration 204: loss 0.48741015791893005
iteration 205: loss 0.4867510199546814
iteration 206: loss 0.48609721660614014
iteration 207: loss 0.48544830083847046
iteration 208: loss 0.484804630279541
iteration 209: loss 0.4841660261154175
iteration 210: loss 0.48353222012519836
iteration 211: loss 0.48290354013442993
iteration 212: loss 0.48227939009666443
iteration 213: loss 0.4816601574420929
iteration 214: loss 0.4810456335544586
iteration 215: loss 0.4804357886314392
iteration 216: loss 0.47983047366142273
iteration 217: loss 0.4792298376560211
iteration 218: loss 0.4786335825920105
iteration 219: loss 0.47804170846939087
iteration 220: loss 0.47745421528816223
iteration 221: loss 0.4768710732460022
iteration 222: loss 0.47629234194755554
iteration 223: loss 0.47571760416030884
iteration 224: loss 0.4751470685005188
iteration 225: loss 0.4745807349681854
iteration 226: loss 0.474018394947052
iteration 227: loss 0.4734601378440857
iteration 228: loss 0.47290587425231934
iteration 229: loss 0.47235551476478577
iteration 230: loss 0.47180894017219543
iteration 231: loss 0.47126632928848267
iteration 232: loss 0.47072750329971313
iteration 233: loss 0.47019246220588684
iteration 234: loss 0.4696609675884247
iteration 235: loss 0.469133198261261
iteration 236: loss 0.46860912442207336
iteration 237: loss 0.4680885672569275
iteration 238: loss 0.46757152676582336
iteration 239: loss 0.46705812215805054
iteration 240: loss 0.46654802560806274
iteration 241: loss 0.46604135632514954
iteration 242: loss 0.4655381441116333
iteration 243: loss 0.46503838896751404
iteration 244: loss 0.46454179286956787
iteration 245: loss 0.4640485644340515
iteration 246: loss 0.4635583460330963
iteration 247: loss 0.4630715847015381
iteration 248: loss 0.4625878930091858
iteration 249: loss 0.462107390165329
iteration 250: loss 0.4616299271583557
iteration 251: loss 0.46115565299987793
iteration 252: loss 0.46068426966667175
iteration 253: loss 0.4602159857749939
iteration 254: loss 0.4597506523132324
iteration 255: loss 0.4592881500720978
iteration 256: loss 0.45882880687713623
iteration 257: loss 0.45837220549583435
iteration 258: loss 0.4579184651374817
iteration 259: loss 0.45746758580207825
iteration 260: loss 0.4570193886756897
iteration 261: loss 0.45657405257225037
iteration 262: loss 0.4561314880847931
iteration 263: loss 0.45569145679473877
iteration 264: loss 0.45525434613227844
iteration 265: loss 0.45481976866722107
iteration 266: loss 0.4543877840042114
iteration 267: loss 0.4539584815502167
iteration 268: loss 0.4535316824913025
iteration 269: loss 0.4531075656414032
iteration 270: loss 0.4526858627796173
iteration 271: loss 0.4522666931152344
iteration 272: loss 0.45184996724128723
iteration 273: loss 0.4514356553554535
iteration 274: loss 0.45102399587631226
iteration 275: loss 0.4506145417690277
iteration 276: loss 0.45020750164985657
iteration 277: loss 0.4498027563095093
iteration 278: loss 0.44940051436424255
iteration 279: loss 0.4490004777908325
iteration 280: loss 0.4486026465892792
iteration 281: loss 0.4482072591781616
iteration 282: loss 0.44781410694122314
iteration 283: loss 0.4474230706691742
iteration 284: loss 0.44703441858291626
iteration 285: loss 0.4466477930545807
iteration 286: loss 0.4462633430957794
iteration 287: loss 0.4458811581134796
iteration 288: loss 0.4455009698867798
iteration 289: loss 0.44512292742729187
iteration 290: loss 0.44474712014198303
iteration 291: loss 0.44437310099601746
iteration 292: loss 0.44400128722190857
iteration 293: loss 0.4436315596103668
iteration 294: loss 0.44326379895210266
iteration 295: loss 0.44289806485176086
iteration 296: loss 0.44253411889076233
iteration 297: loss 0.44217240810394287
iteration 298: loss 0.4418124854564667
iteration 299: loss 0.4414544403553009
iteration 300: loss 0.4410984516143799
iteration 301: loss 0.44074416160583496
iteration 302: loss 0.4403919279575348
iteration 303: loss 0.4400414824485779
iteration 304: loss 0.4396928548812866
iteration 305: loss 0.4393460750579834
iteration 306: loss 0.4390011429786682
iteration 307: loss 0.4386579990386963
iteration 308: loss 0.43831658363342285
iteration 309: loss 0.4379769563674927
iteration 310: loss 0.4376390278339386
iteration 311: loss 0.43730291724205017
iteration 312: loss 0.43696853518486023
iteration 313: loss 0.43663573265075684
iteration 314: loss 0.4363047480583191
iteration 315: loss 0.4359753727912903
iteration 316: loss 0.4356476366519928
iteration 317: loss 0.435321569442749
iteration 318: loss 0.43499717116355896
iteration 319: loss 0.43467438220977783
iteration 320: loss 0.4343531131744385
iteration 321: loss 0.4340336322784424
iteration 322: loss 0.4337155520915985
iteration 323: loss 0.4333990216255188
iteration 324: loss 0.43308424949645996
iteration 325: loss 0.43277084827423096
iteration 326: loss 0.4324590861797333
iteration 327: loss 0.4321487545967102
iteration 328: loss 0.4318399131298065
iteration 329: loss 0.431532621383667
iteration 330: loss 0.43122681975364685
iteration 331: loss 0.4309224486351013
iteration 332: loss 0.4306195378303528
iteration 333: loss 0.43031811714172363
iteration 334: loss 0.4300180971622467
iteration 335: loss 0.429719477891922
iteration 336: loss 0.4294222891330719
iteration 337: loss 0.429126501083374
iteration 338: loss 0.42883214354515076
iteration 339: loss 0.428538978099823
iteration 340: loss 0.42824748158454895
iteration 341: loss 0.42795705795288086
iteration 342: loss 0.42766815423965454
iteration 343: loss 0.4273805618286133
iteration 344: loss 0.4270942509174347
iteration 345: loss 0.4268093407154083
iteration 346: loss 0.42652562260627747
iteration 347: loss 0.42624321579933167
iteration 348: loss 0.4259622395038605
iteration 349: loss 0.42568239569664
iteration 350: loss 0.42540377378463745
iteration 351: loss 0.4251265227794647
iteration 352: loss 0.42485058307647705
iteration 353: loss 0.42457571625709534
iteration 354: loss 0.4243021607398987
iteration 355: loss 0.4240298271179199
iteration 356: loss 0.4237585663795471
iteration 357: loss 0.4234887659549713
iteration 358: loss 0.4232199788093567
iteration 359: loss 0.42295247316360474
iteration 360: loss 0.42268604040145874
iteration 361: loss 0.4224208891391754
iteration 362: loss 0.42215681076049805
iteration 363: loss 0.4218938946723938
iteration 364: loss 0.4216320812702179
iteration 365: loss 0.4213714599609375
iteration 366: loss 0.42111194133758545
iteration 367: loss 0.4208536148071289
iteration 368: loss 0.42059633135795593
iteration 369: loss 0.4203401505947113
iteration 370: loss 0.420085072517395
iteration 371: loss 0.4198310971260071
iteration 372: loss 0.4195781648159027
iteration 373: loss 0.4193262755870819
iteration 374: loss 0.41907548904418945
iteration 375: loss 0.41882574558258057
iteration 376: loss 0.4185771346092224
iteration 377: loss 0.4183294475078583
iteration 378: loss 0.4180828332901001
iteration 379: loss 0.4178371727466583
iteration 380: loss 0.4175926744937897
iteration 381: loss 0.4173491299152374
iteration 382: loss 0.4171065390110016
iteration 383: loss 0.4168650209903717
iteration 384: loss 0.4166245460510254
iteration 385: loss 0.41638487577438354
iteration 386: loss 0.41614633798599243
iteration 387: loss 0.4159086048603058
iteration 388: loss 0.4156720042228699
iteration 389: loss 0.415436327457428
iteration 390: loss 0.4152015447616577
iteration 391: loss 0.41496771574020386
iteration 392: loss 0.41473495960235596
iteration 393: loss 0.41450297832489014
iteration 394: loss 0.4142720103263855
iteration 395: loss 0.4140417277812958
iteration 396: loss 0.41381263732910156
iteration 397: loss 0.4135844111442566
iteration 398: loss 0.4133570194244385
iteration 399: loss 0.41313058137893677
iteration 400: loss 0.4129050672054291
iteration 401: loss 0.41268038749694824
iteration 402: loss 0.4124564528465271
iteration 403: loss 0.41223350167274475
iteration 404: loss 0.41201141476631165
iteration 405: loss 0.41179022192955017
iteration 406: loss 0.41156989336013794
iteration 407: loss 0.4113503098487854
iteration 408: loss 0.41113168001174927
iteration 409: loss 0.4109139144420624
iteration 410: loss 0.410696804523468
iteration 411: loss 0.41048070788383484
iteration 412: loss 0.41026532649993896
iteration 413: loss 0.41005080938339233
iteration 414: loss 0.40983715653419495
iteration 415: loss 0.4096241593360901
iteration 416: loss 0.40941208600997925
iteration 417: loss 0.4092007875442505
iteration 418: loss 0.40899020433425903
iteration 419: loss 0.40878042578697205
iteration 420: loss 0.4085715711116791
iteration 421: loss 0.408363401889801
iteration 422: loss 0.40815600752830505
iteration 423: loss 0.4079493582248688
iteration 424: loss 0.40774351358413696
iteration 425: loss 0.40753844380378723
iteration 426: loss 0.4073340892791748
iteration 427: loss 0.40713053941726685
iteration 428: loss 0.40692761540412903
iteration 429: loss 0.40672552585601807
iteration 430: loss 0.4065242111682892
iteration 431: loss 0.40632352232933044
iteration 432: loss 0.406123548746109
iteration 433: loss 0.40592437982559204
iteration 434: loss 0.4057259261608124
iteration 435: loss 0.40552806854248047
iteration 436: loss 0.4053310453891754
iteration 437: loss 0.4051346778869629
iteration 438: loss 0.4049389660358429
iteration 439: loss 0.40474408864974976
iteration 440: loss 0.404549777507782
iteration 441: loss 0.4043561816215515
iteration 442: loss 0.40416333079338074
iteration 443: loss 0.4039711356163025
iteration 444: loss 0.4037795960903168
iteration 445: loss 0.40358883142471313
iteration 446: loss 0.4033985733985901
iteration 447: loss 0.40320900082588196
iteration 448: loss 0.40302011370658875
iteration 449: loss 0.4028319716453552
iteration 450: loss 0.40264439582824707
iteration 451: loss 0.40245747566223145
iteration 452: loss 0.4022711515426636
iteration 453: loss 0.4020855724811554
iteration 454: loss 0.40190061926841736
iteration 455: loss 0.4017162621021271
iteration 456: loss 0.40153253078460693
iteration 457: loss 0.40134939551353455
iteration 458: loss 0.4011668860912323
iteration 459: loss 0.4009850025177002
iteration 460: loss 0.4008038640022278
iteration 461: loss 0.40062323212623596
iteration 462: loss 0.4004431962966919
iteration 463: loss 0.4002637565135956
iteration 464: loss 0.4000849723815918
iteration 465: loss 0.39990678429603577
iteration 466: loss 0.3997291326522827
iteration 467: loss 0.3995521664619446
iteration 468: loss 0.3993757367134094
iteration 469: loss 0.39919978380203247
iteration 470: loss 0.3990245461463928
iteration 471: loss 0.39884987473487854
iteration 472: loss 0.39867573976516724
iteration 473: loss 0.39850226044654846
iteration 474: loss 0.39832934737205505
iteration 475: loss 0.3981568515300751
iteration 476: loss 0.3979850709438324
iteration 477: loss 0.39781373739242554
iteration 478: loss 0.3976430296897888
iteration 479: loss 0.39747288823127747
iteration 480: loss 0.39730319380760193
iteration 481: loss 0.39713412523269653
iteration 482: loss 0.3969656825065613
iteration 483: loss 0.39679762721061707
iteration 484: loss 0.3966302275657654
iteration 485: loss 0.3964633047580719
iteration 486: loss 0.3962969481945038
iteration 487: loss 0.39613112807273865
iteration 488: loss 0.3959657549858093
iteration 489: loss 0.39580100774765015
iteration 490: loss 0.3956367075443268
iteration 491: loss 0.3954729735851288
iteration 492: loss 0.3953096866607666
iteration 493: loss 0.39514702558517456
iteration 494: loss 0.3949848711490631
iteration 495: loss 0.3948231041431427
iteration 496: loss 0.39466190338134766
iteration 497: loss 0.3945012390613556
iteration 498: loss 0.39434105157852173
iteration 499: loss 0.3941813111305237
Accuracy:  0.9009 
Recall:  [(0.976530612244898, 0.9282250242483027), (0.9709251101321585, 0.9475494411006019), (0.8604651162790697, 0.9117043121149897), (0.8891089108910891, 0.8838582677165354), (0.9185336048879837, 0.8939544103072349), (0.820627802690583, 0.8851269649334945), (0.9363256784968684, 0.9218910585817061), (0.8910505836575876, 0.9132602193419741), (0.8583162217659137, 0.8470111448834853), (0.8731417244796829, 0.8662733529990168)] 
Matrix:
 [[ 957    0    3    3    0    2    8    1    6    0]
 [   0 1102    2    4    1    2    4    0   20    0]
 [  11    7  888   19   15    0   18   21   44    9]
 [   6    1   18  898    1   31    6   15   22   12]
 [   2    6    5    1  902    1   10    1    8   46]
 [  15    6    5   44   15  732   17    9   37   12]
 [  16    3    6    2   12   17  897    1    4    0]
 [   3   20   30    4   11    0    0  916    4   40]
 [   9   10   11   31    7   26   13   14  836   17]
 [  12    8    6   10   45   16    0   25    6  881]]
Architecture: [784, 10]
Experiment done!

iteration 0: loss 2.2991812229156494
iteration 1: loss 2.193664789199829
iteration 2: loss 2.0976433753967285
iteration 3: loss 2.0090458393096924
iteration 4: loss 1.9269624948501587
iteration 5: loss 1.8508530855178833
iteration 6: loss 1.7802913188934326
iteration 7: loss 1.714888095855713
iteration 8: loss 1.6542695760726929
iteration 9: loss 1.598075270652771
iteration 10: loss 1.5459591150283813
iteration 11: loss 1.4975919723510742
iteration 12: loss 1.452665090560913
iteration 13: loss 1.4108901023864746
iteration 14: loss 1.3719987869262695
iteration 15: loss 1.3357460498809814
iteration 16: loss 1.3019064664840698
iteration 17: loss 1.2702734470367432
iteration 18: loss 1.240660309791565
iteration 19: loss 1.2128970623016357
iteration 20: loss 1.186828851699829
iteration 21: loss 1.1623165607452393
iteration 22: loss 1.139233112335205
iteration 23: loss 1.1174640655517578
iteration 24: loss 1.0969053506851196
iteration 25: loss 1.0774632692337036
iteration 26: loss 1.0590518712997437
iteration 27: loss 1.04159414768219
iteration 28: loss 1.0250194072723389
iteration 29: loss 1.0092638731002808
iteration 30: loss 0.994269073009491
iteration 31: loss 0.9799818992614746
iteration 32: loss 0.9663536548614502
iteration 33: loss 0.9533400535583496
iteration 34: loss 0.9409006237983704
iteration 35: loss 0.9289979338645935
iteration 36: loss 0.9175977110862732
iteration 37: loss 0.9066686034202576
iteration 38: loss 0.8961818814277649
iteration 39: loss 0.8861106038093567
iteration 40: loss 0.876430332660675
iteration 41: loss 0.8671184778213501
iteration 42: loss 0.858153760433197
iteration 43: loss 0.8495169281959534
iteration 44: loss 0.8411895632743835
iteration 45: loss 0.8331552147865295
iteration 46: loss 0.8253982067108154
iteration 47: loss 0.8179039359092712
iteration 48: loss 0.8106591105461121
iteration 49: loss 0.8036506772041321
iteration 50: loss 0.7968671917915344
iteration 51: loss 0.7902973890304565
iteration 52: loss 0.7839314341545105
iteration 53: loss 0.7777592539787292
iteration 54: loss 0.7717720866203308
iteration 55: loss 0.7659610509872437
iteration 56: loss 0.7603182196617126
iteration 57: loss 0.7548365592956543
iteration 58: loss 0.7495084404945374
iteration 59: loss 0.7443275451660156
iteration 60: loss 0.7392874956130981
iteration 61: loss 0.7343823313713074
iteration 62: loss 0.7296066284179688
iteration 63: loss 0.7249549031257629
iteration 64: loss 0.7204222679138184
iteration 65: loss 0.7160038352012634
iteration 66: loss 0.7116956114768982
iteration 67: loss 0.7074926495552063
iteration 68: loss 0.7033913135528564
iteration 69: loss 0.6993878483772278
iteration 70: loss 0.6954784393310547
iteration 71: loss 0.691659688949585
iteration 72: loss 0.6879285573959351
iteration 73: loss 0.6842812895774841
iteration 74: loss 0.6807156801223755
iteration 75: loss 0.6772284507751465
iteration 76: loss 0.6738168597221375
iteration 77: loss 0.6704784631729126
iteration 78: loss 0.6672108769416809
iteration 79: loss 0.6640116572380066
iteration 80: loss 0.660878598690033
iteration 81: loss 0.6578094959259033
iteration 82: loss 0.6548022627830505
iteration 83: loss 0.651854932308197
iteration 84: loss 0.6489658355712891
iteration 85: loss 0.6461330056190491
iteration 86: loss 0.643354594707489
iteration 87: loss 0.6406293511390686
iteration 88: loss 0.6379552483558655
iteration 89: loss 0.6353310346603394
iteration 90: loss 0.6327549815177917
iteration 91: loss 0.6302259564399719
iteration 92: loss 0.62774258852005
iteration 93: loss 0.6253033876419067
iteration 94: loss 0.6229072213172913
iteration 95: loss 0.6205530166625977
iteration 96: loss 0.6182392239570618
iteration 97: loss 0.6159651875495911
iteration 98: loss 0.6137297749519348
iteration 99: loss 0.6115316152572632
iteration 100: loss 0.6093698740005493
iteration 101: loss 0.6072436571121216
iteration 102: loss 0.6051521897315979
iteration 103: loss 0.6030942797660828
iteration 104: loss 0.6010692119598389
iteration 105: loss 0.5990762114524841
iteration 106: loss 0.5971143841743469
iteration 107: loss 0.5951828956604004
iteration 108: loss 0.5932811498641968
iteration 109: loss 0.5914080739021301
iteration 110: loss 0.5895636677742004
iteration 111: loss 0.5877465009689331
iteration 112: loss 0.585956335067749
iteration 113: loss 0.5841925144195557
iteration 114: loss 0.5824543833732605
iteration 115: loss 0.5807412266731262
iteration 116: loss 0.5790523886680603
iteration 117: loss 0.5773876309394836
iteration 118: loss 0.5757462978363037
iteration 119: loss 0.574127733707428
iteration 120: loss 0.572531521320343
iteration 121: loss 0.570957362651825
iteration 122: loss 0.5694042444229126
iteration 123: loss 0.5678724050521851
iteration 124: loss 0.5663609504699707
iteration 125: loss 0.5648692846298218
iteration 126: loss 0.5633973479270935
iteration 127: loss 0.5619446039199829
iteration 128: loss 0.5605107545852661
iteration 129: loss 0.5590950846672058
iteration 130: loss 0.5576977133750916
iteration 131: loss 0.5563177466392517
iteration 132: loss 0.5549551248550415
iteration 133: loss 0.5536096692085266
iteration 134: loss 0.552280604839325
iteration 135: loss 0.5509677529335022
iteration 136: loss 0.5496711134910583
iteration 137: loss 0.5483897924423218
iteration 138: loss 0.5471241474151611
iteration 139: loss 0.5458733439445496
iteration 140: loss 0.5446373820304871
iteration 141: loss 0.5434158444404602
iteration 142: loss 0.5422085523605347
iteration 143: loss 0.5410152077674866
iteration 144: loss 0.5398354530334473
iteration 145: loss 0.538669228553772
iteration 146: loss 0.537516176700592
iteration 147: loss 0.5363759994506836
iteration 148: loss 0.5352486371994019
iteration 149: loss 0.5341336727142334
iteration 150: loss 0.5330309867858887
iteration 151: loss 0.5319403409957886
iteration 152: loss 0.5308615565299988
iteration 153: loss 0.5297943353652954
iteration 154: loss 0.5287385582923889
iteration 155: loss 0.527694046497345
iteration 156: loss 0.5266604423522949
iteration 157: loss 0.5256378054618835
iteration 158: loss 0.5246257781982422
iteration 159: loss 0.5236241221427917
iteration 160: loss 0.5226329565048218
iteration 161: loss 0.5216517448425293
iteration 162: loss 0.5206806063652039
iteration 163: loss 0.5197191834449768
iteration 164: loss 0.5187675356864929
iteration 165: loss 0.5178252458572388
iteration 166: loss 0.5168923139572144
iteration 167: loss 0.5159686207771301
iteration 168: loss 0.5150538086891174
iteration 169: loss 0.5141480565071106
iteration 170: loss 0.5132509469985962
iteration 171: loss 0.512362539768219
iteration 172: loss 0.5114825963973999
iteration 173: loss 0.5106108784675598
iteration 174: loss 0.5097474455833435
iteration 175: loss 0.5088921785354614
iteration 176: loss 0.5080447793006897
iteration 177: loss 0.5072051882743835
iteration 178: loss 0.506373405456543
iteration 179: loss 0.5055492520332336
iteration 180: loss 0.5047326683998108
iteration 181: loss 0.5039234161376953
iteration 182: loss 0.5031214356422424
iteration 183: loss 0.5023266077041626
iteration 184: loss 0.501538872718811
iteration 185: loss 0.5007581114768982
iteration 186: loss 0.49998408555984497
iteration 187: loss 0.4992169439792633
iteration 188: loss 0.4984564781188965
iteration 189: loss 0.49770262837409973
iteration 190: loss 0.49695509672164917
iteration 191: loss 0.49621421098709106
iteration 192: loss 0.4954794943332672
iteration 193: loss 0.4947509765625
iteration 194: loss 0.4940285384654999
iteration 195: loss 0.4933122992515564
iteration 196: loss 0.4926019012928009
iteration 197: loss 0.4918975830078125
iteration 198: loss 0.49119895696640015
iteration 199: loss 0.4905059337615967
iteration 200: loss 0.4898187220096588
iteration 201: loss 0.4891369640827179
iteration 202: loss 0.4884607493877411
iteration 203: loss 0.487790048122406
iteration 204: loss 0.48712459206581116
iteration 205: loss 0.4864644706249237
iteration 206: loss 0.4858095049858093
iteration 207: loss 0.4851597249507904
iteration 208: loss 0.4845150411128998
iteration 209: loss 0.48387545347213745
iteration 210: loss 0.48324069380760193
iteration 211: loss 0.48261094093322754
iteration 212: loss 0.48198598623275757
iteration 213: loss 0.48136577010154724
iteration 214: loss 0.48075032234191895
iteration 215: loss 0.4801393747329712
iteration 216: loss 0.47953319549560547
iteration 217: loss 0.4789314270019531
iteration 218: loss 0.47833430767059326
iteration 219: loss 0.47774139046669006
iteration 220: loss 0.47715306282043457
iteration 221: loss 0.4765690267086029
iteration 222: loss 0.47598913311958313
iteration 223: loss 0.4754135310649872
iteration 224: loss 0.4748421311378479
iteration 225: loss 0.47427478432655334
iteration 226: loss 0.4737115204334259
iteration 227: loss 0.47315239906311035
iteration 228: loss 0.4725969433784485
iteration 229: loss 0.4720456600189209
iteration 230: loss 0.4714982509613037
iteration 231: loss 0.47095462679862976
iteration 232: loss 0.4704149067401886
iteration 233: loss 0.46987879276275635
iteration 234: loss 0.46934643387794495
iteration 235: loss 0.46881774067878723
iteration 236: loss 0.46829262375831604
iteration 237: loss 0.4677712321281433
iteration 238: loss 0.4672531485557556
iteration 239: loss 0.46673884987831116
iteration 240: loss 0.46622785925865173
iteration 241: loss 0.4657202959060669
iteration 242: loss 0.46521613001823425
iteration 243: loss 0.4647151529788971
iteration 244: loss 0.46421778202056885
iteration 245: loss 0.4637235701084137
iteration 246: loss 0.4632326364517212
iteration 247: loss 0.46274489164352417
iteration 248: loss 0.4622602164745331
iteration 249: loss 0.46177875995635986
iteration 250: loss 0.46130040287971497
iteration 251: loss 0.46082526445388794
iteration 252: loss 0.46035292744636536
iteration 253: loss 0.4598837196826935
iteration 254: loss 0.4594173729419708
iteration 255: loss 0.4589541554450989
iteration 256: loss 0.45849376916885376
iteration 257: loss 0.458036333322525
iteration 258: loss 0.45758160948753357
iteration 259: loss 0.4571298062801361
iteration 260: loss 0.45668086409568787
iteration 261: loss 0.45623448491096497
iteration 262: loss 0.4557909369468689
iteration 263: loss 0.45535025000572205
iteration 264: loss 0.4549121558666229
iteration 265: loss 0.45447665452957153
iteration 266: loss 0.45404374599456787
iteration 267: loss 0.45361360907554626
iteration 268: loss 0.45318588614463806
iteration 269: loss 0.45276087522506714
iteration 270: loss 0.4523382782936096
iteration 271: loss 0.45191821455955505
iteration 272: loss 0.45150062441825867
iteration 273: loss 0.4510854482650757
iteration 274: loss 0.45067283511161804
iteration 275: loss 0.4502624273300171
iteration 276: loss 0.44985467195510864
iteration 277: loss 0.4494491219520569
iteration 278: loss 0.44904592633247375
iteration 279: loss 0.4486449956893921
iteration 280: loss 0.44824638962745667
iteration 281: loss 0.4478500485420227
iteration 282: loss 0.4474559724330902
iteration 283: loss 0.44706419110298157
iteration 284: loss 0.44667449593544006
iteration 285: loss 0.4462870657444
iteration 286: loss 0.4459018409252167
iteration 287: loss 0.4455186724662781
iteration 288: loss 0.44513770937919617
iteration 289: loss 0.4447587728500366
iteration 290: loss 0.4443819522857666
iteration 291: loss 0.44400733709335327
iteration 292: loss 0.443634569644928
iteration 293: loss 0.443263977766037
iteration 294: loss 0.44289544224739075
iteration 295: loss 0.4425288140773773
iteration 296: loss 0.4421640932559967
iteration 297: loss 0.44180136919021606
iteration 298: loss 0.441440612077713
iteration 299: loss 0.44108185172080994
iteration 300: loss 0.4407249391078949
iteration 301: loss 0.4403699040412903
iteration 302: loss 0.4400167465209961
iteration 303: loss 0.4396655559539795
iteration 304: loss 0.4393159747123718
iteration 305: loss 0.4389685094356537
iteration 306: loss 0.4386226236820221
iteration 307: loss 0.4382786750793457
iteration 308: loss 0.437936395406723
iteration 309: loss 0.4375959634780884
iteration 310: loss 0.4372572898864746
iteration 311: loss 0.43692031502723694
iteration 312: loss 0.43658509850502014
iteration 313: loss 0.43625155091285706
iteration 314: loss 0.43591970205307007
iteration 315: loss 0.4355895221233368
iteration 316: loss 0.43526095151901245
iteration 317: loss 0.4349341094493866
iteration 318: loss 0.43460893630981445
iteration 319: loss 0.4342852830886841
iteration 320: loss 0.4339633285999298
iteration 321: loss 0.4336428940296173
iteration 322: loss 0.43332406878471375
iteration 323: loss 0.43300682306289673
iteration 324: loss 0.4326910972595215
iteration 325: loss 0.432376891374588
iteration 326: loss 0.4320642948150635
iteration 327: loss 0.4317532479763031
iteration 328: loss 0.4314436912536621
iteration 329: loss 0.43113550543785095
iteration 330: loss 0.4308289587497711
iteration 331: loss 0.43052375316619873
iteration 332: loss 0.4302200376987457
iteration 333: loss 0.4299178421497345
iteration 334: loss 0.4296170175075531
iteration 335: loss 0.42931756377220154
iteration 336: loss 0.4290197193622589
iteration 337: loss 0.42872318625450134
iteration 338: loss 0.4284278452396393
iteration 339: loss 0.42813411355018616
iteration 340: loss 0.4278416037559509
iteration 341: loss 0.4275505840778351
iteration 342: loss 0.4272608757019043
iteration 343: loss 0.42697250843048096
iteration 344: loss 0.4266854524612427
iteration 345: loss 0.42639970779418945
iteration 346: loss 0.4261152744293213
iteration 347: loss 0.4258320927619934
iteration 348: loss 0.425550252199173
iteration 349: loss 0.4252697229385376
iteration 350: loss 0.4249904155731201
iteration 351: loss 0.4247123897075653
iteration 352: loss 0.4244355857372284
iteration 353: loss 0.42416006326675415
iteration 354: loss 0.42388567328453064
iteration 355: loss 0.4236125648021698
iteration 356: loss 0.42334073781967163
iteration 357: loss 0.4230699837207794
iteration 358: loss 0.4228005111217499
iteration 359: loss 0.42253223061561584
iteration 360: loss 0.4222651720046997
iteration 361: loss 0.42199909687042236
iteration 362: loss 0.42173436284065247
iteration 363: loss 0.4214707016944885
iteration 364: loss 0.42120814323425293
iteration 365: loss 0.42094680666923523
iteration 366: loss 0.42068660259246826
iteration 367: loss 0.420427531003952
iteration 368: loss 0.4201693534851074
iteration 369: loss 0.41991254687309265
iteration 370: loss 0.41965675354003906
iteration 371: loss 0.41940200328826904
iteration 372: loss 0.419148325920105
iteration 373: loss 0.41889578104019165
iteration 374: loss 0.4186442792415619
iteration 375: loss 0.4183938205242157
iteration 376: loss 0.4181443750858307
iteration 377: loss 0.417896032333374
iteration 378: loss 0.41764870285987854
iteration 379: loss 0.4174024164676666
iteration 380: loss 0.4171570837497711
iteration 381: loss 0.4169127643108368
iteration 382: loss 0.41666945815086365
iteration 383: loss 0.4164271950721741
iteration 384: loss 0.4161858856678009
iteration 385: loss 0.4159456789493561
iteration 386: loss 0.4157063066959381
iteration 387: loss 0.41546809673309326
iteration 388: loss 0.4152306914329529
iteration 389: loss 0.4149942696094513
iteration 390: loss 0.4147587716579437
iteration 391: loss 0.4145243465900421
iteration 392: loss 0.41429078578948975
iteration 393: loss 0.4140580892562866
iteration 394: loss 0.41382643580436707
iteration 395: loss 0.4135955572128296
iteration 396: loss 0.41336575150489807
iteration 397: loss 0.4131367802619934
iteration 398: loss 0.41290873289108276
iteration 399: loss 0.41268154978752136
iteration 400: loss 0.4124552011489868
iteration 401: loss 0.41222986578941345
iteration 402: loss 0.41200533509254456
iteration 403: loss 0.41178175806999207
iteration 404: loss 0.41155898571014404
iteration 405: loss 0.4113369882106781
iteration 406: loss 0.41111594438552856
iteration 407: loss 0.41089582443237305
iteration 408: loss 0.4106764495372772
iteration 409: loss 0.4104579985141754
iteration 410: loss 0.4102402925491333
iteration 411: loss 0.4100235104560852
iteration 412: loss 0.409807413816452
iteration 413: loss 0.4095921814441681
iteration 414: loss 0.409377783536911
iteration 415: loss 0.4091641902923584
iteration 416: loss 0.4089514911174774
iteration 417: loss 0.4087393879890442
iteration 418: loss 0.40852832794189453
iteration 419: loss 0.4083179235458374
iteration 420: loss 0.40810832381248474
iteration 421: loss 0.4078994393348694
iteration 422: loss 0.4076913595199585
iteration 423: loss 0.40748411417007446
iteration 424: loss 0.4072776734828949
iteration 425: loss 0.40707194805145264
iteration 426: loss 0.4068668782711029
iteration 427: loss 0.4066625237464905
iteration 428: loss 0.4064590632915497
iteration 429: loss 0.4062563180923462
iteration 430: loss 0.4060543477535248
iteration 431: loss 0.40585291385650635
iteration 432: loss 0.40565240383148193
iteration 433: loss 0.40545251965522766
iteration 434: loss 0.4052533805370331
iteration 435: loss 0.4050549566745758
iteration 436: loss 0.4048573076725006
iteration 437: loss 0.40466025471687317
iteration 438: loss 0.4044640064239502
iteration 439: loss 0.404268354177475
iteration 440: loss 0.4040733873844147
iteration 441: loss 0.4038792550563812
iteration 442: loss 0.40368565917015076
iteration 443: loss 0.40349283814430237
iteration 444: loss 0.4033006429672241
iteration 445: loss 0.40310919284820557
iteration 446: loss 0.40291842818260193
iteration 447: loss 0.40272825956344604
iteration 448: loss 0.4025387167930603
iteration 449: loss 0.40234991908073425
iteration 450: loss 0.4021616578102112
iteration 451: loss 0.4019741415977478
iteration 452: loss 0.4017872214317322
iteration 453: loss 0.4016009569168091
iteration 454: loss 0.4014154374599457
iteration 455: loss 0.40123042464256287
iteration 456: loss 0.4010459780693054
iteration 457: loss 0.4008622467517853
iteration 458: loss 0.40067917108535767
iteration 459: loss 0.4004967212677002
iteration 460: loss 0.4003148376941681
iteration 461: loss 0.4001336097717285
iteration 462: loss 0.3999529778957367
iteration 463: loss 0.3997730016708374
iteration 464: loss 0.3995935618877411
iteration 465: loss 0.3994147479534149
iteration 466: loss 0.39923644065856934
iteration 467: loss 0.39905884861946106
iteration 468: loss 0.39888182282447815
iteration 469: loss 0.398705393075943
iteration 470: loss 0.3985294997692108
iteration 471: loss 0.3983542323112488
iteration 472: loss 0.3981795012950897
iteration 473: loss 0.39800533652305603
iteration 474: loss 0.3978317379951477
iteration 475: loss 0.3976587951183319
iteration 476: loss 0.3974863290786743
iteration 477: loss 0.3973144590854645
iteration 478: loss 0.39714309573173523
iteration 479: loss 0.39697232842445374
iteration 480: loss 0.39680215716362
iteration 481: loss 0.39663249254226685
iteration 482: loss 0.3964633047580719
iteration 483: loss 0.3962947130203247
iteration 484: loss 0.39612674713134766
iteration 485: loss 0.3959592580795288
iteration 486: loss 0.3957922160625458
iteration 487: loss 0.3956257700920105
iteration 488: loss 0.39545994997024536
iteration 489: loss 0.39529451727867126
iteration 490: loss 0.3951296806335449
iteration 491: loss 0.39496541023254395
iteration 492: loss 0.3948015570640564
iteration 493: loss 0.39463821053504944
iteration 494: loss 0.39447546005249023
iteration 495: loss 0.3943132162094116
iteration 496: loss 0.39415135979652405
iteration 497: loss 0.3939902186393738
iteration 498: loss 0.39382943511009216
iteration 499: loss 0.39366909861564636
Accuracy:  0.9011 
Recall:  [(0.976530612244898, 0.9282250242483027), (0.9718061674008811, 0.947594501718213), (0.8594961240310077, 0.9125514403292181), (0.8910891089108911, 0.8858267716535433), (0.9185336048879837, 0.8904244817374136), (0.8217488789237668, 0.8863361547762999), (0.9342379958246346, 0.9198355601233299), (0.8910505836575876, 0.9123505976095617), (0.8624229979466119, 0.8493427704752275), (0.8701684836471755, 0.8684470820969338)] 
Matrix:
 [[ 957    0    2    3    0    2    9    1    6    0]
 [   0 1103    2    4    1    2    4    0   19    0]
 [  12    7  887   20   16    0   18   22   41    9]
 [   5    1   18  900    1   30    5   15   23   12]
 [   1    6    5    0  902    1   12    1    8   46]
 [  15    6    5   43   15  733   17   10   37   11]
 [  16    3    7    2   13   17  895    1    4    0]
 [   3   20   30    4   11    0    0  916    4   40]
 [   9   10   10   30    7   26   13   14  840   15]
 [  13    8    6   10   47   16    0   24    7  878]]
Architecture: [784, 10]
Experiment done!



 ######################################################################
Regularization factor: 0.1	 accuracy: 0.9007
Regularization factor: 0.0001	 accuracy: 0.9009
Regularization factor: 1e-08	 accuracy: 0.9011
iteration 0: loss 2.3048291206359863
iteration 1: loss 1.5693745613098145
iteration 2: loss 1.2373178005218506
iteration 3: loss 1.1251020431518555
iteration 4: loss 1.037394404411316
iteration 5: loss 1.161381721496582
iteration 6: loss 1.0182645320892334
iteration 7: loss 0.9779934883117676
iteration 8: loss 1.1162992715835571
iteration 9: loss 0.8424331545829773
iteration 10: loss 0.7842918634414673
iteration 11: loss 0.7976739406585693
iteration 12: loss 0.8051261901855469
iteration 13: loss 0.7509527206420898
iteration 14: loss 0.7258331775665283
iteration 15: loss 0.6701230406761169
iteration 16: loss 0.6507209539413452
iteration 17: loss 0.6111202836036682
iteration 18: loss 0.5878304243087769
iteration 19: loss 0.5604418516159058
iteration 20: loss 0.5397894382476807
iteration 21: loss 0.5215401649475098
iteration 22: loss 0.5059307217597961
iteration 23: loss 0.4938954710960388
iteration 24: loss 0.48324066400527954
iteration 25: loss 0.4751613736152649
iteration 26: loss 0.4680342972278595
iteration 27: loss 0.4623478949069977
iteration 28: loss 0.45731937885284424
iteration 29: loss 0.4530228078365326
iteration 30: loss 0.4491436779499054
iteration 31: loss 0.4456409513950348
iteration 32: loss 0.44239434599876404
iteration 33: loss 0.43936190009117126
iteration 34: loss 0.43649640679359436
iteration 35: loss 0.4337729513645172
iteration 36: loss 0.43117138743400574
iteration 37: loss 0.4286782741546631
iteration 38: loss 0.42628368735313416
iteration 39: loss 0.4239794909954071
iteration 40: loss 0.4217592775821686
iteration 41: loss 0.41961780190467834
iteration 42: loss 0.41755008697509766
iteration 43: loss 0.41555193066596985
iteration 44: loss 0.4136194884777069
iteration 45: loss 0.4117490351200104
iteration 46: loss 0.40993741154670715
iteration 47: loss 0.4081815183162689
iteration 48: loss 0.40647849440574646
iteration 49: loss 0.4048258364200592
iteration 50: loss 0.4032209515571594
iteration 51: loss 0.40166160464286804
iteration 52: loss 0.40014588832855225
iteration 53: loss 0.398671418428421
iteration 54: loss 0.39723655581474304
iteration 55: loss 0.39583954215049744
iteration 56: loss 0.3944786489009857
iteration 57: loss 0.39315247535705566
iteration 58: loss 0.3918594717979431
iteration 59: loss 0.3905981779098511
iteration 60: loss 0.38936755061149597
iteration 61: loss 0.38816606998443604
iteration 62: loss 0.38699275255203247
iteration 63: loss 0.3858465850353241
iteration 64: loss 0.384726345539093
iteration 65: loss 0.3836310803890228
iteration 66: loss 0.3825600743293762
iteration 67: loss 0.38151219487190247
iteration 68: loss 0.38048672676086426
iteration 69: loss 0.37948283553123474
iteration 70: loss 0.3784998059272766
iteration 71: loss 0.3775370121002197
iteration 72: loss 0.37659353017807007
iteration 73: loss 0.3756689429283142
iteration 74: loss 0.3747623860836029
iteration 75: loss 0.37387359142303467
iteration 76: loss 0.3730016350746155
iteration 77: loss 0.372146338224411
iteration 78: loss 0.371306836605072
iteration 79: loss 0.3704829216003418
iteration 80: loss 0.3696737587451935
iteration 81: loss 0.3688793480396271
iteration 82: loss 0.3680989444255829
iteration 83: loss 0.36733224987983704
iteration 84: loss 0.3665788769721985
iteration 85: loss 0.3658382296562195
iteration 86: loss 0.3651103079319
iteration 87: loss 0.3643944263458252
iteration 88: loss 0.3636905252933502
iteration 89: loss 0.36299797892570496
iteration 90: loss 0.362316757440567
iteration 91: loss 0.36164626479148865
iteration 92: loss 0.36098647117614746
iteration 93: loss 0.3603370487689972
iteration 94: loss 0.3596976101398468
iteration 95: loss 0.35906803607940674
iteration 96: loss 0.3584479093551636
iteration 97: loss 0.35783711075782776
iteration 98: loss 0.35723549127578735
iteration 99: loss 0.35664263367652893
iteration 100: loss 0.35605841875076294
iteration 101: loss 0.3554825186729431
iteration 102: loss 0.3549148738384247
iteration 103: loss 0.3543553352355957
iteration 104: loss 0.3538036048412323
iteration 105: loss 0.35325950384140015
iteration 106: loss 0.3527228534221649
iteration 107: loss 0.35219353437423706
iteration 108: loss 0.3516712784767151
iteration 109: loss 0.3511560559272766
iteration 110: loss 0.35064759850502014
iteration 111: loss 0.3501458168029785
iteration 112: loss 0.3496505618095398
iteration 113: loss 0.3491617441177368
iteration 114: loss 0.3486790657043457
iteration 115: loss 0.3482026159763336
iteration 116: loss 0.34773197770118713
iteration 117: loss 0.34726735949516296
iteration 118: loss 0.3468083143234253
iteration 119: loss 0.34635499119758606
iteration 120: loss 0.3459070920944214
iteration 121: loss 0.3454645872116089
iteration 122: loss 0.3450274169445038
iteration 123: loss 0.34459537267684937
iteration 124: loss 0.34416842460632324
iteration 125: loss 0.3437463641166687
iteration 126: loss 0.34332922101020813
iteration 127: loss 0.34291693568229675
iteration 128: loss 0.34250926971435547
iteration 129: loss 0.3421061336994171
iteration 130: loss 0.34170761704444885
iteration 131: loss 0.34131351113319397
iteration 132: loss 0.3409236669540405
iteration 133: loss 0.3405381143093109
iteration 134: loss 0.34015682339668274
iteration 135: loss 0.3397795259952545
iteration 136: loss 0.3394063115119934
iteration 137: loss 0.33903709053993225
iteration 138: loss 0.3386717438697815
iteration 139: loss 0.338310182094574
iteration 140: loss 0.3379523754119873
iteration 141: loss 0.3375982940196991
iteration 142: loss 0.3372478485107422
iteration 143: loss 0.3369009494781494
iteration 144: loss 0.336557537317276
iteration 145: loss 0.33621758222579956
iteration 146: loss 0.3358810544013977
iteration 147: loss 0.33554771542549133
iteration 148: loss 0.3352178931236267
iteration 149: loss 0.33489108085632324
iteration 150: loss 0.33456742763519287
iteration 151: loss 0.33424702286720276
iteration 152: loss 0.3339296281337738
iteration 153: loss 0.3336152732372284
iteration 154: loss 0.333303838968277
iteration 155: loss 0.33299535512924194
iteration 156: loss 0.33268970251083374
iteration 157: loss 0.33238697052001953
iteration 158: loss 0.3320869505405426
iteration 159: loss 0.33178964257240295
iteration 160: loss 0.33149516582489014
iteration 161: loss 0.33120331168174744
iteration 162: loss 0.3309139311313629
iteration 163: loss 0.3306272327899933
iteration 164: loss 0.3303431272506714
iteration 165: loss 0.3300614655017853
iteration 166: loss 0.3297822177410126
iteration 167: loss 0.3295055031776428
iteration 168: loss 0.3292310833930969
iteration 169: loss 0.32895907759666443
iteration 170: loss 0.32868948578834534
iteration 171: loss 0.32842206954956055
iteration 172: loss 0.3281569182872772
iteration 173: loss 0.327894002199173
iteration 174: loss 0.3276333212852478
iteration 175: loss 0.32737481594085693
iteration 176: loss 0.3271183967590332
iteration 177: loss 0.32686418294906616
iteration 178: loss 0.32661184668540955
iteration 179: loss 0.326361745595932
iteration 180: loss 0.32611367106437683
iteration 181: loss 0.3258674144744873
iteration 182: loss 0.3256232738494873
iteration 183: loss 0.32538101077079773
iteration 184: loss 0.32514068484306335
iteration 185: loss 0.3249022364616394
iteration 186: loss 0.3246656656265259
iteration 187: loss 0.324430912733078
iteration 188: loss 0.3241979777812958
iteration 189: loss 0.3239668309688568
iteration 190: loss 0.3237374722957611
iteration 191: loss 0.3235098123550415
iteration 192: loss 0.32328397035598755
iteration 193: loss 0.32305967807769775
iteration 194: loss 0.322837233543396
iteration 195: loss 0.3226163685321808
iteration 196: loss 0.3223970830440521
iteration 197: loss 0.32217952609062195
iteration 198: loss 0.32196351885795593
iteration 199: loss 0.32174909114837646
iteration 200: loss 0.32153627276420593
iteration 201: loss 0.3213249146938324
iteration 202: loss 0.321115106344223
iteration 203: loss 0.32090678811073303
iteration 204: loss 0.32069993019104004
iteration 205: loss 0.32049453258514404
iteration 206: loss 0.3202906847000122
iteration 207: loss 0.3200881779193878
iteration 208: loss 0.31988710165023804
iteration 209: loss 0.3196874260902405
iteration 210: loss 0.3194892108440399
iteration 211: loss 0.319292277097702
iteration 212: loss 0.3190966546535492
iteration 213: loss 0.3189024031162262
iteration 214: loss 0.31870949268341064
iteration 215: loss 0.31851789355278015
iteration 216: loss 0.31832754611968994
iteration 217: loss 0.3181384801864624
iteration 218: loss 0.31795069575309753
iteration 219: loss 0.31776419281959534
iteration 220: loss 0.31757891178131104
iteration 221: loss 0.31739476323127747
iteration 222: loss 0.317211776971817
iteration 223: loss 0.3170301914215088
iteration 224: loss 0.3168496787548065
iteration 225: loss 0.3166702687740326
iteration 226: loss 0.31649211049079895
iteration 227: loss 0.3163149952888489
iteration 228: loss 0.3161391615867615
iteration 229: loss 0.31596434116363525
iteration 230: loss 0.315790593624115
iteration 231: loss 0.31561797857284546
iteration 232: loss 0.3154464066028595
iteration 233: loss 0.3152758777141571
iteration 234: loss 0.3151065409183502
iteration 235: loss 0.31493815779685974
iteration 236: loss 0.3147709369659424
iteration 237: loss 0.31460461020469666
iteration 238: loss 0.31443923711776733
iteration 239: loss 0.31427496671676636
iteration 240: loss 0.31411170959472656
iteration 241: loss 0.31394943594932556
iteration 242: loss 0.3137879967689514
iteration 243: loss 0.313627690076828
iteration 244: loss 0.31346824765205383
iteration 245: loss 0.31330981850624084
iteration 246: loss 0.3131522834300995
iteration 247: loss 0.31299570202827454
iteration 248: loss 0.31283998489379883
iteration 249: loss 0.3126852512359619
iteration 250: loss 0.31253132224082947
iteration 251: loss 0.31237831711769104
iteration 252: loss 0.31222623586654663
iteration 253: loss 0.31207501888275146
iteration 254: loss 0.311924546957016
iteration 255: loss 0.3117750585079193
iteration 256: loss 0.3116263747215271
iteration 257: loss 0.3114785850048065
iteration 258: loss 0.3113315999507904
iteration 259: loss 0.31118544936180115
iteration 260: loss 0.31104010343551636
iteration 261: loss 0.3108954429626465
iteration 262: loss 0.3107517659664154
iteration 263: loss 0.31060880422592163
iteration 264: loss 0.31046658754348755
iteration 265: loss 0.3103252649307251
iteration 266: loss 0.31018462777137756
iteration 267: loss 0.3100447356700897
iteration 268: loss 0.30990567803382874
iteration 269: loss 0.3097672760486603
iteration 270: loss 0.30962973833084106
iteration 271: loss 0.309492826461792
iteration 272: loss 0.3093567192554474
iteration 273: loss 0.3092212677001953
iteration 274: loss 0.30908656120300293
iteration 275: loss 0.3089526295661926
iteration 276: loss 0.30881932377815247
iteration 277: loss 0.308686763048172
iteration 278: loss 0.30855488777160645
iteration 279: loss 0.3084236681461334
iteration 280: loss 0.30829310417175293
iteration 281: loss 0.30816328525543213
iteration 282: loss 0.30803412199020386
iteration 283: loss 0.30790552496910095
iteration 284: loss 0.3077777028083801
iteration 285: loss 0.30765044689178467
iteration 286: loss 0.3075239062309265
iteration 287: loss 0.3073979914188385
iteration 288: loss 0.307272732257843
iteration 289: loss 0.3071480095386505
iteration 290: loss 0.30702394247055054
iteration 291: loss 0.30690041184425354
iteration 292: loss 0.306777685880661
iteration 293: loss 0.30665549635887146
iteration 294: loss 0.30653393268585205
iteration 295: loss 0.3064129650592804
iteration 296: loss 0.30629244446754456
iteration 297: loss 0.30617254972457886
iteration 298: loss 0.3060533404350281
iteration 299: loss 0.3059346675872803
Accuracy:  0.9184 
Recall:  [(0.9795918367346939, 0.9467455621301775), (0.973568281938326, 0.9608695652173913), (0.8827519379844961, 0.9314928425357873), (0.9108910891089109, 0.9019607843137255), (0.9317718940936863, 0.9077380952380952), (0.8632286995515696, 0.9090909090909091), (0.9498956158663883, 0.9276248725790011), (0.914396887159533, 0.9233791748526523), (0.8788501026694046, 0.8708036622583927), (0.8889990089197225, 0.8961038961038961)] 
Matrix:
 [[ 960    0    2    2    0    4    9    1    2    0]
 [   0 1105    2    2    0    2    4    2   18    0]
 [   8    8  911   16   15    1   14   13   40    6]
 [   3    1   23  920    0   24    2   12   17    8]
 [   1    1    4    1  915    0   13    2    8   37]
 [  10    3    3   35    9  770   16    7   31    8]
 [  12    3    3    2   11   13  910    2    2    0]
 [   3   12   21    8    8    0    0  940    3   33]
 [   6   10    6   23    9   25   13   14  856   12]
 [  11    7    3   11   41    8    0   25    6  897]]
Architecture: [784, 10]
Experiment done!

iteration 0: loss 2.3031036853790283
iteration 1: loss 2.297778606414795
iteration 2: loss 2.291696786880493
iteration 3: loss 2.283109664916992
iteration 4: loss 2.270200729370117
iteration 5: loss 2.25060772895813
iteration 6: loss 2.221226453781128
iteration 7: loss 2.178367853164673
iteration 8: loss 2.118278980255127
iteration 9: loss 2.037829637527466
iteration 10: loss 1.935089349746704
iteration 11: loss 1.8108150959014893
iteration 12: loss 1.6706303358078003
iteration 13: loss 1.5248483419418335
iteration 14: loss 1.3847137689590454
iteration 15: loss 1.2581017017364502
iteration 16: loss 1.1480761766433716
iteration 17: loss 1.0542974472045898
iteration 18: loss 0.9748724699020386
iteration 19: loss 0.9077297449111938
iteration 20: loss 0.8512250781059265
iteration 21: loss 0.8059925436973572
iteration 22: loss 0.7859642505645752
iteration 23: loss 0.8846638798713684
iteration 24: loss 1.4635323286056519
iteration 25: loss 1.6235382556915283
iteration 26: loss 1.468932867050171
iteration 27: loss 0.9678987264633179
iteration 28: loss 0.7690041661262512
iteration 29: loss 0.6782647371292114
iteration 30: loss 0.6341697573661804
iteration 31: loss 0.606941282749176
iteration 32: loss 0.5935525894165039
iteration 33: loss 0.6039152145385742
iteration 34: loss 0.670593798160553
iteration 35: loss 0.8327945470809937
iteration 36: loss 1.001588225364685
iteration 37: loss 0.7957218289375305
iteration 38: loss 0.6149117350578308
iteration 39: loss 0.5454157590866089
iteration 40: loss 0.5150510668754578
iteration 41: loss 0.5000520944595337
iteration 42: loss 0.49347633123397827
iteration 43: loss 0.49113592505455017
iteration 44: loss 0.502913236618042
iteration 45: loss 0.5105149149894714
iteration 46: loss 0.5522858500480652
iteration 47: loss 0.5349224209785461
iteration 48: loss 0.5866482257843018
iteration 49: loss 0.5044952034950256
iteration 50: loss 0.5145949125289917
iteration 51: loss 0.4706917703151703
iteration 52: loss 0.47269946336746216
iteration 53: loss 0.46519947052001953
iteration 54: loss 0.478199303150177
iteration 55: loss 0.4903164505958557
iteration 56: loss 0.5201412439346313
iteration 57: loss 0.521263837814331
iteration 58: loss 0.532793402671814
iteration 59: loss 0.49454763531684875
iteration 60: loss 0.47136062383651733
iteration 61: loss 0.4353097975254059
iteration 62: loss 0.41618502140045166
iteration 63: loss 0.39889800548553467
iteration 64: loss 0.389578640460968
iteration 65: loss 0.3814907968044281
iteration 66: loss 0.3764845132827759
iteration 67: loss 0.37175697088241577
iteration 68: loss 0.36844316124916077
iteration 69: loss 0.3651050925254822
iteration 70: loss 0.36257991194725037
iteration 71: loss 0.35995471477508545
iteration 72: loss 0.35791027545928955
iteration 73: loss 0.3557511866092682
iteration 74: loss 0.3540745973587036
iteration 75: loss 0.35224756598472595
iteration 76: loss 0.35087233781814575
iteration 77: loss 0.3493324816226959
iteration 78: loss 0.34825682640075684
iteration 79: loss 0.34696492552757263
iteration 80: loss 0.34615132212638855
iteration 81: loss 0.3450480103492737
iteration 82: loss 0.344439297914505
iteration 83: loss 0.34345361590385437
iteration 84: loss 0.34295403957366943
iteration 85: loss 0.3419824242591858
iteration 86: loss 0.34141552448272705
iteration 87: loss 0.34030428528785706
iteration 88: loss 0.3394675850868225
iteration 89: loss 0.3380580544471741
iteration 90: loss 0.33679571747779846
iteration 91: loss 0.33501335978507996
iteration 92: loss 0.33328521251678467
iteration 93: loss 0.3311702609062195
iteration 94: loss 0.3290815055370331
iteration 95: loss 0.32678091526031494
iteration 96: loss 0.32452908158302307
iteration 97: loss 0.3222084045410156
iteration 98: loss 0.32000014185905457
iteration 99: loss 0.3178064525127411
iteration 100: loss 0.3157544732093811
iteration 101: loss 0.3137591779232025
iteration 102: loss 0.3119080066680908
iteration 103: loss 0.3101312816143036
iteration 104: loss 0.30847078561782837
iteration 105: loss 0.30688023567199707
iteration 106: loss 0.3053852915763855
iteration 107: loss 0.3039577007293701
iteration 108: loss 0.3025900423526764
iteration 109: loss 0.30127471685409546
iteration 110: loss 0.3000110387802124
iteration 111: loss 0.29878270626068115
iteration 112: loss 0.2976020276546478
iteration 113: loss 0.29645127058029175
iteration 114: loss 0.29532793164253235
iteration 115: loss 0.2942349910736084
iteration 116: loss 0.2931594252586365
iteration 117: loss 0.29211118817329407
iteration 118: loss 0.29107609391212463
iteration 119: loss 0.2900596857070923
iteration 120: loss 0.2890584170818329
iteration 121: loss 0.28807365894317627
iteration 122: loss 0.28710100054740906
iteration 123: loss 0.2861425280570984
iteration 124: loss 0.2851947247982025
iteration 125: loss 0.28425753116607666
iteration 126: loss 0.2833315432071686
iteration 127: loss 0.2824154198169708
iteration 128: loss 0.28150999546051025
iteration 129: loss 0.28061431646347046
iteration 130: loss 0.2797277271747589
iteration 131: loss 0.2788501977920532
iteration 132: loss 0.27798065543174744
iteration 133: loss 0.27711912989616394
iteration 134: loss 0.2762638330459595
iteration 135: loss 0.27541792392730713
iteration 136: loss 0.27457913756370544
iteration 137: loss 0.2737474739551544
iteration 138: loss 0.2729218304157257
iteration 139: loss 0.2721029818058014
iteration 140: loss 0.2712906002998352
iteration 141: loss 0.27048468589782715
iteration 142: loss 0.2696833908557892
iteration 143: loss 0.2688872516155243
iteration 144: loss 0.268097460269928
iteration 145: loss 0.26731374859809875
iteration 146: loss 0.26653462648391724
iteration 147: loss 0.26576098799705505
iteration 148: loss 0.26499322056770325
iteration 149: loss 0.26423177123069763
iteration 150: loss 0.2634754180908203
iteration 151: loss 0.26272356510162354
iteration 152: loss 0.26197636127471924
iteration 153: loss 0.26123350858688354
iteration 154: loss 0.26049456000328064
iteration 155: loss 0.25976014137268066
iteration 156: loss 0.25903087854385376
iteration 157: loss 0.25830650329589844
iteration 158: loss 0.2575870752334595
iteration 159: loss 0.2568732500076294
iteration 160: loss 0.25616440176963806
iteration 161: loss 0.2554596960544586
iteration 162: loss 0.2547597885131836
iteration 163: loss 0.25406497716903687
iteration 164: loss 0.2533753514289856
iteration 165: loss 0.2526906430721283
iteration 166: loss 0.2520088851451874
iteration 167: loss 0.2513311803340912
iteration 168: loss 0.25065878033638
iteration 169: loss 0.2499912679195404
iteration 170: loss 0.24932771921157837
iteration 171: loss 0.24866849184036255
iteration 172: loss 0.24801357090473175
iteration 173: loss 0.24736297130584717
iteration 174: loss 0.2467154860496521
iteration 175: loss 0.24607163667678833
iteration 176: loss 0.24543118476867676
iteration 177: loss 0.2447943091392517
iteration 178: loss 0.2441614866256714
iteration 179: loss 0.24353210628032684
iteration 180: loss 0.24290649592876434
iteration 181: loss 0.24228444695472717
iteration 182: loss 0.24166661500930786
iteration 183: loss 0.24105282127857208
iteration 184: loss 0.24044351279735565
iteration 185: loss 0.23983842134475708
iteration 186: loss 0.23923742771148682
iteration 187: loss 0.23864026367664337
iteration 188: loss 0.2380465567111969
iteration 189: loss 0.23745694756507874
iteration 190: loss 0.23687146604061127
iteration 191: loss 0.2362903654575348
iteration 192: loss 0.23571264743804932
iteration 193: loss 0.23513834178447723
iteration 194: loss 0.23456741869449615
iteration 195: loss 0.23399952054023743
iteration 196: loss 0.23343561589717865
iteration 197: loss 0.2328752875328064
iteration 198: loss 0.23231759667396545
iteration 199: loss 0.23176375031471252
iteration 200: loss 0.2312130331993103
iteration 201: loss 0.23066522181034088
iteration 202: loss 0.23012064397335052
iteration 203: loss 0.22957900166511536
iteration 204: loss 0.22904108464717865
iteration 205: loss 0.22850586473941803
iteration 206: loss 0.22797369956970215
iteration 207: loss 0.22744446992874146
iteration 208: loss 0.2269182950258255
iteration 209: loss 0.22639496624469757
iteration 210: loss 0.22587451338768005
iteration 211: loss 0.2253570407629013
iteration 212: loss 0.22484250366687775
iteration 213: loss 0.22433070838451385
iteration 214: loss 0.22382159531116486
iteration 215: loss 0.22331508994102478
iteration 216: loss 0.22281156480312347
iteration 217: loss 0.2223115712404251
iteration 218: loss 0.22181400656700134
iteration 219: loss 0.2213193029165268
iteration 220: loss 0.22082754969596863
iteration 221: loss 0.2203381210565567
iteration 222: loss 0.21985115110874176
iteration 223: loss 0.2193659394979477
iteration 224: loss 0.21888293325901031
iteration 225: loss 0.21840231120586395
iteration 226: loss 0.21792463958263397
iteration 227: loss 0.21744973957538605
iteration 228: loss 0.21697835624217987
iteration 229: loss 0.2165094017982483
iteration 230: loss 0.21604320406913757
iteration 231: loss 0.21557949483394623
iteration 232: loss 0.21511803567409515
iteration 233: loss 0.2146594524383545
iteration 234: loss 0.21420393884181976
iteration 235: loss 0.21375110745429993
iteration 236: loss 0.21330076456069946
iteration 237: loss 0.21285301446914673
iteration 238: loss 0.2124074101448059
iteration 239: loss 0.21196456253528595
iteration 240: loss 0.21152406930923462
iteration 241: loss 0.21108505129814148
iteration 242: loss 0.21064846217632294
iteration 243: loss 0.21021421253681183
iteration 244: loss 0.20978206396102905
iteration 245: loss 0.2093524932861328
iteration 246: loss 0.2089252769947052
iteration 247: loss 0.20849987864494324
iteration 248: loss 0.2080763876438141
iteration 249: loss 0.20765499770641327
iteration 250: loss 0.20723553001880646
iteration 251: loss 0.20681807398796082
iteration 252: loss 0.20640289783477783
iteration 253: loss 0.2059902548789978
iteration 254: loss 0.20557977259159088
iteration 255: loss 0.205171599984169
iteration 256: loss 0.20476564764976501
iteration 257: loss 0.2043619453907013
iteration 258: loss 0.2039603292942047
iteration 259: loss 0.20356078445911407
iteration 260: loss 0.20316366851329803
iteration 261: loss 0.20276835560798645
iteration 262: loss 0.20237477123737335
iteration 263: loss 0.20198304951190948
iteration 264: loss 0.20159360766410828
iteration 265: loss 0.201206237077713
iteration 266: loss 0.20082008838653564
iteration 267: loss 0.20043614506721497
iteration 268: loss 0.2000541090965271
iteration 269: loss 0.19967398047447205
iteration 270: loss 0.1992960423231125
iteration 271: loss 0.19891978800296783
iteration 272: loss 0.19854506850242615
iteration 273: loss 0.19817200303077698
iteration 274: loss 0.1978006809949875
iteration 275: loss 0.19743108749389648
iteration 276: loss 0.19706299901008606
iteration 277: loss 0.19669663906097412
iteration 278: loss 0.1963321417570114
iteration 279: loss 0.19596928358078003
iteration 280: loss 0.19560833275318146
iteration 281: loss 0.1952493041753769
iteration 282: loss 0.1948920339345932
iteration 283: loss 0.1945367008447647
iteration 284: loss 0.1941826045513153
iteration 285: loss 0.19383031129837036
iteration 286: loss 0.19347943365573883
iteration 287: loss 0.1931300163269043
iteration 288: loss 0.19278199970722198
iteration 289: loss 0.19243569672107697
iteration 290: loss 0.19209088385105133
iteration 291: loss 0.19174763560295105
iteration 292: loss 0.1914059966802597
iteration 293: loss 0.19106543064117432
iteration 294: loss 0.19072645902633667
iteration 295: loss 0.19038893282413483
iteration 296: loss 0.1900528371334076
iteration 297: loss 0.1897183507680893
iteration 298: loss 0.18938538432121277
iteration 299: loss 0.18905393779277802
iteration 300: loss 0.18872369825839996
iteration 301: loss 0.18839479982852936
iteration 302: loss 0.1880672425031662
iteration 303: loss 0.18774110078811646
iteration 304: loss 0.18741627037525177
iteration 305: loss 0.1870928257703781
iteration 306: loss 0.18677066266536713
iteration 307: loss 0.18644995987415314
iteration 308: loss 0.18613028526306152
iteration 309: loss 0.18581166863441467
iteration 310: loss 0.1854940503835678
iteration 311: loss 0.18517743051052094
iteration 312: loss 0.18486231565475464
iteration 313: loss 0.18454794585704803
iteration 314: loss 0.18423444032669067
iteration 315: loss 0.18392197787761688
iteration 316: loss 0.183610737323761
iteration 317: loss 0.18330073356628418
iteration 318: loss 0.18299196660518646
iteration 319: loss 0.18268424272537231
iteration 320: loss 0.18237759172916412
iteration 321: loss 0.18207192420959473
iteration 322: loss 0.18176724016666412
iteration 323: loss 0.18146367371082306
iteration 324: loss 0.18116141855716705
iteration 325: loss 0.18085992336273193
iteration 326: loss 0.18055938184261322
iteration 327: loss 0.18026013672351837
iteration 328: loss 0.17996171116828918
iteration 329: loss 0.17966383695602417
iteration 330: loss 0.1793665736913681
iteration 331: loss 0.17907080054283142
iteration 332: loss 0.17877642810344696
iteration 333: loss 0.17848312854766846
iteration 334: loss 0.17819097638130188
iteration 335: loss 0.1779000163078308
iteration 336: loss 0.17760995030403137
iteration 337: loss 0.17732073366641998
iteration 338: loss 0.1770327091217041
iteration 339: loss 0.17674586176872253
iteration 340: loss 0.17646051943302155
iteration 341: loss 0.17617599666118622
iteration 342: loss 0.1758926957845688
iteration 343: loss 0.17561013996601105
iteration 344: loss 0.17532873153686523
iteration 345: loss 0.17504838109016418
iteration 346: loss 0.17476911842823029
iteration 347: loss 0.17449060082435608
iteration 348: loss 0.17421290278434753
iteration 349: loss 0.1739361733198166
iteration 350: loss 0.1736602783203125
iteration 351: loss 0.17338533699512482
iteration 352: loss 0.17311128973960876
iteration 353: loss 0.1728384792804718
iteration 354: loss 0.172566756606102
iteration 355: loss 0.1722957342863083
iteration 356: loss 0.17202574014663696
iteration 357: loss 0.17175672948360443
iteration 358: loss 0.17148855328559875
iteration 359: loss 0.17122124135494232
Accuracy:  0.9511 
Recall:  [(0.9836734693877551, 0.9620758483033932), (0.9814977973568282, 0.9754816112084063), (0.9486434108527132, 0.956989247311828), (0.9554455445544554, 0.9368932038834952), (0.9490835030549898, 0.9442755825734549), (0.929372197309417, 0.9506880733944955), (0.9624217118997912, 0.9495365602471678), (0.9426070038910506, 0.9528023598820059), (0.9291581108829569, 0.9427083333333334), (0.9236868186323092, 0.9357429718875502)] 
Matrix:
 [[ 964    0    1    1    0    4    6    1    2    1]
 [   0 1114    3    2    0    1    3    2   10    0]
 [   7    3  979    9    7    0    8    8    8    3]
 [   0    0    7  965    1   14    0   12    8    3]
 [   1    2    4    0  932    0   11    2    3   27]
 [   7    2    1   18    3  829   11    3   11    7]
 [   8    3    2    0    9    9  922    0    5    0]
 [   3    8   22    3    4    0    0  969    2   17]
 [   4    4    3   18    7   10    9    8  905    6]
 [   8    6    1   14   24    5    1   12    6  932]]
Architecture: [784, 100, 10]
Experiment done!

iteration 0: loss 2.30302095413208
iteration 1: loss 2.302842378616333
iteration 2: loss 2.302677631378174
iteration 3: loss 2.302523612976074
iteration 4: loss 2.302380084991455
iteration 5: loss 2.30224347114563
iteration 6: loss 2.302117109298706
iteration 7: loss 2.301994800567627
iteration 8: loss 2.301880121231079
iteration 9: loss 2.301769495010376
iteration 10: loss 2.301664113998413
iteration 11: loss 2.3015620708465576
iteration 12: loss 2.301462173461914
iteration 13: loss 2.3013648986816406
iteration 14: loss 2.301267623901367
iteration 15: loss 2.301170587539673
iteration 16: loss 2.301074981689453
iteration 17: loss 2.300976276397705
iteration 18: loss 2.3008759021759033
iteration 19: loss 2.300773859024048
iteration 20: loss 2.300668239593506
iteration 21: loss 2.3005576133728027
iteration 22: loss 2.300442934036255
iteration 23: loss 2.3003220558166504
iteration 24: loss 2.3001935482025146
iteration 25: loss 2.3000571727752686
iteration 26: loss 2.2999119758605957
iteration 27: loss 2.2997560501098633
iteration 28: loss 2.299588203430176
iteration 29: loss 2.2994070053100586
iteration 30: loss 2.2992093563079834
iteration 31: loss 2.298994302749634
iteration 32: loss 2.298759698867798
iteration 33: loss 2.298501491546631
iteration 34: loss 2.298217296600342
iteration 35: loss 2.297903537750244
iteration 36: loss 2.2975547313690186
iteration 37: loss 2.2971668243408203
iteration 38: loss 2.2967333793640137
iteration 39: loss 2.2962465286254883
iteration 40: loss 2.295698404312134
iteration 41: loss 2.2950775623321533
iteration 42: loss 2.2943713665008545
iteration 43: loss 2.293564558029175
iteration 44: loss 2.2926363945007324
iteration 45: loss 2.29156494140625
iteration 46: loss 2.29032039642334
iteration 47: loss 2.2888641357421875
iteration 48: loss 2.28714919090271
iteration 49: loss 2.285116195678711
iteration 50: loss 2.2826876640319824
iteration 51: loss 2.2797627449035645
iteration 52: loss 2.2762105464935303
iteration 53: loss 2.2718586921691895
iteration 54: loss 2.2664778232574463
iteration 55: loss 2.2597665786743164
iteration 56: loss 2.251322031021118
iteration 57: loss 2.240609645843506
iteration 58: loss 2.226930856704712
iteration 59: loss 2.2093870639801025
iteration 60: loss 2.1868553161621094
iteration 61: loss 2.1579976081848145
iteration 62: loss 2.121330976486206
iteration 63: loss 2.075406789779663
iteration 64: loss 2.0191969871520996
iteration 65: loss 1.9527629613876343
iteration 66: loss 1.8780134916305542
iteration 67: loss 1.7987351417541504
iteration 68: loss 1.7191287279129028
iteration 69: loss 1.6418081521987915
iteration 70: loss 1.5673468112945557
iteration 71: loss 1.4952996969223022
iteration 72: loss 1.4262821674346924
iteration 73: loss 1.3800264596939087
iteration 74: loss 1.6386570930480957
iteration 75: loss 4.15830659866333
iteration 76: loss 1.9788204431533813
iteration 77: loss 1.8261754512786865
iteration 78: loss 1.680856704711914
iteration 79: loss 1.6016533374786377
iteration 80: loss 1.5415605306625366
iteration 81: loss 1.4880812168121338
iteration 82: loss 1.440289855003357
iteration 83: loss 1.4009785652160645
iteration 84: loss 1.383599877357483
iteration 85: loss 1.456257939338684
iteration 86: loss 1.6173557043075562
iteration 87: loss 1.676591157913208
iteration 88: loss 1.4137096405029297
iteration 89: loss 1.2541115283966064
iteration 90: loss 1.181633472442627
iteration 91: loss 1.1388485431671143
iteration 92: loss 1.1002541780471802
iteration 93: loss 1.0625431537628174
iteration 94: loss 1.031009316444397
iteration 95: loss 1.0344901084899902
iteration 96: loss 1.2824912071228027
iteration 97: loss 2.038403272628784
iteration 98: loss 1.9124953746795654
iteration 99: loss 1.4821478128433228
iteration 100: loss 1.3731623888015747
iteration 101: loss 1.3018405437469482
iteration 102: loss 1.2347486019134521
iteration 103: loss 1.165938377380371
iteration 104: loss 1.0957242250442505
iteration 105: loss 1.0270520448684692
iteration 106: loss 0.9818446040153503
iteration 107: loss 1.0364680290222168
iteration 108: loss 1.3702791929244995
iteration 109: loss 1.3706985712051392
iteration 110: loss 1.1840763092041016
iteration 111: loss 1.0022040605545044
iteration 112: loss 0.8850978016853333
iteration 113: loss 0.8114702105522156
iteration 114: loss 0.7860589623451233
iteration 115: loss 0.8046799898147583
iteration 116: loss 0.9038190841674805
iteration 117: loss 1.171200156211853
iteration 118: loss 1.1373355388641357
iteration 119: loss 0.9444302916526794
iteration 120: loss 0.8165292143821716
iteration 121: loss 0.747299075126648
iteration 122: loss 0.7295817136764526
iteration 123: loss 0.746042013168335
iteration 124: loss 0.8054848909378052
iteration 125: loss 0.8747252821922302
iteration 126: loss 0.9073314666748047
iteration 127: loss 0.7838019132614136
iteration 128: loss 0.7064788341522217
iteration 129: loss 0.668801486492157
iteration 130: loss 0.6522539258003235
iteration 131: loss 0.6510187387466431
iteration 132: loss 0.651504397392273
iteration 133: loss 0.6719292402267456
iteration 134: loss 0.6653900146484375
iteration 135: loss 0.678213357925415
iteration 136: loss 0.6431866884231567
iteration 137: loss 0.6261758804321289
iteration 138: loss 0.5944527983665466
iteration 139: loss 0.575034499168396
iteration 140: loss 0.5571482181549072
iteration 141: loss 0.5462210178375244
iteration 142: loss 0.5432738065719604
iteration 143: loss 0.5560877323150635
iteration 144: loss 0.5929644107818604
iteration 145: loss 0.6944060325622559
iteration 146: loss 0.7037102580070496
iteration 147: loss 0.6948577761650085
iteration 148: loss 0.6079401969909668
iteration 149: loss 0.5622081756591797
iteration 150: loss 0.5415648221969604
iteration 151: loss 0.5228323340415955
iteration 152: loss 0.5164557099342346
iteration 153: loss 0.5084936022758484
iteration 154: loss 0.5030484199523926
iteration 155: loss 0.4982834756374359
iteration 156: loss 0.49049779772758484
iteration 157: loss 0.4851347804069519
iteration 158: loss 0.4750337600708008
iteration 159: loss 0.4685567319393158
iteration 160: loss 0.4585157334804535
iteration 161: loss 0.4524037539958954
iteration 162: loss 0.44414159655570984
iteration 163: loss 0.4396008253097534
iteration 164: loss 0.4341219961643219
iteration 165: loss 0.4314895272254944
iteration 166: loss 0.4290924370288849
iteration 167: loss 0.42841199040412903
iteration 168: loss 0.42979925870895386
iteration 169: loss 0.43116873502731323
iteration 170: loss 0.43625307083129883
iteration 171: loss 0.43868374824523926
iteration 172: loss 0.44328510761260986
iteration 173: loss 0.44234225153923035
iteration 174: loss 0.4370943605899811
iteration 175: loss 0.427934855222702
iteration 176: loss 0.4133562445640564
iteration 177: loss 0.4016737937927246
iteration 178: loss 0.3887176811695099
iteration 179: loss 0.3808380663394928
iteration 180: loss 0.37286314368247986
iteration 181: loss 0.36854228377342224
iteration 182: loss 0.3638532757759094
iteration 183: loss 0.36160752177238464
iteration 184: loss 0.3587097227573395
iteration 185: loss 0.3579663336277008
iteration 186: loss 0.3565114438533783
iteration 187: loss 0.3572644889354706
iteration 188: loss 0.3575512766838074
iteration 189: loss 0.36024656891822815
iteration 190: loss 0.36323583126068115
iteration 191: loss 0.36843302845954895
iteration 192: loss 0.3753385543823242
iteration 193: loss 0.38236138224601746
iteration 194: loss 0.39189413189888
iteration 195: loss 0.3944995105266571
iteration 196: loss 0.39880406856536865
iteration 197: loss 0.3869227170944214
iteration 198: loss 0.3790062665939331
iteration 199: loss 0.35982978343963623
iteration 200: loss 0.3484598994255066
iteration 201: loss 0.3364766240119934
iteration 202: loss 0.32973530888557434
iteration 203: loss 0.3246261179447174
iteration 204: loss 0.3212754428386688
iteration 205: loss 0.3191035985946655
iteration 206: loss 0.3172374963760376
iteration 207: loss 0.3163582980632782
iteration 208: loss 0.3153008222579956
iteration 209: loss 0.3154884874820709
iteration 210: loss 0.31509214639663696
iteration 211: loss 0.31667736172676086
iteration 212: loss 0.3170192241668701
iteration 213: loss 0.32057449221611023
iteration 214: loss 0.3213384747505188
iteration 215: loss 0.3273218870162964
iteration 216: loss 0.3275236487388611
iteration 217: loss 0.33494871854782104
iteration 218: loss 0.33201512694358826
iteration 219: loss 0.33723464608192444
iteration 220: loss 0.3288951814174652
iteration 221: loss 0.32815971970558167
iteration 222: loss 0.3166380822658539
iteration 223: loss 0.31161218881607056
iteration 224: loss 0.30229324102401733
iteration 225: loss 0.2974925637245178
iteration 226: loss 0.2919995188713074
iteration 227: loss 0.2887539863586426
iteration 228: loss 0.28559786081314087
iteration 229: loss 0.2833998203277588
iteration 230: loss 0.2813391089439392
iteration 231: loss 0.2796480655670166
iteration 232: loss 0.27804797887802124
iteration 233: loss 0.27663031220436096
iteration 234: loss 0.27525594830513
iteration 235: loss 0.2739783525466919
iteration 236: loss 0.2727203369140625
iteration 237: loss 0.27152135968208313
iteration 238: loss 0.27033811807632446
iteration 239: loss 0.2691898047924042
iteration 240: loss 0.26805800199508667
iteration 241: loss 0.2669525444507599
iteration 242: loss 0.26585790514945984
iteration 243: loss 0.2647838294506073
iteration 244: loss 0.26372334361076355
iteration 245: loss 0.2626798450946808
iteration 246: loss 0.26164793968200684
iteration 247: loss 0.26064178347587585
iteration 248: loss 0.25963231921195984
iteration 249: loss 0.25865498185157776
iteration 250: loss 0.2576677203178406
iteration 251: loss 0.25671693682670593
iteration 252: loss 0.2557560205459595
iteration 253: loss 0.2548366189002991
iteration 254: loss 0.2539061903953552
iteration 255: loss 0.25302380323410034
iteration 256: loss 0.25211864709854126
iteration 257: loss 0.25127822160720825
iteration 258: loss 0.2504131495952606
iteration 259: loss 0.24962635338306427
iteration 260: loss 0.248799130320549
iteration 261: loss 0.2480858713388443
iteration 262: loss 0.24730683863162994
iteration 263: loss 0.24668367207050323
iteration 264: loss 0.24595658481121063
iteration 265: loss 0.24545466899871826
iteration 266: loss 0.24478332698345184
iteration 267: loss 0.2444353550672531
iteration 268: loss 0.24381668865680695
iteration 269: loss 0.24366937577724457
iteration 270: loss 0.2431575506925583
iteration 271: loss 0.24328596889972687
iteration 272: loss 0.24284707009792328
iteration 273: loss 0.24334833025932312
iteration 274: loss 0.2429465651512146
iteration 275: loss 0.24387294054031372
iteration 276: loss 0.24355104565620422
iteration 277: loss 0.24494993686676025
iteration 278: loss 0.2445496767759323
iteration 279: loss 0.24638527631759644
iteration 280: loss 0.24568746984004974
iteration 281: loss 0.24764879047870636
iteration 282: loss 0.24644052982330322
iteration 283: loss 0.24807102978229523
iteration 284: loss 0.24632732570171356
iteration 285: loss 0.2472517490386963
iteration 286: loss 0.24520030617713928
iteration 287: loss 0.24521911144256592
iteration 288: loss 0.243363618850708
iteration 289: loss 0.24257802963256836
iteration 290: loss 0.24133655428886414
iteration 291: loss 0.2400714010000229
iteration 292: loss 0.23951536417007446
iteration 293: loss 0.23804834485054016
iteration 294: loss 0.2380952537059784
iteration 295: loss 0.2364627569913864
iteration 296: loss 0.23696604371070862
iteration 297: loss 0.23508897423744202
iteration 298: loss 0.23583780229091644
iteration 299: loss 0.2335864156484604
iteration 300: loss 0.2341926097869873
iteration 301: loss 0.23146574199199677
iteration 302: loss 0.23164890706539154
iteration 303: loss 0.2284742295742035
iteration 304: loss 0.22804009914398193
iteration 305: loss 0.22468549013137817
iteration 306: loss 0.22371722757816315
iteration 307: loss 0.22060450911521912
iteration 308: loss 0.2193707823753357
iteration 309: loss 0.2167346030473709
iteration 310: loss 0.2154763787984848
iteration 311: loss 0.21342884004116058
iteration 312: loss 0.21227934956550598
iteration 313: loss 0.2106635719537735
iteration 314: loss 0.2096434384584427
iteration 315: loss 0.20837944746017456
iteration 316: loss 0.20747163891792297
iteration 317: loss 0.2064308077096939
iteration 318: loss 0.2056075781583786
iteration 319: loss 0.20469960570335388
iteration 320: loss 0.2039475440979004
iteration 321: loss 0.20313186943531036
iteration 322: loss 0.2024374157190323
iteration 323: loss 0.201682910323143
iteration 324: loss 0.20103101432323456
iteration 325: loss 0.20032668113708496
iteration 326: loss 0.19970178604125977
iteration 327: loss 0.19904005527496338
iteration 328: loss 0.1984412521123886
iteration 329: loss 0.19780972599983215
iteration 330: loss 0.19723445177078247
iteration 331: loss 0.1966232806444168
iteration 332: loss 0.19607201218605042
iteration 333: loss 0.19548949599266052
iteration 334: loss 0.1949634999036789
iteration 335: loss 0.19440999627113342
iteration 336: loss 0.19391554594039917
iteration 337: loss 0.19339384138584137
iteration 338: loss 0.19293507933616638
iteration 339: loss 0.19244977831840515
iteration 340: loss 0.1920417696237564
iteration 341: loss 0.1916077733039856
iteration 342: loss 0.191282719373703
iteration 343: loss 0.19091112911701202
iteration 344: loss 0.19069182872772217
iteration 345: loss 0.19040432572364807
iteration 346: loss 0.19034714996814728
iteration 347: loss 0.19020773470401764
iteration 348: loss 0.19037601351737976
iteration 349: loss 0.19041325151920319
iteration 350: loss 0.1908925324678421
iteration 351: loss 0.19120965898036957
iteration 352: loss 0.19218608736991882
iteration 353: loss 0.19290059804916382
iteration 354: loss 0.19460026919841766
iteration 355: loss 0.19577419757843018
iteration 356: loss 0.1985636204481125
iteration 357: loss 0.20028769969940186
iteration 358: loss 0.20453809201717377
iteration 359: loss 0.206496924161911
iteration 360: loss 0.2120099812746048
iteration 361: loss 0.2135082483291626
iteration 362: loss 0.2192172408103943
iteration 363: loss 0.21835976839065552
iteration 364: loss 0.22091300785541534
iteration 365: loss 0.2151668220758438
iteration 366: loss 0.21160770952701569
iteration 367: loss 0.20191828906536102
iteration 368: loss 0.1949995458126068
iteration 369: loss 0.1873958855867386
iteration 370: loss 0.18291811645030975
iteration 371: loss 0.17953793704509735
iteration 372: loss 0.1775597780942917
iteration 373: loss 0.1761309653520584
iteration 374: loss 0.17513754963874817
iteration 375: loss 0.1743299663066864
iteration 376: loss 0.17365755140781403
iteration 377: loss 0.1730514019727707
iteration 378: loss 0.17249442636966705
iteration 379: loss 0.17196229100227356
iteration 380: loss 0.17145103216171265
iteration 381: loss 0.17095209658145905
iteration 382: loss 0.17046408355236053
iteration 383: loss 0.1699836701154709
iteration 384: loss 0.16951176524162292
iteration 385: loss 0.16904529929161072
iteration 386: loss 0.16858483850955963
iteration 387: loss 0.16812929511070251
iteration 388: loss 0.16767950356006622
iteration 389: loss 0.16723422706127167
iteration 390: loss 0.16679280996322632
iteration 391: loss 0.16635480523109436
iteration 392: loss 0.1659202128648758
iteration 393: loss 0.16548855602741241
iteration 394: loss 0.16505995392799377
iteration 395: loss 0.16463452577590942
iteration 396: loss 0.16421300172805786
iteration 397: loss 0.16379466652870178
iteration 398: loss 0.16337938606739044
iteration 399: loss 0.16296696662902832
iteration 400: loss 0.1625574380159378
iteration 401: loss 0.162150576710701
iteration 402: loss 0.1617465615272522
iteration 403: loss 0.16134445369243622
iteration 404: loss 0.16094465553760529
iteration 405: loss 0.1605473756790161
iteration 406: loss 0.16015256941318512
iteration 407: loss 0.15976013243198395
iteration 408: loss 0.15936970710754395
iteration 409: loss 0.15898139774799347
iteration 410: loss 0.1585952639579773
iteration 411: loss 0.15821149945259094
iteration 412: loss 0.15782983601093292
iteration 413: loss 0.1574508100748062
iteration 414: loss 0.15707363188266754
iteration 415: loss 0.1566983014345169
iteration 416: loss 0.15632526576519012
iteration 417: loss 0.15595468878746033
iteration 418: loss 0.15558631718158722
iteration 419: loss 0.15522003173828125
iteration 420: loss 0.15485580265522003
iteration 421: loss 0.15449394285678864
iteration 422: loss 0.15413394570350647
iteration 423: loss 0.1537752002477646
iteration 424: loss 0.15341880917549133
iteration 425: loss 0.15306419134140015
iteration 426: loss 0.15271195769309998
iteration 427: loss 0.15236221253871918
iteration 428: loss 0.15201415121555328
iteration 429: loss 0.15166811645030975
iteration 430: loss 0.15132488310337067
iteration 431: loss 0.1509835571050644
iteration 432: loss 0.15064436197280884
iteration 433: loss 0.15030787885189056
iteration 434: loss 0.14997412264347076
iteration 435: loss 0.14964279532432556
iteration 436: loss 0.14931486546993256
iteration 437: loss 0.14898976683616638
iteration 438: loss 0.14866898953914642
iteration 439: loss 0.14835360646247864
iteration 440: loss 0.14804217219352722
iteration 441: loss 0.14773863554000854
iteration 442: loss 0.14744409918785095
iteration 443: loss 0.14715617895126343
iteration 444: loss 0.14688262343406677
iteration 445: loss 0.14662352204322815
iteration 446: loss 0.14639200270175934
iteration 447: loss 0.14617468416690826
iteration 448: loss 0.14600569009780884
iteration 449: loss 0.14585614204406738
iteration 450: loss 0.14578472077846527
iteration 451: loss 0.14575247466564178
iteration 452: loss 0.14584359526634216
iteration 453: loss 0.1459893137216568
iteration 454: loss 0.1463412046432495
iteration 455: loss 0.14679346978664398
iteration 456: loss 0.14762602746486664
iteration 457: loss 0.1485797017812729
iteration 458: loss 0.1502758115530014
iteration 459: loss 0.15215536952018738
iteration 460: loss 0.1555386632680893
iteration 461: loss 0.15915285050868988
iteration 462: loss 0.1660739779472351
iteration 463: loss 0.17403435707092285
iteration 464: loss 0.19088242948055267
iteration 465: loss 0.21549414098262787
iteration 466: loss 0.28400808572769165
iteration 467: loss 0.5022190809249878
iteration 468: loss 1.431434988975525
iteration 469: loss 2.246722459793091
iteration 470: loss 1.4486840963363647
iteration 471: loss 1.0526143312454224
iteration 472: loss 0.7038940787315369
iteration 473: loss 0.5794761180877686
iteration 474: loss 0.47184520959854126
iteration 475: loss 0.37155526876449585
iteration 476: loss 0.32195886969566345
iteration 477: loss 0.29580458998680115
iteration 478: loss 0.27610185742378235
iteration 479: loss 0.2622915208339691
iteration 480: loss 0.2502270042896271
iteration 481: loss 0.24120782315731049
iteration 482: loss 0.2336476892232895
iteration 483: loss 0.22760702669620514
iteration 484: loss 0.22248320281505585
iteration 485: loss 0.2181280106306076
iteration 486: loss 0.21425755321979523
iteration 487: loss 0.21075531840324402
iteration 488: loss 0.2075531929731369
iteration 489: loss 0.20462289452552795
iteration 490: loss 0.2019110918045044
iteration 491: loss 0.1994081735610962
iteration 492: loss 0.19707106053829193
iteration 493: loss 0.19487988948822021
iteration 494: loss 0.1928081065416336
iteration 495: loss 0.19084693491458893
iteration 496: loss 0.18898747861385345
iteration 497: loss 0.18723665177822113
iteration 498: loss 0.1855812966823578
iteration 499: loss 0.18400314450263977
iteration 500: loss 0.18249599635601044
iteration 501: loss 0.18105670809745789
iteration 502: loss 0.1796877086162567
iteration 503: loss 0.17838139832019806
iteration 504: loss 0.17712830007076263
iteration 505: loss 0.1759294718503952
iteration 506: loss 0.17477770149707794
iteration 507: loss 0.17367012798786163
iteration 508: loss 0.1725982278585434
iteration 509: loss 0.17155838012695312
iteration 510: loss 0.17054928839206696
iteration 511: loss 0.16956721246242523
iteration 512: loss 0.1686139553785324
iteration 513: loss 0.16768445074558258
iteration 514: loss 0.1667788326740265
iteration 515: loss 0.16590477526187897
iteration 516: loss 0.1650553196668625
iteration 517: loss 0.16423258185386658
iteration 518: loss 0.16342569887638092
iteration 519: loss 0.16264042258262634
iteration 520: loss 0.16187648475170135
iteration 521: loss 0.16113197803497314
iteration 522: loss 0.16040906310081482
iteration 523: loss 0.15970328450202942
iteration 524: loss 0.15902328491210938
iteration 525: loss 0.15836144983768463
iteration 526: loss 0.1577136218547821
iteration 527: loss 0.1570875197649002
iteration 528: loss 0.15647538006305695
iteration 529: loss 0.15588662028312683
iteration 530: loss 0.15530145168304443
iteration 531: loss 0.15474945306777954
iteration 532: loss 0.1541896015405655
iteration 533: loss 0.15366362035274506
iteration 534: loss 0.15312810242176056
iteration 535: loss 0.1526370644569397
iteration 536: loss 0.1521334946155548
iteration 537: loss 0.15167270600795746
iteration 538: loss 0.15118874609470367
iteration 539: loss 0.1507592648267746
iteration 540: loss 0.15029805898666382
iteration 541: loss 0.14988449215888977
iteration 542: loss 0.14944221079349518
iteration 543: loss 0.14905183017253876
iteration 544: loss 0.1486201286315918
iteration 545: loss 0.1482497602701187
iteration 546: loss 0.1478305160999298
iteration 547: loss 0.14749056100845337
iteration 548: loss 0.147079735994339
iteration 549: loss 0.14675454795360565
iteration 550: loss 0.1463499814271927
iteration 551: loss 0.14604954421520233
iteration 552: loss 0.1456473171710968
iteration 553: loss 0.14536352455615997
iteration 554: loss 0.1449604332447052
iteration 555: loss 0.14471179246902466
iteration 556: loss 0.14431624114513397
iteration 557: loss 0.14410623908042908
iteration 558: loss 0.1437249332666397
iteration 559: loss 0.14355815947055817
iteration 560: loss 0.1431959867477417
iteration 561: loss 0.1430777609348297
iteration 562: loss 0.14273478090763092
iteration 563: loss 0.1426805555820465
iteration 564: loss 0.14237035810947418
iteration 565: loss 0.14236675202846527
iteration 566: loss 0.14207737147808075
iteration 567: loss 0.1421574503183365
iteration 568: loss 0.14190605282783508
iteration 569: loss 0.14209039509296417
iteration 570: loss 0.1418425738811493
iteration 571: loss 0.14209716022014618
iteration 572: loss 0.1418943852186203
iteration 573: loss 0.14222025871276855
iteration 574: loss 0.142048180103302
iteration 575: loss 0.1424233317375183
iteration 576: loss 0.14223867654800415
iteration 577: loss 0.14269262552261353
iteration 578: loss 0.1424511969089508
iteration 579: loss 0.1428268402814865
iteration 580: loss 0.14244098961353302
iteration 581: loss 0.14264412224292755
iteration 582: loss 0.14204145967960358
iteration 583: loss 0.14191284775733948
iteration 584: loss 0.14101609587669373
iteration 585: loss 0.14048631489276886
iteration 586: loss 0.13930091261863708
iteration 587: loss 0.13839438557624817
iteration 588: loss 0.13708671927452087
iteration 589: loss 0.13600149750709534
iteration 590: loss 0.13472668826580048
iteration 591: loss 0.1336999386548996
iteration 592: loss 0.13265366852283478
iteration 593: loss 0.1318293660879135
iteration 594: loss 0.13103018701076508
iteration 595: loss 0.13038797676563263
iteration 596: loss 0.12978562712669373
iteration 597: loss 0.12928307056427002
iteration 598: loss 0.12880639731884003
iteration 599: loss 0.12837925553321838
iteration 600: loss 0.1279716044664383
iteration 601: loss 0.1276027113199234
iteration 602: loss 0.1272430717945099
iteration 603: loss 0.12690448760986328
iteration 604: loss 0.12657736241817474
iteration 605: loss 0.12626080214977264
iteration 606: loss 0.12595440447330475
iteration 607: loss 0.12565159797668457
iteration 608: loss 0.125355526804924
iteration 609: loss 0.125061497092247
iteration 610: loss 0.12477510422468185
iteration 611: loss 0.1244923397898674
iteration 612: loss 0.12421189248561859
iteration 613: loss 0.12393524497747421
iteration 614: loss 0.12366144359111786
iteration 615: loss 0.12339100986719131
iteration 616: loss 0.12312101572751999
iteration 617: loss 0.12285436689853668
iteration 618: loss 0.12259061634540558
iteration 619: loss 0.12233100086450577
iteration 620: loss 0.12207328528165817
iteration 621: loss 0.1218189001083374
iteration 622: loss 0.12156612426042557
iteration 623: loss 0.12131411582231522
iteration 624: loss 0.12106456607580185
iteration 625: loss 0.12081597000360489
iteration 626: loss 0.12057001888751984
iteration 627: loss 0.12032530456781387
iteration 628: loss 0.12008167058229446
iteration 629: loss 0.11984101682901382
iteration 630: loss 0.11960124969482422
iteration 631: loss 0.11936430633068085
iteration 632: loss 0.1191292405128479
iteration 633: loss 0.11889571696519852
iteration 634: loss 0.11866427958011627
iteration 635: loss 0.1184348315000534
iteration 636: loss 0.11820799857378006
iteration 637: loss 0.11798065900802612
iteration 638: loss 0.11775520443916321
iteration 639: loss 0.11753083020448685
Accuracy:  0.9584 
Recall:  [(0.9826530612244898, 0.9658976930792377), (0.9859030837004406, 0.9798598949211909), (0.9544573643410853, 0.9507722007722008), (0.9643564356435643, 0.945631067961165), (0.9582484725050916, 0.9533941236068896), (0.9394618834080718, 0.9555302166476625), (0.9634655532359081, 0.9574688796680498), (0.9484435797665369, 0.9701492537313433), (0.9476386036960985, 0.9505664263645726), (0.9345887016848364, 0.9515640766902119)] 
Matrix:
 [[ 963    0    0    1    0    8    5    1    2    0]
 [   0 1119    3    2    0    1    4    0    6    0]
 [   8    3  985    7    5    0    6    7   11    0]
 [   0    0   12  974    0    9    0    6    8    1]
 [   1    1    6    0  941    1    8    2    2   20]
 [   7    1    0   20    1  838    9    1   10    5]
 [   9    3    3    0    8    8  923    1    3    0]
 [   1    8   18    3    3    0    0  975    1   19]
 [   3    2    7   13    3    7    8    5  923    3]
 [   5    5    2   10   26    5    1    7    5  943]]
Architecture: [784, 100, 100, 10]
Experiment done!

iteration 0: loss 2.3030991554260254
iteration 1: loss 2.3030502796173096
iteration 2: loss 2.3030049800872803
iteration 3: loss 2.3029725551605225
iteration 4: loss 2.302928924560547
iteration 5: loss 2.302891969680786
iteration 6: loss 2.302852153778076
iteration 7: loss 2.3028159141540527
iteration 8: loss 2.302783727645874
iteration 9: loss 2.3027455806732178
iteration 10: loss 2.302710771560669
iteration 11: loss 2.3026819229125977
iteration 12: loss 2.3026418685913086
iteration 13: loss 2.3026182651519775
iteration 14: loss 2.3025877475738525
iteration 15: loss 2.3025636672973633
iteration 16: loss 2.3025357723236084
iteration 17: loss 2.302509069442749
iteration 18: loss 2.3024818897247314
iteration 19: loss 2.3024580478668213
iteration 20: loss 2.3024303913116455
iteration 21: loss 2.3024113178253174
iteration 22: loss 2.3023858070373535
iteration 23: loss 2.3023626804351807
iteration 24: loss 2.302345037460327
iteration 25: loss 2.302323341369629
iteration 26: loss 2.302299737930298
iteration 27: loss 2.302283525466919
iteration 28: loss 2.3022615909576416
iteration 29: loss 2.3022429943084717
iteration 30: loss 2.30222487449646
iteration 31: loss 2.3022093772888184
iteration 32: loss 2.302194356918335
iteration 33: loss 2.3021762371063232
iteration 34: loss 2.3021605014801025
iteration 35: loss 2.3021464347839355
iteration 36: loss 2.302130937576294
iteration 37: loss 2.3021183013916016
iteration 38: loss 2.3021039962768555
iteration 39: loss 2.3020856380462646
iteration 40: loss 2.302075147628784
iteration 41: loss 2.3020589351654053
iteration 42: loss 2.302049160003662
iteration 43: loss 2.3020360469818115
iteration 44: loss 2.3020272254943848
iteration 45: loss 2.302013635635376
iteration 46: loss 2.302008628845215
iteration 47: loss 2.301992416381836
iteration 48: loss 2.301985263824463
iteration 49: loss 2.3019776344299316
iteration 50: loss 2.301966905593872
iteration 51: loss 2.301952362060547
iteration 52: loss 2.3019442558288574
iteration 53: loss 2.301936626434326
iteration 54: loss 2.3019323348999023
iteration 55: loss 2.3019235134124756
iteration 56: loss 2.301914691925049
iteration 57: loss 2.301901340484619
iteration 58: loss 2.3018972873687744
iteration 59: loss 2.3018908500671387
iteration 60: loss 2.3018815517425537
iteration 61: loss 2.3018717765808105
iteration 62: loss 2.3018698692321777
iteration 63: loss 2.3018581867218018
iteration 64: loss 2.3018548488616943
iteration 65: loss 2.3018486499786377
iteration 66: loss 2.3018405437469482
iteration 67: loss 2.3018393516540527
iteration 68: loss 2.301831007003784
iteration 69: loss 2.3018295764923096
iteration 70: loss 2.301820993423462
iteration 71: loss 2.3018157482147217
iteration 72: loss 2.3018100261688232
iteration 73: loss 2.3018059730529785
iteration 74: loss 2.3018054962158203
iteration 75: loss 2.3017940521240234
iteration 76: loss 2.301793336868286
iteration 77: loss 2.3017935752868652
iteration 78: loss 2.301788806915283
iteration 79: loss 2.301779270172119
iteration 80: loss 2.3017799854278564
iteration 81: loss 2.3017728328704834
iteration 82: loss 2.301774263381958
iteration 83: loss 2.301769495010376
iteration 84: loss 2.3017659187316895
iteration 85: loss 2.3017592430114746
iteration 86: loss 2.301759719848633
iteration 87: loss 2.301753282546997
iteration 88: loss 2.3017494678497314
iteration 89: loss 2.301743984222412
iteration 90: loss 2.3017477989196777
iteration 91: loss 2.301741361618042
iteration 92: loss 2.301741361618042
iteration 93: loss 2.3017377853393555
iteration 94: loss 2.301732301712036
iteration 95: loss 2.3017287254333496
iteration 96: loss 2.301727771759033
iteration 97: loss 2.3017280101776123
iteration 98: loss 2.301724672317505
iteration 99: loss 2.3017196655273438
iteration 100: loss 2.3017187118530273
iteration 101: loss 2.301717519760132
iteration 102: loss 2.301715135574341
iteration 103: loss 2.301710605621338
iteration 104: loss 2.3017115592956543
iteration 105: loss 2.301713228225708
iteration 106: loss 2.3017053604125977
iteration 107: loss 2.3017101287841797
iteration 108: loss 2.301704168319702
iteration 109: loss 2.3017048835754395
iteration 110: loss 2.3017046451568604
iteration 111: loss 2.3017020225524902
iteration 112: loss 2.3016984462738037
iteration 113: loss 2.3016974925994873
iteration 114: loss 2.3016982078552246
iteration 115: loss 2.3016977310180664
iteration 116: loss 2.3016903400421143
iteration 117: loss 2.3016977310180664
iteration 118: loss 2.3016862869262695
iteration 119: loss 2.3016860485076904
iteration 120: loss 2.301682949066162
iteration 121: loss 2.3016908168792725
iteration 122: loss 2.301689624786377
iteration 123: loss 2.3016843795776367
iteration 124: loss 2.301683187484741
iteration 125: loss 2.3016817569732666
iteration 126: loss 2.301682472229004
iteration 127: loss 2.3016796112060547
iteration 128: loss 2.301677703857422
iteration 129: loss 2.30168080329895
iteration 130: loss 2.301679849624634
iteration 131: loss 2.301682472229004
iteration 132: loss 2.3016767501831055
iteration 133: loss 2.30167293548584
iteration 134: loss 2.3016750812530518
iteration 135: loss 2.3016786575317383
iteration 136: loss 2.3016738891601562
iteration 137: loss 2.3016700744628906
iteration 138: loss 2.301670551300049
iteration 139: loss 2.301670551300049
iteration 140: loss 2.3016719818115234
iteration 141: loss 2.3016726970672607
iteration 142: loss 2.301666259765625
iteration 143: loss 2.3016746044158936
iteration 144: loss 2.3016674518585205
iteration 145: loss 2.301670789718628
iteration 146: loss 2.301670551300049
iteration 147: loss 2.3016655445098877
iteration 148: loss 2.301663398742676
iteration 149: loss 2.301665782928467
iteration 150: loss 2.301663637161255
iteration 151: loss 2.3016624450683594
iteration 152: loss 2.301665782928467
iteration 153: loss 2.301666021347046
iteration 154: loss 2.301663875579834
iteration 155: loss 2.3016653060913086
iteration 156: loss 2.3016631603240967
iteration 157: loss 2.3016648292541504
iteration 158: loss 2.3016645908355713
iteration 159: loss 2.301661252975464
iteration 160: loss 2.3016560077667236
iteration 161: loss 2.301658868789673
iteration 162: loss 2.301659345626831
iteration 163: loss 2.3016600608825684
iteration 164: loss 2.301659345626831
iteration 165: loss 2.301658868789673
iteration 166: loss 2.3016607761383057
iteration 167: loss 2.301657199859619
iteration 168: loss 2.30165958404541
iteration 169: loss 2.30165958404541
iteration 170: loss 2.301661729812622
iteration 171: loss 2.3016602993011475
iteration 172: loss 2.301659107208252
iteration 173: loss 2.301659345626831
iteration 174: loss 2.301657199859619
iteration 175: loss 2.301654577255249
iteration 176: loss 2.301652669906616
iteration 177: loss 2.3016510009765625
iteration 178: loss 2.301654815673828
iteration 179: loss 2.3016555309295654
iteration 180: loss 2.3016536235809326
iteration 181: loss 2.301654100418091
iteration 182: loss 2.3016531467437744
iteration 183: loss 2.301654577255249
iteration 184: loss 2.3016576766967773
iteration 185: loss 2.3016531467437744
iteration 186: loss 2.3016481399536133
iteration 187: loss 2.301649570465088
iteration 188: loss 2.301652431488037
iteration 189: loss 2.3016531467437744
iteration 190: loss 2.301652669906616
iteration 191: loss 2.3016517162323
iteration 192: loss 2.301649570465088
iteration 193: loss 2.301649808883667
iteration 194: loss 2.3016552925109863
iteration 195: loss 2.301654100418091
iteration 196: loss 2.301649332046509
iteration 197: loss 2.3016512393951416
iteration 198: loss 2.301647901535034
iteration 199: loss 2.3016457557678223
iteration 200: loss 2.3016490936279297
iteration 201: loss 2.3016510009765625
iteration 202: loss 2.301652431488037
iteration 203: loss 2.3016483783721924
iteration 204: loss 2.3016510009765625
iteration 205: loss 2.3016464710235596
iteration 206: loss 2.3016481399536133
iteration 207: loss 2.301649332046509
iteration 208: loss 2.301649332046509
iteration 209: loss 2.301654100418091
iteration 210: loss 2.301652669906616
iteration 211: loss 2.301649808883667
iteration 212: loss 2.301647901535034
iteration 213: loss 2.3016462326049805
iteration 214: loss 2.3016488552093506
iteration 215: loss 2.301645517349243
iteration 216: loss 2.301649570465088
iteration 217: loss 2.3016486167907715
iteration 218: loss 2.3016533851623535
iteration 219: loss 2.301649332046509
iteration 220: loss 2.3016483783721924
iteration 221: loss 2.301645278930664
iteration 222: loss 2.3016436100006104
iteration 223: loss 2.3016462326049805
iteration 224: loss 2.3016436100006104
iteration 225: loss 2.3016464710235596
iteration 226: loss 2.301645040512085
iteration 227: loss 2.3016457557678223
iteration 228: loss 2.301649570465088
iteration 229: loss 2.3016462326049805
iteration 230: loss 2.3016467094421387
iteration 231: loss 2.3016440868377686
iteration 232: loss 2.3016409873962402
iteration 233: loss 2.3016414642333984
iteration 234: loss 2.3016438484191895
iteration 235: loss 2.3016409873962402
iteration 236: loss 2.301643133163452
iteration 237: loss 2.301645278930664
iteration 238: loss 2.3016414642333984
iteration 239: loss 2.3016457557678223
iteration 240: loss 2.3016459941864014
iteration 241: loss 2.3016433715820312
iteration 242: loss 2.301644802093506
iteration 243: loss 2.3016440868377686
iteration 244: loss 2.301642656326294
iteration 245: loss 2.301640033721924
iteration 246: loss 2.301640272140503
iteration 247: loss 2.301642656326294
iteration 248: loss 2.3016395568847656
iteration 249: loss 2.3016395568847656
iteration 250: loss 2.301642894744873
iteration 251: loss 2.3016421794891357
iteration 252: loss 2.301637887954712
iteration 253: loss 2.301640033721924
iteration 254: loss 2.3016409873962402
iteration 255: loss 2.3016414642333984
iteration 256: loss 2.301640510559082
iteration 257: loss 2.3016397953033447
iteration 258: loss 2.3016395568847656
iteration 259: loss 2.3016412258148193
iteration 260: loss 2.3016390800476074
iteration 261: loss 2.3016364574432373
iteration 262: loss 2.3016374111175537
iteration 263: loss 2.3016395568847656
iteration 264: loss 2.3016390800476074
iteration 265: loss 2.301635503768921
iteration 266: loss 2.3016366958618164
iteration 267: loss 2.301637887954712
iteration 268: loss 2.301638126373291
iteration 269: loss 2.301638603210449
iteration 270: loss 2.301637649536133
iteration 271: loss 2.3016357421875
iteration 272: loss 2.3016350269317627
iteration 273: loss 2.3016366958618164
iteration 274: loss 2.301638603210449
iteration 275: loss 2.3016388416290283
iteration 276: loss 2.3016366958618164
iteration 277: loss 2.3016371726989746
iteration 278: loss 2.3016393184661865
iteration 279: loss 2.301640510559082
iteration 280: loss 2.301640272140503
iteration 281: loss 2.301638603210449
iteration 282: loss 2.301637649536133
iteration 283: loss 2.301637649536133
iteration 284: loss 2.3016374111175537
iteration 285: loss 2.301637887954712
iteration 286: loss 2.301638603210449
iteration 287: loss 2.3016395568847656
iteration 288: loss 2.3016388416290283
iteration 289: loss 2.3016364574432373
iteration 290: loss 2.3016350269317627
iteration 291: loss 2.3016350269317627
iteration 292: loss 2.3016364574432373
iteration 293: loss 2.3016371726989746
iteration 294: loss 2.3016374111175537
iteration 295: loss 2.3016369342803955
iteration 296: loss 2.3016347885131836
iteration 297: loss 2.3016340732574463
iteration 298: loss 2.3016350269317627
iteration 299: loss 2.301635503768921
iteration 300: loss 2.301635503768921
iteration 301: loss 2.301635980606079
iteration 302: loss 2.3016364574432373
iteration 303: loss 2.301637887954712
iteration 304: loss 2.30163836479187
iteration 305: loss 2.301637649536133
iteration 306: loss 2.3016369342803955
iteration 307: loss 2.3016366958618164
iteration 308: loss 2.3016366958618164
iteration 309: loss 2.301637649536133
iteration 310: loss 2.301638603210449
iteration 311: loss 2.3016397953033447
iteration 312: loss 2.3016397953033447
iteration 313: loss 2.301638603210449
iteration 314: loss 2.3016371726989746
iteration 315: loss 2.3016350269317627
iteration 316: loss 2.3016345500946045
iteration 317: loss 2.3016343116760254
iteration 318: loss 2.3016343116760254
iteration 319: loss 2.3016343116760254
iteration 320: loss 2.3016343116760254
iteration 321: loss 2.3016345500946045
iteration 322: loss 2.3016345500946045
iteration 323: loss 2.3016345500946045
iteration 324: loss 2.3016347885131836
iteration 325: loss 2.3016350269317627
iteration 326: loss 2.3016350269317627
iteration 327: loss 2.3016340732574463
iteration 328: loss 2.301632881164551
iteration 329: loss 2.3016316890716553
iteration 330: loss 2.301630973815918
iteration 331: loss 2.3016302585601807
iteration 332: loss 2.3016302585601807
iteration 333: loss 2.301630973815918
iteration 334: loss 2.301631450653076
iteration 335: loss 2.3016319274902344
iteration 336: loss 2.3016321659088135
iteration 337: loss 2.3016324043273926
iteration 338: loss 2.3016319274902344
iteration 339: loss 2.3016316890716553
iteration 340: loss 2.301631450653076
iteration 341: loss 2.301630973815918
iteration 342: loss 2.3016300201416016
iteration 343: loss 2.3016297817230225
iteration 344: loss 2.3016295433044434
iteration 345: loss 2.3016295433044434
iteration 346: loss 2.3016293048858643
iteration 347: loss 2.3016293048858643
iteration 348: loss 2.301629066467285
iteration 349: loss 2.301629066467285
iteration 350: loss 2.301629066467285
iteration 351: loss 2.301629066467285
iteration 352: loss 2.301629066467285
iteration 353: loss 2.3016293048858643
iteration 354: loss 2.3016300201416016
iteration 355: loss 2.3016304969787598
iteration 356: loss 2.301631212234497
iteration 357: loss 2.301631212234497
iteration 358: loss 2.3016316890716553
iteration 359: loss 2.3016316890716553
iteration 360: loss 2.301631450653076
iteration 361: loss 2.301630735397339
iteration 362: loss 2.3016297817230225
iteration 363: loss 2.301629066467285
iteration 364: loss 2.301628351211548
iteration 365: loss 2.3016278743743896
iteration 366: loss 2.3016271591186523
iteration 367: loss 2.3016269207000732
iteration 368: loss 2.3016269207000732
iteration 369: loss 2.3016269207000732
iteration 370: loss 2.3016269207000732
iteration 371: loss 2.3016273975372314
iteration 372: loss 2.3016271591186523
iteration 373: loss 2.3016273975372314
iteration 374: loss 2.3016273975372314
iteration 375: loss 2.3016273975372314
iteration 376: loss 2.3016269207000732
iteration 377: loss 2.3016269207000732
iteration 378: loss 2.301626443862915
iteration 379: loss 2.301625967025757
iteration 380: loss 2.301625967025757
iteration 381: loss 2.3016254901885986
iteration 382: loss 2.3016254901885986
iteration 383: loss 2.3016247749328613
iteration 384: loss 2.3016247749328613
iteration 385: loss 2.3016247749328613
iteration 386: loss 2.3016247749328613
iteration 387: loss 2.3016247749328613
iteration 388: loss 2.3016247749328613
iteration 389: loss 2.301624298095703
iteration 390: loss 2.301624298095703
iteration 391: loss 2.301623821258545
iteration 392: loss 2.301623821258545
iteration 393: loss 2.3016233444213867
iteration 394: loss 2.3016228675842285
iteration 395: loss 2.3016228675842285
iteration 396: loss 2.301622152328491
iteration 397: loss 2.301622152328491
iteration 398: loss 2.301621913909912
iteration 399: loss 2.301621675491333
iteration 400: loss 2.301621198654175
iteration 401: loss 2.301621198654175
iteration 402: loss 2.3016207218170166
iteration 403: loss 2.3016207218170166
iteration 404: loss 2.3016204833984375
iteration 405: loss 2.3016207218170166
iteration 406: loss 2.3016204833984375
iteration 407: loss 2.3016204833984375
iteration 408: loss 2.3016204833984375
iteration 409: loss 2.3016207218170166
iteration 410: loss 2.3016207218170166
iteration 411: loss 2.3016207218170166
iteration 412: loss 2.301621198654175
iteration 413: loss 2.3016209602355957
iteration 414: loss 2.301621198654175
iteration 415: loss 2.301621437072754
iteration 416: loss 2.301621675491333
iteration 417: loss 2.301621675491333
iteration 418: loss 2.301621675491333
iteration 419: loss 2.301621675491333
iteration 420: loss 2.301621437072754
iteration 421: loss 2.301621437072754
iteration 422: loss 2.301621437072754
iteration 423: loss 2.301621437072754
iteration 424: loss 2.301621437072754
iteration 425: loss 2.301621198654175
iteration 426: loss 2.3016209602355957
iteration 427: loss 2.3016207218170166
iteration 428: loss 2.3016204833984375
iteration 429: loss 2.3016204833984375
iteration 430: loss 2.3016200065612793
iteration 431: loss 2.301619529724121
iteration 432: loss 2.301619291305542
iteration 433: loss 2.301619291305542
iteration 434: loss 2.301618814468384
iteration 435: loss 2.3016185760498047
iteration 436: loss 2.3016183376312256
iteration 437: loss 2.3016183376312256
iteration 438: loss 2.3016178607940674
iteration 439: loss 2.3016176223754883
iteration 440: loss 2.301617383956909
iteration 441: loss 2.301617383956909
iteration 442: loss 2.30161714553833
iteration 443: loss 2.301616668701172
iteration 444: loss 2.3016164302825928
iteration 445: loss 2.3016161918640137
iteration 446: loss 2.3016159534454346
iteration 447: loss 2.3016157150268555
iteration 448: loss 2.3016154766082764
iteration 449: loss 2.3016154766082764
iteration 450: loss 2.301614999771118
iteration 451: loss 2.301614761352539
iteration 452: loss 2.301614761352539
iteration 453: loss 2.30161452293396
iteration 454: loss 2.3016140460968018
iteration 455: loss 2.3016138076782227
iteration 456: loss 2.3016135692596436
iteration 457: loss 2.3016133308410645
iteration 458: loss 2.3016130924224854
iteration 459: loss 2.3016130924224854
iteration 460: loss 2.3016128540039062
iteration 461: loss 2.301612615585327
iteration 462: loss 2.301612615585327
iteration 463: loss 2.301612138748169
iteration 464: loss 2.30161190032959
iteration 465: loss 2.3016116619110107
iteration 466: loss 2.3016111850738525
iteration 467: loss 2.3016111850738525
iteration 468: loss 2.3016107082366943
iteration 469: loss 2.3016111850738525
iteration 470: loss 2.3016107082366943
iteration 471: loss 2.301610231399536
iteration 472: loss 2.301610231399536
iteration 473: loss 2.301609992980957
iteration 474: loss 2.301609754562378
iteration 475: loss 2.3016092777252197
iteration 476: loss 2.3016092777252197
iteration 477: loss 2.3016090393066406
iteration 478: loss 2.3016085624694824
iteration 479: loss 2.3016085624694824
iteration 480: loss 2.3016083240509033
iteration 481: loss 2.301608085632324
iteration 482: loss 2.301607847213745
iteration 483: loss 2.301607608795166
iteration 484: loss 2.301607608795166
iteration 485: loss 2.301607608795166
iteration 486: loss 2.301607131958008
iteration 487: loss 2.301607131958008
iteration 488: loss 2.301607131958008
iteration 489: loss 2.3016066551208496
iteration 490: loss 2.3016066551208496
iteration 491: loss 2.3016061782836914
iteration 492: loss 2.3016061782836914
iteration 493: loss 2.3016061782836914
iteration 494: loss 2.301605463027954
iteration 495: loss 2.301605463027954
iteration 496: loss 2.301605224609375
iteration 497: loss 2.301604986190796
iteration 498: loss 2.301604986190796
iteration 499: loss 2.301604747772217
iteration 500: loss 2.3016045093536377
iteration 501: loss 2.3016045093536377
iteration 502: loss 2.3016040325164795
iteration 503: loss 2.3016040325164795
iteration 504: loss 2.3016040325164795
iteration 505: loss 2.3016037940979004
iteration 506: loss 2.3016035556793213
iteration 507: loss 2.3016035556793213
iteration 508: loss 2.3016035556793213
iteration 509: loss 2.301602840423584
iteration 510: loss 2.301602840423584
iteration 511: loss 2.301602840423584
iteration 512: loss 2.301602602005005
iteration 513: loss 2.301602363586426
iteration 514: loss 2.301602363586426
iteration 515: loss 2.3016018867492676
iteration 516: loss 2.3016021251678467
iteration 517: loss 2.3016021251678467
iteration 518: loss 2.3016018867492676
iteration 519: loss 2.3016016483306885
iteration 520: loss 2.3016016483306885
iteration 521: loss 2.3016016483306885
iteration 522: loss 2.3016011714935303
iteration 523: loss 2.3016011714935303
iteration 524: loss 2.301600933074951
iteration 525: loss 2.301600694656372
iteration 526: loss 2.301600694656372
iteration 527: loss 2.301600217819214
iteration 528: loss 2.3015999794006348
iteration 529: loss 2.3015999794006348
iteration 530: loss 2.3015995025634766
iteration 531: loss 2.3015995025634766
iteration 532: loss 2.3015995025634766
iteration 533: loss 2.3015990257263184
iteration 534: loss 2.3015990257263184
iteration 535: loss 2.3015987873077393
iteration 536: loss 2.30159854888916
iteration 537: loss 2.30159854888916
iteration 538: loss 2.301598072052002
iteration 539: loss 2.301598072052002
iteration 540: loss 2.301598072052002
iteration 541: loss 2.3015975952148438
iteration 542: loss 2.3015973567962646
iteration 543: loss 2.3015971183776855
iteration 544: loss 2.3015968799591064
iteration 545: loss 2.3015966415405273
iteration 546: loss 2.3015964031219482
iteration 547: loss 2.3015964031219482
iteration 548: loss 2.30159592628479
iteration 549: loss 2.30159592628479
iteration 550: loss 2.301595687866211
iteration 551: loss 2.301595449447632
iteration 552: loss 2.301595449447632
iteration 553: loss 2.3015947341918945
iteration 554: loss 2.3015944957733154
iteration 555: loss 2.3015942573547363
iteration 556: loss 2.3015944957733154
iteration 557: loss 2.3015944957733154
iteration 558: loss 2.3015940189361572
iteration 559: loss 2.301593780517578
iteration 560: loss 2.301593542098999
iteration 561: loss 2.30159330368042
iteration 562: loss 2.301593065261841
iteration 563: loss 2.3015928268432617
iteration 564: loss 2.3015925884246826
iteration 565: loss 2.3015918731689453
iteration 566: loss 2.301591634750366
iteration 567: loss 2.301591396331787
iteration 568: loss 2.301591157913208
iteration 569: loss 2.301590919494629
iteration 570: loss 2.30159068107605
iteration 571: loss 2.3015904426574707
iteration 572: loss 2.3015902042388916
iteration 573: loss 2.3015899658203125
iteration 574: loss 2.301589250564575
iteration 575: loss 2.301589250564575
iteration 576: loss 2.301588773727417
iteration 577: loss 2.301588535308838
iteration 578: loss 2.301588296890259
iteration 579: loss 2.3015880584716797
iteration 580: loss 2.3015878200531006
iteration 581: loss 2.3015875816345215
iteration 582: loss 2.3015871047973633
iteration 583: loss 2.301586866378784
iteration 584: loss 2.301586866378784
iteration 585: loss 2.301586151123047
iteration 586: loss 2.3015859127044678
iteration 587: loss 2.3015856742858887
iteration 588: loss 2.3015854358673096
iteration 589: loss 2.3015849590301514
iteration 590: loss 2.3015847206115723
iteration 591: loss 2.301584243774414
iteration 592: loss 2.301584482192993
iteration 593: loss 2.301583766937256
iteration 594: loss 2.301583766937256
iteration 595: loss 2.3015832901000977
iteration 596: loss 2.3015832901000977
iteration 597: loss 2.3015828132629395
iteration 598: loss 2.3015825748443604
iteration 599: loss 2.3015823364257812
iteration 600: loss 2.301582098007202
iteration 601: loss 2.301581859588623
iteration 602: loss 2.3015811443328857
iteration 603: loss 2.3015811443328857
iteration 604: loss 2.3015806674957275
iteration 605: loss 2.3015804290771484
iteration 606: loss 2.3015801906585693
iteration 607: loss 2.3015799522399902
iteration 608: loss 2.301579713821411
iteration 609: loss 2.301579236984253
iteration 610: loss 2.301579236984253
iteration 611: loss 2.3015787601470947
iteration 612: loss 2.3015782833099365
iteration 613: loss 2.3015780448913574
iteration 614: loss 2.3015778064727783
iteration 615: loss 2.30157732963562
iteration 616: loss 2.301577091217041
iteration 617: loss 2.301577091217041
iteration 618: loss 2.301576614379883
iteration 619: loss 2.3015763759613037
iteration 620: loss 2.3015761375427246
iteration 621: loss 2.3015754222869873
iteration 622: loss 2.3015754222869873
iteration 623: loss 2.301574945449829
iteration 624: loss 2.30157470703125
iteration 625: loss 2.30157470703125
iteration 626: loss 2.301574230194092
iteration 627: loss 2.301574230194092
iteration 628: loss 2.3015737533569336
iteration 629: loss 2.3015735149383545
iteration 630: loss 2.3015732765197754
iteration 631: loss 2.301572561264038
iteration 632: loss 2.301572561264038
iteration 633: loss 2.30157208442688
iteration 634: loss 2.3015716075897217
iteration 635: loss 2.3015713691711426
iteration 636: loss 2.3015711307525635
iteration 637: loss 2.3015708923339844
iteration 638: loss 2.3015706539154053
iteration 639: loss 2.301569938659668
iteration 640: loss 2.301569700241089
iteration 641: loss 2.3015694618225098
iteration 642: loss 2.3015689849853516
iteration 643: loss 2.3015685081481934
iteration 644: loss 2.3015685081481934
iteration 645: loss 2.301568031311035
iteration 646: loss 2.301568031311035
iteration 647: loss 2.301567554473877
iteration 648: loss 2.3015670776367188
iteration 649: loss 2.3015668392181396
iteration 650: loss 2.3015666007995605
iteration 651: loss 2.3015663623809814
iteration 652: loss 2.3015658855438232
iteration 653: loss 2.301565408706665
iteration 654: loss 2.301565647125244
iteration 655: loss 2.301565170288086
iteration 656: loss 2.301565170288086
iteration 657: loss 2.3015644550323486
iteration 658: loss 2.3015639781951904
iteration 659: loss 2.3015639781951904
iteration 660: loss 2.3015635013580322
iteration 661: loss 2.301563262939453
iteration 662: loss 2.301563024520874
iteration 663: loss 2.301562547683716
iteration 664: loss 2.3015618324279785
iteration 665: loss 2.3015618324279785
iteration 666: loss 2.3015613555908203
iteration 667: loss 2.301560878753662
iteration 668: loss 2.301560640335083
iteration 669: loss 2.301560401916504
iteration 670: loss 2.301560401916504
iteration 671: loss 2.3015599250793457
iteration 672: loss 2.3015594482421875
iteration 673: loss 2.30155873298645
iteration 674: loss 2.30155873298645
iteration 675: loss 2.301558256149292
iteration 676: loss 2.301557779312134
iteration 677: loss 2.3015573024749756
iteration 678: loss 2.3015573024749756
iteration 679: loss 2.3015568256378174
iteration 680: loss 2.3015565872192383
iteration 681: loss 2.301556348800659
iteration 682: loss 2.301555871963501
iteration 683: loss 2.301555633544922
iteration 684: loss 2.3015553951263428
iteration 685: loss 2.3015549182891846
iteration 686: loss 2.3015544414520264
iteration 687: loss 2.301553964614868
iteration 688: loss 2.301553964614868
iteration 689: loss 2.301553249359131
iteration 690: loss 2.3015527725219727
iteration 691: loss 2.3015522956848145
iteration 692: loss 2.3015520572662354
iteration 693: loss 2.301551580429077
iteration 694: loss 2.301551342010498
iteration 695: loss 2.3015506267547607
iteration 696: loss 2.3015501499176025
iteration 697: loss 2.3015496730804443
iteration 698: loss 2.301549196243286
iteration 699: loss 2.301548957824707
iteration 700: loss 2.301548719406128
iteration 701: loss 2.3015480041503906
iteration 702: loss 2.3015475273132324
iteration 703: loss 2.3015472888946533
iteration 704: loss 2.301546573638916
iteration 705: loss 2.301546573638916
iteration 706: loss 2.301546096801758
iteration 707: loss 2.3015458583831787
iteration 708: loss 2.3015451431274414
iteration 709: loss 2.301544666290283
iteration 710: loss 2.301544189453125
iteration 711: loss 2.301543712615967
iteration 712: loss 2.3015432357788086
iteration 713: loss 2.3015425205230713
iteration 714: loss 2.301542043685913
iteration 715: loss 2.301541805267334
iteration 716: loss 2.301541328430176
iteration 717: loss 2.3015408515930176
iteration 718: loss 2.3015406131744385
iteration 719: loss 2.301539659500122
iteration 720: loss 2.301539421081543
iteration 721: loss 2.3015384674072266
iteration 722: loss 2.3015384674072266
iteration 723: loss 2.3015377521514893
iteration 724: loss 2.30153751373291
iteration 725: loss 2.301536798477173
iteration 726: loss 2.3015360832214355
iteration 727: loss 2.3015358448028564
iteration 728: loss 2.3015356063842773
iteration 729: loss 2.301535129547119
iteration 730: loss 2.301534652709961
iteration 731: loss 2.3015336990356445
iteration 732: loss 2.3015334606170654
iteration 733: loss 2.3015329837799072
iteration 734: loss 2.301532506942749
iteration 735: loss 2.301532030105591
iteration 736: loss 2.3015313148498535
iteration 737: loss 2.3015308380126953
iteration 738: loss 2.301530361175537
iteration 739: loss 2.301529884338379
iteration 740: loss 2.3015294075012207
iteration 741: loss 2.3015286922454834
iteration 742: loss 2.301528215408325
iteration 743: loss 2.301527738571167
iteration 744: loss 2.301527261734009
iteration 745: loss 2.3015267848968506
iteration 746: loss 2.301525831222534
iteration 747: loss 2.301525592803955
iteration 748: loss 2.3015248775482178
iteration 749: loss 2.3015248775482178
iteration 750: loss 2.3015241622924805
iteration 751: loss 2.3015239238739014
iteration 752: loss 2.301523208618164
iteration 753: loss 2.3015224933624268
iteration 754: loss 2.3015217781066895
iteration 755: loss 2.3015215396881104
iteration 756: loss 2.301521062850952
iteration 757: loss 2.3015201091766357
iteration 758: loss 2.3015198707580566
iteration 759: loss 2.3015193939208984
iteration 760: loss 2.301518678665161
iteration 761: loss 2.301518201828003
iteration 762: loss 2.3015174865722656
iteration 763: loss 2.3015170097351074
iteration 764: loss 2.301516532897949
iteration 765: loss 2.301516056060791
iteration 766: loss 2.3015153408050537
iteration 767: loss 2.3015146255493164
iteration 768: loss 2.301513910293579
iteration 769: loss 2.301513671875
iteration 770: loss 2.301513195037842
iteration 771: loss 2.3015127182006836
iteration 772: loss 2.3015120029449463
iteration 773: loss 2.301511526107788
iteration 774: loss 2.30151104927063
iteration 775: loss 2.3015105724334717
iteration 776: loss 2.3015100955963135
iteration 777: loss 2.301509380340576
iteration 778: loss 2.3015084266662598
iteration 779: loss 2.3015079498291016
iteration 780: loss 2.3015074729919434
iteration 781: loss 2.301506996154785
iteration 782: loss 2.3015060424804688
iteration 783: loss 2.3015055656433105
iteration 784: loss 2.3015048503875732
iteration 785: loss 2.301504373550415
iteration 786: loss 2.3015036582946777
iteration 787: loss 2.3015031814575195
iteration 788: loss 2.3015029430389404
iteration 789: loss 2.301501989364624
iteration 790: loss 2.3015010356903076
iteration 791: loss 2.3015003204345703
iteration 792: loss 2.301499843597412
iteration 793: loss 2.301499366760254
iteration 794: loss 2.3014986515045166
iteration 795: loss 2.3014976978302
iteration 796: loss 2.301497220993042
iteration 797: loss 2.3014965057373047
iteration 798: loss 2.3014955520629883
iteration 799: loss 2.30149507522583
iteration 800: loss 2.301494598388672
iteration 801: loss 2.3014936447143555
iteration 802: loss 2.3014931678771973
iteration 803: loss 2.30149245262146
iteration 804: loss 2.3014919757843018
iteration 805: loss 2.3014912605285645
iteration 806: loss 2.301490545272827
iteration 807: loss 2.3014895915985107
iteration 808: loss 2.3014891147613525
iteration 809: loss 2.3014886379241943
iteration 810: loss 2.301487684249878
iteration 811: loss 2.3014869689941406
iteration 812: loss 2.3014864921569824
iteration 813: loss 2.301485538482666
iteration 814: loss 2.301485061645508
iteration 815: loss 2.3014838695526123
iteration 816: loss 2.301483392715454
iteration 817: loss 2.301482677459717
iteration 818: loss 2.3014819622039795
iteration 819: loss 2.301481008529663
iteration 820: loss 2.301480293273926
iteration 821: loss 2.3014800548553467
iteration 822: loss 2.3014791011810303
iteration 823: loss 2.301478385925293
iteration 824: loss 2.3014776706695557
iteration 825: loss 2.3014769554138184
iteration 826: loss 2.301476001739502
iteration 827: loss 2.3014752864837646
iteration 828: loss 2.3014748096466064
iteration 829: loss 2.30147385597229
iteration 830: loss 2.3014731407165527
iteration 831: loss 2.3014721870422363
iteration 832: loss 2.30147123336792
iteration 833: loss 2.3014705181121826
iteration 834: loss 2.3014700412750244
iteration 835: loss 2.301469087600708
iteration 836: loss 2.30146861076355
iteration 837: loss 2.3014676570892334
iteration 838: loss 2.301466703414917
iteration 839: loss 2.301466226577759
iteration 840: loss 2.3014655113220215
iteration 841: loss 2.301464557647705
iteration 842: loss 2.3014636039733887
iteration 843: loss 2.3014631271362305
iteration 844: loss 2.301461935043335
iteration 845: loss 2.3014614582061768
iteration 846: loss 2.3014605045318604
iteration 847: loss 2.301460027694702
iteration 848: loss 2.3014588356018066
iteration 849: loss 2.3014581203460693
iteration 850: loss 2.301457405090332
iteration 851: loss 2.3014564514160156
iteration 852: loss 2.301455497741699
iteration 853: loss 2.301454544067383
iteration 854: loss 2.3014538288116455
iteration 855: loss 2.301452875137329
iteration 856: loss 2.301452398300171
iteration 857: loss 2.3014512062072754
iteration 858: loss 2.301450252532959
iteration 859: loss 2.3014492988586426
iteration 860: loss 2.3014488220214844
iteration 861: loss 2.3014473915100098
iteration 862: loss 2.3014466762542725
iteration 863: loss 2.301445484161377
iteration 864: loss 2.3014445304870605
iteration 865: loss 2.3014438152313232
iteration 866: loss 2.301443099975586
iteration 867: loss 2.3014421463012695
iteration 868: loss 2.301441192626953
iteration 869: loss 2.3014400005340576
iteration 870: loss 2.3014392852783203
iteration 871: loss 2.301438570022583
iteration 872: loss 2.3014371395111084
iteration 873: loss 2.301436185836792
iteration 874: loss 2.3014352321624756
iteration 875: loss 2.301434278488159
iteration 876: loss 2.3014330863952637
iteration 877: loss 2.3014323711395264
iteration 878: loss 2.30143141746521
iteration 879: loss 2.3014302253723145
iteration 880: loss 2.30142879486084
iteration 881: loss 2.3014280796051025
iteration 882: loss 2.3014273643493652
iteration 883: loss 2.3014259338378906
iteration 884: loss 2.3014252185821533
iteration 885: loss 2.301424026489258
iteration 886: loss 2.301422595977783
iteration 887: loss 2.301421642303467
iteration 888: loss 2.3014204502105713
iteration 889: loss 2.301419496536255
iteration 890: loss 2.3014187812805176
iteration 891: loss 2.301417589187622
iteration 892: loss 2.3014166355133057
iteration 893: loss 2.3014156818389893
iteration 894: loss 2.3014144897460938
iteration 895: loss 2.301413059234619
iteration 896: loss 2.3014118671417236
iteration 897: loss 2.3014109134674072
iteration 898: loss 2.3014092445373535
iteration 899: loss 2.301408290863037
iteration 900: loss 2.3014073371887207
iteration 901: loss 2.301405906677246
iteration 902: loss 2.301405429840088
iteration 903: loss 2.301403760910034
iteration 904: loss 2.3014028072357178
iteration 905: loss 2.3014018535614014
iteration 906: loss 2.3014001846313477
iteration 907: loss 2.3013992309570312
iteration 908: loss 2.3013978004455566
iteration 909: loss 2.3013968467712402
iteration 910: loss 2.3013954162597656
iteration 911: loss 2.301394462585449
iteration 912: loss 2.3013927936553955
iteration 913: loss 2.301392078399658
iteration 914: loss 2.3013904094696045
iteration 915: loss 2.301389455795288
iteration 916: loss 2.3013882637023926
iteration 917: loss 2.301386833190918
iteration 918: loss 2.3013854026794434
iteration 919: loss 2.301384210586548
iteration 920: loss 2.3013827800750732
iteration 921: loss 2.3013811111450195
iteration 922: loss 2.301380157470703
iteration 923: loss 2.3013784885406494
iteration 924: loss 2.301377296447754
iteration 925: loss 2.3013761043548584
iteration 926: loss 2.301374673843384
iteration 927: loss 2.3013734817504883
iteration 928: loss 2.3013720512390137
iteration 929: loss 2.30137038230896
iteration 930: loss 2.3013694286346436
iteration 931: loss 2.30136775970459
iteration 932: loss 2.3013665676116943
iteration 933: loss 2.3013651371002197
iteration 934: loss 2.301363945007324
iteration 935: loss 2.3013625144958496
iteration 936: loss 2.301361083984375
iteration 937: loss 2.3013596534729004
iteration 938: loss 2.301358222961426
iteration 939: loss 2.301356554031372
iteration 940: loss 2.3013551235198975
iteration 941: loss 2.3013534545898438
iteration 942: loss 2.3013525009155273
iteration 943: loss 2.3013505935668945
iteration 944: loss 2.30134916305542
iteration 945: loss 2.3013479709625244
iteration 946: loss 2.3013463020324707
iteration 947: loss 2.301344633102417
iteration 948: loss 2.301342725753784
iteration 949: loss 2.3013412952423096
iteration 950: loss 2.301339626312256
iteration 951: loss 2.3013381958007812
iteration 952: loss 2.3013365268707275
iteration 953: loss 2.3013346195220947
iteration 954: loss 2.301333427429199
iteration 955: loss 2.3013315200805664
iteration 956: loss 2.301330089569092
iteration 957: loss 2.301328182220459
iteration 958: loss 2.3013267517089844
iteration 959: loss 2.3013250827789307
iteration 960: loss 2.301323175430298
iteration 961: loss 2.3013217449188232
iteration 962: loss 2.3013198375701904
iteration 963: loss 2.3013179302215576
iteration 964: loss 2.301316261291504
iteration 965: loss 2.30131459236145
iteration 966: loss 2.3013129234313965
iteration 967: loss 2.3013110160827637
iteration 968: loss 2.3013088703155518
iteration 969: loss 2.301306962966919
iteration 970: loss 2.3013052940368652
iteration 971: loss 2.3013036251068115
iteration 972: loss 2.301301956176758
iteration 973: loss 2.301300048828125
iteration 974: loss 2.301298141479492
iteration 975: loss 2.3012962341308594
iteration 976: loss 2.3012943267822266
iteration 977: loss 2.3012921810150146
iteration 978: loss 2.301290273666382
iteration 979: loss 2.301288604736328
iteration 980: loss 2.3012866973876953
iteration 981: loss 2.3012843132019043
iteration 982: loss 2.3012824058532715
iteration 983: loss 2.3012804985046387
iteration 984: loss 2.3012781143188477
iteration 985: loss 2.301276206970215
iteration 986: loss 2.301274299621582
iteration 987: loss 2.301272392272949
iteration 988: loss 2.301270008087158
iteration 989: loss 2.3012678623199463
iteration 990: loss 2.3012659549713135
iteration 991: loss 2.3012638092041016
iteration 992: loss 2.3012616634368896
iteration 993: loss 2.3012592792510986
iteration 994: loss 2.3012568950653076
iteration 995: loss 2.301255226135254
iteration 996: loss 2.301252603530884
iteration 997: loss 2.301250457763672
iteration 998: loss 2.30124831199646
iteration 999: loss 2.30124568939209
iteration 1000: loss 2.301243782043457
iteration 1001: loss 2.301241159439087
iteration 1002: loss 2.301239013671875
iteration 1003: loss 2.301236629486084
iteration 1004: loss 2.301234483718872
iteration 1005: loss 2.301231861114502
iteration 1006: loss 2.30122971534729
iteration 1007: loss 2.30122709274292
iteration 1008: loss 2.301224708557129
iteration 1009: loss 2.3012218475341797
iteration 1010: loss 2.3012197017669678
iteration 1011: loss 2.3012170791625977
iteration 1012: loss 2.3012144565582275
iteration 1013: loss 2.3012118339538574
iteration 1014: loss 2.3012092113494873
iteration 1015: loss 2.301206588745117
iteration 1016: loss 2.301203966140747
iteration 1017: loss 2.301201820373535
iteration 1018: loss 2.301199197769165
iteration 1019: loss 2.301196575164795
iteration 1020: loss 2.301193952560425
iteration 1021: loss 2.3011908531188965
iteration 1022: loss 2.3011882305145264
iteration 1023: loss 2.3011856079101562
iteration 1024: loss 2.301182746887207
iteration 1025: loss 2.301180124282837
iteration 1026: loss 2.3011770248413086
iteration 1027: loss 2.3011739253997803
iteration 1028: loss 2.30117130279541
iteration 1029: loss 2.301168203353882
iteration 1030: loss 2.3011651039123535
iteration 1031: loss 2.301162004470825
iteration 1032: loss 2.301159143447876
iteration 1033: loss 2.3011560440063477
iteration 1034: loss 2.3011531829833984
iteration 1035: loss 2.301149845123291
iteration 1036: loss 2.301146984100342
iteration 1037: loss 2.3011438846588135
iteration 1038: loss 2.3011410236358643
iteration 1039: loss 2.301137685775757
iteration 1040: loss 2.3011341094970703
iteration 1041: loss 2.301131010055542
iteration 1042: loss 2.3011276721954346
iteration 1043: loss 2.3011245727539062
iteration 1044: loss 2.3011209964752197
iteration 1045: loss 2.301117181777954
iteration 1046: loss 2.301114082336426
iteration 1047: loss 2.3011105060577393
iteration 1048: loss 2.30110764503479
iteration 1049: loss 2.3011035919189453
iteration 1050: loss 2.301100254058838
iteration 1051: loss 2.3010966777801514
iteration 1052: loss 2.301093101501465
iteration 1053: loss 2.3010895252227783
iteration 1054: loss 2.3010857105255127
iteration 1055: loss 2.301081895828247
iteration 1056: loss 2.3010780811309814
iteration 1057: loss 2.301074504852295
iteration 1058: loss 2.3010706901550293
iteration 1059: loss 2.3010668754577637
iteration 1060: loss 2.301062822341919
iteration 1061: loss 2.3010590076446533
iteration 1062: loss 2.3010549545288086
iteration 1063: loss 2.301051378250122
iteration 1064: loss 2.3010473251342773
iteration 1065: loss 2.3010432720184326
iteration 1066: loss 2.3010387420654297
iteration 1067: loss 2.301034927368164
iteration 1068: loss 2.301030397415161
iteration 1069: loss 2.3010265827178955
iteration 1070: loss 2.3010220527648926
iteration 1071: loss 2.301017999649048
iteration 1072: loss 2.301013231277466
iteration 1073: loss 2.301009178161621
iteration 1074: loss 2.301004648208618
iteration 1075: loss 2.300999879837036
iteration 1076: loss 2.300995349884033
iteration 1077: loss 2.300990581512451
iteration 1078: loss 2.300985813140869
iteration 1079: loss 2.3009815216064453
iteration 1080: loss 2.3009767532348633
iteration 1081: loss 2.300971746444702
iteration 1082: loss 2.300967216491699
iteration 1083: loss 2.300962209701538
iteration 1084: loss 2.300957441329956
iteration 1085: loss 2.300952434539795
iteration 1086: loss 2.3009471893310547
iteration 1087: loss 2.3009421825408936
iteration 1088: loss 2.3009371757507324
iteration 1089: loss 2.300931930541992
iteration 1090: loss 2.300926685333252
iteration 1091: loss 2.3009212017059326
iteration 1092: loss 2.3009157180786133
iteration 1093: loss 2.300910234451294
iteration 1094: loss 2.3009049892425537
iteration 1095: loss 2.3008992671966553
iteration 1096: loss 2.300893545150757
iteration 1097: loss 2.3008878231048584
iteration 1098: loss 2.300881862640381
iteration 1099: loss 2.3008763790130615
iteration 1100: loss 2.300870418548584
iteration 1101: loss 2.3008649349212646
iteration 1102: loss 2.300858736038208
iteration 1103: loss 2.3008527755737305
iteration 1104: loss 2.3008463382720947
iteration 1105: loss 2.300840377807617
iteration 1106: loss 2.3008339405059814
iteration 1107: loss 2.3008275032043457
iteration 1108: loss 2.300820827484131
iteration 1109: loss 2.300814390182495
iteration 1110: loss 2.3008077144622803
iteration 1111: loss 2.3008012771606445
iteration 1112: loss 2.3007946014404297
iteration 1113: loss 2.3007876873016357
iteration 1114: loss 2.300780773162842
iteration 1115: loss 2.3007736206054688
iteration 1116: loss 2.300766706466675
iteration 1117: loss 2.3007595539093018
iteration 1118: loss 2.3007524013519287
iteration 1119: loss 2.3007445335388184
iteration 1120: loss 2.3007373809814453
iteration 1121: loss 2.300729990005493
iteration 1122: loss 2.300722360610962
iteration 1123: loss 2.3007144927978516
iteration 1124: loss 2.3007068634033203
iteration 1125: loss 2.3006985187530518
iteration 1126: loss 2.3006908893585205
iteration 1127: loss 2.300682783126831
iteration 1128: loss 2.3006746768951416
iteration 1129: loss 2.300666332244873
iteration 1130: loss 2.3006577491760254
iteration 1131: loss 2.3006491661071777
iteration 1132: loss 2.300640821456909
iteration 1133: loss 2.3006319999694824
iteration 1134: loss 2.3006229400634766
iteration 1135: loss 2.30061411857605
iteration 1136: loss 2.300605058670044
iteration 1137: loss 2.300596237182617
iteration 1138: loss 2.300586462020874
iteration 1139: loss 2.300577163696289
iteration 1140: loss 2.300567626953125
iteration 1141: loss 2.300557851791382
iteration 1142: loss 2.3005480766296387
iteration 1143: loss 2.3005380630493164
iteration 1144: loss 2.3005282878875732
iteration 1145: loss 2.300518035888672
iteration 1146: loss 2.3005075454711914
iteration 1147: loss 2.300497055053711
iteration 1148: loss 2.3004865646362305
iteration 1149: loss 2.300475597381592
iteration 1150: loss 2.300464630126953
iteration 1151: loss 2.3004534244537354
iteration 1152: loss 2.3004422187805176
iteration 1153: loss 2.3004310131073
iteration 1154: loss 2.3004190921783447
iteration 1155: loss 2.3004071712493896
iteration 1156: loss 2.3003954887390137
iteration 1157: loss 2.3003835678100586
iteration 1158: loss 2.300370931625366
iteration 1159: loss 2.300358533859253
iteration 1160: loss 2.3003463745117188
iteration 1161: loss 2.300333261489868
iteration 1162: loss 2.3003199100494385
iteration 1163: loss 2.300307035446167
iteration 1164: loss 2.300293445587158
iteration 1165: loss 2.3002800941467285
iteration 1166: loss 2.3002662658691406
iteration 1167: loss 2.3002521991729736
iteration 1168: loss 2.3002381324768066
iteration 1169: loss 2.3002233505249023
iteration 1170: loss 2.3002090454101562
iteration 1171: loss 2.300194025039673
iteration 1172: loss 2.3001787662506104
iteration 1173: loss 2.300163984298706
iteration 1174: loss 2.3001480102539062
iteration 1175: loss 2.3001320362091064
iteration 1176: loss 2.3001158237457275
iteration 1177: loss 2.3000996112823486
iteration 1178: loss 2.3000829219818115
iteration 1179: loss 2.3000662326812744
iteration 1180: loss 2.300049066543579
iteration 1181: loss 2.3000314235687256
iteration 1182: loss 2.300013542175293
iteration 1183: loss 2.2999956607818604
iteration 1184: loss 2.2999770641326904
iteration 1185: loss 2.2999584674835205
iteration 1186: loss 2.2999396324157715
iteration 1187: loss 2.2999203205108643
iteration 1188: loss 2.299900770187378
iteration 1189: loss 2.2998809814453125
iteration 1190: loss 2.2998602390289307
iteration 1191: loss 2.299839735031128
iteration 1192: loss 2.299818992614746
iteration 1193: loss 2.299797534942627
iteration 1194: loss 2.2997756004333496
iteration 1195: loss 2.299753427505493
iteration 1196: loss 2.2997307777404785
iteration 1197: loss 2.2997076511383057
iteration 1198: loss 2.299684762954712
iteration 1199: loss 2.2996609210968018
iteration 1200: loss 2.2996366024017334
iteration 1201: loss 2.299611806869507
iteration 1202: loss 2.299586296081543
iteration 1203: loss 2.299561023712158
iteration 1204: loss 2.299534797668457
iteration 1205: loss 2.2995080947875977
iteration 1206: loss 2.299481153488159
iteration 1207: loss 2.2994532585144043
iteration 1208: loss 2.2994251251220703
iteration 1209: loss 2.2993967533111572
iteration 1210: loss 2.299367666244507
iteration 1211: loss 2.29933762550354
iteration 1212: loss 2.299307107925415
iteration 1213: loss 2.299276113510132
iteration 1214: loss 2.2992444038391113
iteration 1215: loss 2.2992119789123535
iteration 1216: loss 2.2991790771484375
iteration 1217: loss 2.2991456985473633
iteration 1218: loss 2.2991111278533936
iteration 1219: loss 2.2990758419036865
iteration 1220: loss 2.2990403175354004
iteration 1221: loss 2.299003839492798
iteration 1222: loss 2.2989661693573
iteration 1223: loss 2.2989280223846436
iteration 1224: loss 2.298889398574829
iteration 1225: loss 2.29884934425354
iteration 1226: loss 2.2988088130950928
iteration 1227: loss 2.298766851425171
iteration 1228: loss 2.298724889755249
iteration 1229: loss 2.2986814975738525
iteration 1230: loss 2.2986366748809814
iteration 1231: loss 2.2985916137695312
iteration 1232: loss 2.2985453605651855
iteration 1233: loss 2.2984976768493652
iteration 1234: loss 2.2984490394592285
iteration 1235: loss 2.2983992099761963
iteration 1236: loss 2.2983479499816895
iteration 1237: loss 2.298295736312866
iteration 1238: loss 2.2982423305511475
iteration 1239: loss 2.2981879711151123
iteration 1240: loss 2.2981319427490234
iteration 1241: loss 2.29807448387146
iteration 1242: loss 2.298015832901001
iteration 1243: loss 2.297955274581909
iteration 1244: loss 2.2978928089141846
iteration 1245: loss 2.2978291511535645
iteration 1246: loss 2.2977635860443115
iteration 1247: loss 2.297696828842163
iteration 1248: loss 2.297628164291382
iteration 1249: loss 2.2975575923919678
iteration 1250: loss 2.2974853515625
iteration 1251: loss 2.2974116802215576
iteration 1252: loss 2.297335147857666
iteration 1253: loss 2.2972569465637207
iteration 1254: loss 2.2971770763397217
iteration 1255: loss 2.2970943450927734
iteration 1256: loss 2.2970099449157715
iteration 1257: loss 2.2969226837158203
iteration 1258: loss 2.296832799911499
iteration 1259: loss 2.296741008758545
iteration 1260: loss 2.2966463565826416
iteration 1261: loss 2.296548843383789
iteration 1262: loss 2.296447992324829
iteration 1263: loss 2.296344518661499
iteration 1264: loss 2.296238422393799
iteration 1265: loss 2.296128273010254
iteration 1266: loss 2.2960150241851807
iteration 1267: loss 2.295898675918579
iteration 1268: loss 2.2957777976989746
iteration 1269: loss 2.295654058456421
iteration 1270: loss 2.2955257892608643
iteration 1271: loss 2.295393705368042
iteration 1272: loss 2.295257091522217
iteration 1273: loss 2.2951159477233887
iteration 1274: loss 2.294970989227295
iteration 1275: loss 2.2948200702667236
iteration 1276: loss 2.2946646213531494
iteration 1277: loss 2.2945029735565186
iteration 1278: loss 2.2943363189697266
iteration 1279: loss 2.294163942337036
iteration 1280: loss 2.2939846515655518
iteration 1281: loss 2.2938003540039062
iteration 1282: loss 2.2936084270477295
iteration 1283: loss 2.293409585952759
iteration 1284: loss 2.293203830718994
iteration 1285: loss 2.292990207672119
iteration 1286: loss 2.2927682399749756
iteration 1287: loss 2.292538642883301
iteration 1288: loss 2.292299509048462
iteration 1289: loss 2.2920501232147217
iteration 1290: loss 2.291792154312134
iteration 1291: loss 2.2915232181549072
iteration 1292: loss 2.291243076324463
iteration 1293: loss 2.290952205657959
iteration 1294: loss 2.2906484603881836
iteration 1295: loss 2.290332078933716
iteration 1296: loss 2.290001630783081
iteration 1297: loss 2.2896575927734375
iteration 1298: loss 2.2892985343933105
iteration 1299: loss 2.2889232635498047
iteration 1300: loss 2.2885305881500244
iteration 1301: loss 2.2881202697753906
iteration 1302: loss 2.2876906394958496
iteration 1303: loss 2.2872414588928223
iteration 1304: loss 2.286770820617676
iteration 1305: loss 2.2862772941589355
iteration 1306: loss 2.285759449005127
iteration 1307: loss 2.2852163314819336
iteration 1308: loss 2.2846457958221436
iteration 1309: loss 2.284045696258545
iteration 1310: loss 2.283414363861084
iteration 1311: loss 2.282750368118286
iteration 1312: loss 2.282050848007202
iteration 1313: loss 2.2813141345977783
iteration 1314: loss 2.2805368900299072
iteration 1315: loss 2.2797157764434814
iteration 1316: loss 2.2788491249084473
iteration 1317: loss 2.277933120727539
iteration 1318: loss 2.276963710784912
iteration 1319: loss 2.275938034057617
iteration 1320: loss 2.2748515605926514
iteration 1321: loss 2.2736995220184326
iteration 1322: loss 2.2724766731262207
iteration 1323: loss 2.271179437637329
iteration 1324: loss 2.269801616668701
iteration 1325: loss 2.2683374881744385
iteration 1326: loss 2.2667810916900635
iteration 1327: loss 2.265125274658203
iteration 1328: loss 2.2633631229400635
iteration 1329: loss 2.2614872455596924
iteration 1330: loss 2.2594902515411377
iteration 1331: loss 2.257363796234131
iteration 1332: loss 2.2551002502441406
iteration 1333: loss 2.252690315246582
iteration 1334: loss 2.250126838684082
iteration 1335: loss 2.2474007606506348
iteration 1336: loss 2.2445034980773926
iteration 1337: loss 2.241429090499878
iteration 1338: loss 2.238170862197876
iteration 1339: loss 2.234722137451172
iteration 1340: loss 2.231079578399658
iteration 1341: loss 2.2272400856018066
iteration 1342: loss 2.22320294380188
iteration 1343: loss 2.21897029876709
iteration 1344: loss 2.2145445346832275
iteration 1345: loss 2.209932327270508
iteration 1346: loss 2.2051408290863037
iteration 1347: loss 2.2001793384552
iteration 1348: loss 2.1950595378875732
iteration 1349: loss 2.189791202545166
iteration 1350: loss 2.184384346008301
iteration 1351: loss 2.178849220275879
iteration 1352: loss 2.1731932163238525
iteration 1353: loss 2.1674251556396484
iteration 1354: loss 2.1615524291992188
iteration 1355: loss 2.155580759048462
iteration 1356: loss 2.1495180130004883
iteration 1357: loss 2.1433684825897217
iteration 1358: loss 2.1371350288391113
iteration 1359: loss 2.1308228969573975
iteration 1360: loss 2.124434471130371
iteration 1361: loss 2.1179728507995605
iteration 1362: loss 2.1114392280578613
iteration 1363: loss 2.1048336029052734
iteration 1364: loss 2.098151922225952
iteration 1365: loss 2.0913939476013184
iteration 1366: loss 2.084552526473999
iteration 1367: loss 2.077623128890991
iteration 1368: loss 2.07059907913208
iteration 1369: loss 2.0634748935699463
iteration 1370: loss 2.0562424659729004
iteration 1371: loss 2.04889178276062
iteration 1372: loss 2.0414113998413086
iteration 1373: loss 2.03379225730896
iteration 1374: loss 2.026024341583252
iteration 1375: loss 2.018097162246704
iteration 1376: loss 2.010000228881836
iteration 1377: loss 2.0017249584198
iteration 1378: loss 1.9932647943496704
iteration 1379: loss 1.9846166372299194
iteration 1380: loss 1.9757752418518066
iteration 1381: loss 1.966740608215332
iteration 1382: loss 1.957512617111206
iteration 1383: loss 1.9480929374694824
iteration 1384: loss 1.9384815692901611
iteration 1385: loss 1.928681492805481
iteration 1386: loss 1.9187079668045044
iteration 1387: loss 1.9085767269134521
iteration 1388: loss 1.898285150527954
iteration 1389: loss 1.887833595275879
iteration 1390: loss 1.8772265911102295
iteration 1391: loss 1.8664758205413818
iteration 1392: loss 1.8555961847305298
iteration 1393: loss 1.8446159362792969
iteration 1394: loss 1.833586573600769
iteration 1395: loss 1.822584867477417
iteration 1396: loss 1.8116754293441772
iteration 1397: loss 1.8009024858474731
iteration 1398: loss 1.7903269529342651
iteration 1399: loss 1.7799842357635498
iteration 1400: loss 1.7698863744735718
iteration 1401: loss 1.760021686553955
iteration 1402: loss 1.750365138053894
iteration 1403: loss 1.7408932447433472
iteration 1404: loss 1.7315901517868042
iteration 1405: loss 1.7224361896514893
iteration 1406: loss 1.7134140729904175
iteration 1407: loss 1.7045109272003174
iteration 1408: loss 1.6957511901855469
iteration 1409: loss 1.6872915029525757
iteration 1410: loss 1.6799134016036987
iteration 1411: loss 1.6774603128433228
iteration 1412: loss 1.699655532836914
iteration 1413: loss 1.8420759439468384
iteration 1414: loss 2.4312143325805664
iteration 1415: loss 3.1303224563598633
iteration 1416: loss 1.9439115524291992
iteration 1417: loss 1.7669581174850464
iteration 1418: loss 1.7318956851959229
iteration 1419: loss 1.7095947265625
iteration 1420: loss 1.6900733709335327
iteration 1421: loss 1.6718640327453613
iteration 1422: loss 1.6545525789260864
iteration 1423: loss 1.6381100416183472
iteration 1424: loss 1.6225156784057617
iteration 1425: loss 1.607714295387268
iteration 1426: loss 1.5935876369476318
iteration 1427: loss 1.5800126791000366
iteration 1428: loss 1.5668864250183105
iteration 1429: loss 1.554134726524353
iteration 1430: loss 1.5417007207870483
iteration 1431: loss 1.5296103954315186
iteration 1432: loss 1.518094539642334
iteration 1433: loss 1.5080667734146118
iteration 1434: loss 1.502881407737732
iteration 1435: loss 1.51578950881958
iteration 1436: loss 1.596106767654419
iteration 1437: loss 1.8915908336639404
iteration 1438: loss 2.4617435932159424
iteration 1439: loss 2.537707567214966
iteration 1440: loss 1.7836642265319824
iteration 1441: loss 1.5779515504837036
iteration 1442: loss 1.530234456062317
iteration 1443: loss 1.5002330541610718
iteration 1444: loss 1.4755889177322388
iteration 1445: loss 1.4548470973968506
iteration 1446: loss 1.4369866847991943
iteration 1447: loss 1.4213019609451294
iteration 1448: loss 1.4073400497436523
iteration 1449: loss 1.3947845697402954
iteration 1450: loss 1.3833897113800049
iteration 1451: loss 1.37294602394104
iteration 1452: loss 1.3632795810699463
iteration 1453: loss 1.3542338609695435
iteration 1454: loss 1.345711350440979
iteration 1455: loss 1.3376126289367676
iteration 1456: loss 1.329878807067871
iteration 1457: loss 1.3225101232528687
iteration 1458: loss 1.3157312870025635
iteration 1459: loss 1.3109378814697266
iteration 1460: loss 1.3154677152633667
iteration 1461: loss 1.3682526350021362
iteration 1462: loss 1.6076079607009888
iteration 1463: loss 2.3415114879608154
iteration 1464: loss 2.3384218215942383
iteration 1465: loss 1.6783555746078491
iteration 1466: loss 1.4556331634521484
iteration 1467: loss 1.3831120729446411
iteration 1468: loss 1.3515673875808716
iteration 1469: loss 1.332794427871704
iteration 1470: loss 1.318053126335144
iteration 1471: loss 1.3073437213897705
iteration 1472: loss 1.2996482849121094
iteration 1473: loss 1.2990620136260986
iteration 1474: loss 1.3086590766906738
iteration 1475: loss 1.3491965532302856
iteration 1476: loss 1.4113205671310425
iteration 1477: loss 1.5793559551239014
iteration 1478: loss 1.5151467323303223
iteration 1479: loss 1.6016077995300293
iteration 1480: loss 1.4079231023788452
iteration 1481: loss 1.3720654249191284
iteration 1482: loss 1.3276461362838745
iteration 1483: loss 1.340817928314209
iteration 1484: loss 1.3207632303237915
iteration 1485: loss 1.3555248975753784
iteration 1486: loss 1.3413351774215698
iteration 1487: loss 1.4064465761184692
iteration 1488: loss 1.3738123178482056
iteration 1489: loss 1.4476412534713745
iteration 1490: loss 1.3729108572006226
iteration 1491: loss 1.4107822179794312
iteration 1492: loss 1.3323016166687012
iteration 1493: loss 1.3546472787857056
iteration 1494: loss 1.3037135601043701
iteration 1495: loss 1.3340483903884888
iteration 1496: loss 1.3010135889053345
iteration 1497: loss 1.3488829135894775
iteration 1498: loss 1.3189829587936401
iteration 1499: loss 1.384795904159546
iteration 1500: loss 1.3375941514968872
iteration 1501: loss 1.397273302078247
iteration 1502: loss 1.324082851409912
iteration 1503: loss 1.3587743043899536
iteration 1504: loss 1.2899179458618164
iteration 1505: loss 1.318355679512024
iteration 1506: loss 1.271628975868225
iteration 1507: loss 1.3118085861206055
iteration 1508: loss 1.2806612253189087
iteration 1509: loss 1.3417818546295166
iteration 1510: loss 1.3089113235473633
iteration 1511: loss 1.3778630495071411
iteration 1512: loss 1.3176709413528442
iteration 1513: loss 1.3566819429397583
iteration 1514: loss 1.281079888343811
iteration 1515: loss 1.2916810512542725
iteration 1516: loss 1.2418439388275146
iteration 1517: loss 1.252327561378479
iteration 1518: loss 1.2327029705047607
iteration 1519: loss 1.2642202377319336
iteration 1520: loss 1.2725547552108765
iteration 1521: loss 1.3624076843261719
iteration 1522: loss 1.3746932744979858
iteration 1523: loss 1.5275564193725586
iteration 1524: loss 1.4080774784088135
iteration 1525: loss 1.4677923917770386
iteration 1526: loss 1.3595927953720093
iteration 1527: loss 1.3038593530654907
iteration 1528: loss 1.257074236869812
iteration 1529: loss 1.2105294466018677
iteration 1530: loss 1.1953201293945312
iteration 1531: loss 1.182431697845459
iteration 1532: loss 1.1950114965438843
iteration 1533: loss 1.211938500404358
iteration 1534: loss 1.2686768770217896
iteration 1535: loss 1.3122950792312622
iteration 1536: loss 1.4016876220703125
iteration 1537: loss 1.368032693862915
iteration 1538: loss 1.3678183555603027
iteration 1539: loss 1.2447824478149414
iteration 1540: loss 1.228447437286377
iteration 1541: loss 1.1952060461044312
iteration 1542: loss 1.2375108003616333
iteration 1543: loss 1.2879058122634888
iteration 1544: loss 1.4245426654815674
iteration 1545: loss 1.4492214918136597
iteration 1546: loss 1.464190125465393
iteration 1547: loss 1.2282531261444092
iteration 1548: loss 1.148552656173706
iteration 1549: loss 1.107853889465332
iteration 1550: loss 1.1083295345306396
iteration 1551: loss 1.1122465133666992
iteration 1552: loss 1.1733824014663696
iteration 1553: loss 1.2384929656982422
iteration 1554: loss 1.4323256015777588
iteration 1555: loss 1.4373440742492676
iteration 1556: loss 1.469925880432129
iteration 1557: loss 1.3084548711776733
iteration 1558: loss 1.1853411197662354
iteration 1559: loss 1.1433769464492798
iteration 1560: loss 1.1101069450378418
iteration 1561: loss 1.1181848049163818
iteration 1562: loss 1.1342567205429077
iteration 1563: loss 1.213066816329956
iteration 1564: loss 1.3186572790145874
iteration 1565: loss 1.523159146308899
iteration 1566: loss 1.4874881505966187
iteration 1567: loss 1.4269378185272217
iteration 1568: loss 1.1541606187820435
iteration 1569: loss 1.0865764617919922
iteration 1570: loss 1.047743320465088
iteration 1571: loss 1.0291006565093994
iteration 1572: loss 1.0162599086761475
iteration 1573: loss 1.013528823852539
iteration 1574: loss 1.0215941667556763
iteration 1575: loss 1.0743591785430908
iteration 1576: loss 1.1746283769607544
iteration 1577: loss 1.5390020608901978
iteration 1578: loss 1.5408508777618408
iteration 1579: loss 1.6715586185455322
iteration 1580: loss 1.1859787702560425
iteration 1581: loss 1.1103352308273315
iteration 1582: loss 1.0903035402297974
iteration 1583: loss 1.0753127336502075
iteration 1584: loss 1.0937076807022095
iteration 1585: loss 1.124013900756836
iteration 1586: loss 1.2178981304168701
iteration 1587: loss 1.3116048574447632
iteration 1588: loss 1.4729663133621216
iteration 1589: loss 1.3802094459533691
iteration 1590: loss 1.2747796773910522
iteration 1591: loss 1.0970525741577148
iteration 1592: loss 1.0406622886657715
iteration 1593: loss 1.0058131217956543
iteration 1594: loss 0.9923475384712219
iteration 1595: loss 0.9857542514801025
iteration 1596: loss 0.9957632422447205
iteration 1597: loss 1.0218496322631836
iteration 1598: loss 1.0930471420288086
iteration 1599: loss 1.2071843147277832
iteration 1600: loss 1.4187276363372803
iteration 1601: loss 1.4856739044189453
iteration 1602: loss 1.4785478115081787
iteration 1603: loss 1.1328892707824707
iteration 1604: loss 1.0177607536315918
iteration 1605: loss 0.9687259197235107
iteration 1606: loss 0.9468022584915161
iteration 1607: loss 0.9318750500679016
iteration 1608: loss 0.9229221343994141
iteration 1609: loss 0.9192218780517578
iteration 1610: loss 0.9274572134017944
iteration 1611: loss 0.9633592963218689
iteration 1612: loss 1.0931895971298218
iteration 1613: loss 1.4682891368865967
iteration 1614: loss 1.9329358339309692
iteration 1615: loss 1.749571442604065
iteration 1616: loss 1.1752995252609253
iteration 1617: loss 1.0328867435455322
iteration 1618: loss 0.9689544439315796
iteration 1619: loss 0.9410679936408997
iteration 1620: loss 0.9210481643676758
iteration 1621: loss 0.9072535634040833
iteration 1622: loss 0.8984637260437012
iteration 1623: loss 0.896884560585022
iteration 1624: loss 0.9046178460121155
iteration 1625: loss 0.9364292025566101
iteration 1626: loss 1.0053318738937378
iteration 1627: loss 1.176709532737732
iteration 1628: loss 1.3865315914154053
iteration 1629: loss 1.6711918115615845
iteration 1630: loss 1.3708064556121826
iteration 1631: loss 1.1530590057373047
iteration 1632: loss 0.9816866517066956
iteration 1633: loss 0.9446147680282593
iteration 1634: loss 0.9252739548683167
iteration 1635: loss 0.926606297492981
iteration 1636: loss 0.9395180344581604
iteration 1637: loss 0.9816598892211914
iteration 1638: loss 1.0555663108825684
iteration 1639: loss 1.181702971458435
iteration 1640: loss 1.2816240787506104
iteration 1641: loss 1.31448495388031
iteration 1642: loss 1.1321556568145752
iteration 1643: loss 1.016247034072876
iteration 1644: loss 0.9258047938346863
iteration 1645: loss 0.8950363993644714
iteration 1646: loss 0.8726562261581421
iteration 1647: loss 0.8638083934783936
iteration 1648: loss 0.8583468198776245
iteration 1649: loss 0.8630240559577942
iteration 1650: loss 0.8732001185417175
iteration 1651: loss 0.9045966863632202
iteration 1652: loss 0.9463301301002502
iteration 1653: loss 1.0412696599960327
iteration 1654: loss 1.1038480997085571
iteration 1655: loss 1.2376105785369873
iteration 1656: loss 1.1473591327667236
iteration 1657: loss 1.1170923709869385
iteration 1658: loss 0.9526686072349548
iteration 1659: loss 0.9039956331253052
iteration 1660: loss 0.8596076369285583
iteration 1661: loss 0.8467656970024109
iteration 1662: loss 0.833961009979248
iteration 1663: loss 0.8337088227272034
iteration 1664: loss 0.8332006931304932
iteration 1665: loss 0.84806889295578
iteration 1666: loss 0.8621578812599182
iteration 1667: loss 0.9042013883590698
iteration 1668: loss 0.9322206377983093
iteration 1669: loss 0.9854388236999512
iteration 1670: loss 0.9869727492332458
iteration 1671: loss 0.9996005296707153
iteration 1672: loss 0.9489361643791199
iteration 1673: loss 0.9258518218994141
iteration 1674: loss 0.8763183355331421
iteration 1675: loss 0.8565275073051453
iteration 1676: loss 0.833588182926178
iteration 1677: loss 0.8260852098464966
iteration 1678: loss 0.8182376623153687
iteration 1679: loss 0.8184086680412292
iteration 1680: loss 0.8191685080528259
iteration 1681: loss 0.823608934879303
iteration 1682: loss 0.8293786644935608
iteration 1683: loss 0.8337756395339966
iteration 1684: loss 0.8392255902290344
iteration 1685: loss 0.8420872092247009
iteration 1686: loss 0.8425644040107727
iteration 1687: loss 0.8478463292121887
iteration 1688: loss 0.8421541452407837
iteration 1689: loss 0.8555241227149963
iteration 1690: loss 0.8448061943054199
iteration 1691: loss 0.8733354210853577
iteration 1692: loss 0.8603671789169312
iteration 1693: loss 0.9112231731414795
iteration 1694: loss 0.903186023235321
iteration 1695: loss 0.9727035164833069
iteration 1696: loss 0.9736104011535645
iteration 1697: loss 1.0103265047073364
iteration 1698: loss 0.9775252342224121
iteration 1699: loss 0.9311285614967346
iteration 1700: loss 0.8730637431144714
iteration 1701: loss 0.8250041604042053
iteration 1702: loss 0.7972466945648193
iteration 1703: loss 0.7759963870048523
iteration 1704: loss 0.7673431038856506
iteration 1705: loss 0.7597413063049316
iteration 1706: loss 0.7604641914367676
iteration 1707: loss 0.7603529691696167
iteration 1708: loss 0.7672410607337952
iteration 1709: loss 0.7715347409248352
iteration 1710: loss 0.7790966629981995
iteration 1711: loss 0.7822173833847046
iteration 1712: loss 0.7848256826400757
iteration 1713: loss 0.782844603061676
iteration 1714: loss 0.7812151312828064
iteration 1715: loss 0.7749150991439819
iteration 1716: loss 0.7709197402000427
iteration 1717: loss 0.7630342245101929
iteration 1718: loss 0.758105993270874
iteration 1719: loss 0.7510010004043579
iteration 1720: loss 0.7465887069702148
iteration 1721: loss 0.7412357926368713
iteration 1722: loss 0.738051176071167
iteration 1723: loss 0.7344434261322021
iteration 1724: loss 0.7328961491584778
iteration 1725: loss 0.730806827545166
iteration 1726: loss 0.7317073345184326
iteration 1727: loss 0.7317326068878174
iteration 1728: loss 0.7372269630432129
iteration 1729: loss 0.7434203028678894
iteration 1730: loss 0.7587945461273193
iteration 1731: loss 0.7865861654281616
iteration 1732: loss 0.8167043924331665
iteration 1733: loss 0.8982462286949158
iteration 1734: loss 0.8988832235336304
iteration 1735: loss 0.9902473092079163
iteration 1736: loss 0.8701327443122864
iteration 1737: loss 0.8584519028663635
iteration 1738: loss 0.7612422704696655
iteration 1739: loss 0.7385640740394592
iteration 1740: loss 0.706530749797821
iteration 1741: loss 0.6990360021591187
iteration 1742: loss 0.6886475086212158
iteration 1743: loss 0.6897699236869812
iteration 1744: loss 0.6887621879577637
iteration 1745: loss 0.6959137916564941
iteration 1746: loss 0.7003137469291687
iteration 1747: loss 0.7101072072982788
iteration 1748: loss 0.7141852974891663
iteration 1749: loss 0.7216261029243469
iteration 1750: loss 0.7191792130470276
iteration 1751: loss 0.723077654838562
iteration 1752: loss 0.7131689190864563
iteration 1753: loss 0.7154237627983093
iteration 1754: loss 0.7014756798744202
iteration 1755: loss 0.7039116621017456
iteration 1756: loss 0.6895466446876526
iteration 1757: loss 0.6933228969573975
iteration 1758: loss 0.6803045272827148
iteration 1759: loss 0.6857371926307678
iteration 1760: loss 0.6739503145217896
iteration 1761: loss 0.6809195280075073
iteration 1762: loss 0.6696422696113586
iteration 1763: loss 0.6776731610298157
iteration 1764: loss 0.666233479976654
iteration 1765: loss 0.6748992204666138
iteration 1766: loss 0.6627053618431091
iteration 1767: loss 0.6715268492698669
iteration 1768: loss 0.6584766507148743
iteration 1769: loss 0.6671521067619324
iteration 1770: loss 0.6534313559532166
iteration 1771: loss 0.6617027521133423
iteration 1772: loss 0.6476355195045471
iteration 1773: loss 0.655498206615448
iteration 1774: loss 0.6415624618530273
iteration 1775: loss 0.6490435004234314
iteration 1776: loss 0.6355366110801697
iteration 1777: loss 0.6427783966064453
iteration 1778: loss 0.629775881767273
iteration 1779: loss 0.6368055939674377
iteration 1780: loss 0.6243454217910767
iteration 1781: loss 0.6311957836151123
iteration 1782: loss 0.6192310452461243
iteration 1783: loss 0.6258544921875
iteration 1784: loss 0.6142577528953552
iteration 1785: loss 0.620631992816925
iteration 1786: loss 0.6093087196350098
iteration 1787: loss 0.6153774261474609
iteration 1788: loss 0.6043069362640381
iteration 1789: loss 0.6099429726600647
iteration 1790: loss 0.5990833640098572
iteration 1791: loss 0.6043033003807068
iteration 1792: loss 0.5938052535057068
iteration 1793: loss 0.5987339019775391
iteration 1794: loss 0.5884550213813782
iteration 1795: loss 0.5930951833724976
iteration 1796: loss 0.5832061171531677
iteration 1797: loss 0.5876362919807434
iteration 1798: loss 0.5780390501022339
iteration 1799: loss 0.5824342966079712
iteration 1800: loss 0.5730929374694824
iteration 1801: loss 0.5774459838867188
iteration 1802: loss 0.5682654976844788
iteration 1803: loss 0.572638750076294
iteration 1804: loss 0.5635725855827332
iteration 1805: loss 0.5679860711097717
iteration 1806: loss 0.5589897036552429
iteration 1807: loss 0.5634717345237732
iteration 1808: loss 0.554562509059906
iteration 1809: loss 0.5591393709182739
iteration 1810: loss 0.5502719283103943
iteration 1811: loss 0.5549027323722839
iteration 1812: loss 0.5461652874946594
iteration 1813: loss 0.5508084893226624
iteration 1814: loss 0.5422269105911255
iteration 1815: loss 0.5467864871025085
iteration 1816: loss 0.5385467410087585
iteration 1817: loss 0.5429599285125732
iteration 1818: loss 0.5352584719657898
iteration 1819: loss 0.5392764210700989
iteration 1820: loss 0.5323246121406555
iteration 1821: loss 0.5357615947723389
iteration 1822: loss 0.5298415422439575
iteration 1823: loss 0.5324104428291321
iteration 1824: loss 0.5276707410812378
iteration 1825: loss 0.529065728187561
iteration 1826: loss 0.5256360173225403
iteration 1827: loss 0.5254864692687988
iteration 1828: loss 0.5232399106025696
iteration 1829: loss 0.5213566422462463
iteration 1830: loss 0.5198648571968079
iteration 1831: loss 0.516162633895874
iteration 1832: loss 0.514718770980835
iteration 1833: loss 0.5092964768409729
iteration 1834: loss 0.507318377494812
iteration 1835: loss 0.5008174777030945
iteration 1836: loss 0.4981461763381958
iteration 1837: loss 0.4913743734359741
iteration 1838: loss 0.48825597763061523
iteration 1839: loss 0.4819384813308716
iteration 1840: loss 0.4787426292896271
iteration 1841: loss 0.47322866320610046
iteration 1842: loss 0.4702393412590027
iteration 1843: loss 0.4655906856060028
iteration 1844: loss 0.4629305899143219
iteration 1845: loss 0.45906803011894226
iteration 1846: loss 0.4567646384239197
iteration 1847: loss 0.45360663533210754
iteration 1848: loss 0.4516405761241913
iteration 1849: loss 0.4490630030632019
iteration 1850: loss 0.4474375247955322
iteration 1851: loss 0.4453507959842682
iteration 1852: loss 0.4440646171569824
iteration 1853: loss 0.44255316257476807
iteration 1854: loss 0.44170790910720825
iteration 1855: loss 0.44088131189346313
iteration 1856: loss 0.44063079357147217
iteration 1857: loss 0.4409111440181732
iteration 1858: loss 0.4415932893753052
iteration 1859: loss 0.443774551153183
iteration 1860: loss 0.44598251581192017
iteration 1861: loss 0.45163583755493164
iteration 1862: loss 0.4561251997947693
iteration 1863: loss 0.4679659307003021
iteration 1864: loss 0.47550809383392334
iteration 1865: loss 0.497612863779068
iteration 1866: loss 0.5068525075912476
iteration 1867: loss 0.5411157011985779
iteration 1868: loss 0.5417516231536865
iteration 1869: loss 0.5730615854263306
iteration 1870: loss 0.5436999797821045
iteration 1871: loss 0.5424426198005676
iteration 1872: loss 0.4940079152584076
iteration 1873: loss 0.4732213616371155
iteration 1874: loss 0.44385460019111633
iteration 1875: loss 0.43069422245025635
iteration 1876: loss 0.4196281433105469
iteration 1877: loss 0.4138961136341095
iteration 1878: loss 0.4094347655773163
iteration 1879: loss 0.40653708577156067
iteration 1880: loss 0.4041256010532379
iteration 1881: loss 0.4022311568260193
iteration 1882: loss 0.4005299210548401
iteration 1883: loss 0.39903533458709717
iteration 1884: loss 0.3976297378540039
iteration 1885: loss 0.3963285982608795
iteration 1886: loss 0.3950812518596649
iteration 1887: loss 0.3938898742198944
iteration 1888: loss 0.3927348256111145
iteration 1889: loss 0.39161768555641174
iteration 1890: loss 0.39052748680114746
iteration 1891: loss 0.38946595788002014
iteration 1892: loss 0.38842663168907166
iteration 1893: loss 0.38741227984428406
iteration 1894: loss 0.38641008734703064
iteration 1895: loss 0.3854329288005829
iteration 1896: loss 0.3844710886478424
iteration 1897: loss 0.3835287094116211
iteration 1898: loss 0.382603257894516
iteration 1899: loss 0.3816991448402405
iteration 1900: loss 0.38081490993499756
iteration 1901: loss 0.3799547553062439
iteration 1902: loss 0.37912124395370483
iteration 1903: loss 0.3783182203769684
iteration 1904: loss 0.3775460422039032
iteration 1905: loss 0.37682783603668213
iteration 1906: loss 0.3761451244354248
iteration 1907: loss 0.3755470812320709
iteration 1908: loss 0.37500646710395813
iteration 1909: loss 0.3746008574962616
iteration 1910: loss 0.3742952346801758
iteration 1911: loss 0.37421634793281555
iteration 1912: loss 0.37429991364479065
iteration 1913: loss 0.3747744560241699
iteration 1914: loss 0.37549689412117004
iteration 1915: loss 0.3769475817680359
iteration 1916: loss 0.37874528765678406
iteration 1917: loss 0.3819112777709961
iteration 1918: loss 0.3855281174182892
iteration 1919: loss 0.3915979862213135
iteration 1920: loss 0.3979530334472656
iteration 1921: loss 0.40883249044418335
iteration 1922: loss 0.4185659885406494
iteration 1923: loss 0.4361185133457184
iteration 1924: loss 0.44698888063430786
iteration 1925: loss 0.46891114115715027
iteration 1926: loss 0.470194935798645
iteration 1927: loss 0.48199599981307983
iteration 1928: loss 0.45835116505622864
iteration 1929: loss 0.4460028111934662
iteration 1930: loss 0.4123845100402832
iteration 1931: loss 0.39493101835250854
iteration 1932: loss 0.37741339206695557
iteration 1933: loss 0.3689914047718048
iteration 1934: loss 0.36278051137924194
iteration 1935: loss 0.35933250188827515
iteration 1936: loss 0.35677188634872437
iteration 1937: loss 0.35499709844589233
iteration 1938: loss 0.3535369038581848
iteration 1939: loss 0.35233476758003235
iteration 1940: loss 0.3512594699859619
iteration 1941: loss 0.35028040409088135
iteration 1942: loss 0.34936073422431946
iteration 1943: loss 0.3484916090965271
iteration 1944: loss 0.34765616059303284
iteration 1945: loss 0.346849262714386
iteration 1946: loss 0.34606054425239563
iteration 1947: loss 0.3452887535095215
iteration 1948: loss 0.34452909231185913
iteration 1949: loss 0.3437829911708832
iteration 1950: loss 0.3430466055870056
iteration 1951: loss 0.3423202931880951
iteration 1952: loss 0.3416016399860382
iteration 1953: loss 0.3408910632133484
iteration 1954: loss 0.3401893973350525
iteration 1955: loss 0.33949628472328186
iteration 1956: loss 0.3388093113899231
iteration 1957: loss 0.338129460811615
iteration 1958: loss 0.3374543786048889
iteration 1959: loss 0.33678439259529114
iteration 1960: loss 0.3361189067363739
iteration 1961: loss 0.33545902371406555
iteration 1962: loss 0.3348028063774109
iteration 1963: loss 0.33414995670318604
iteration 1964: loss 0.3335022032260895
iteration 1965: loss 0.332858681678772
iteration 1966: loss 0.33221906423568726
iteration 1967: loss 0.3315848410129547
iteration 1968: loss 0.3309550881385803
iteration 1969: loss 0.33033230900764465
iteration 1970: loss 0.32971322536468506
iteration 1971: loss 0.3291018307209015
iteration 1972: loss 0.328495055437088
iteration 1973: loss 0.3278970420360565
iteration 1974: loss 0.3273065388202667
iteration 1975: loss 0.3267260491847992
iteration 1976: loss 0.32615554332733154
iteration 1977: loss 0.3256012201309204
iteration 1978: loss 0.32506629824638367
iteration 1979: loss 0.3245522677898407
iteration 1980: loss 0.324070543050766
iteration 1981: loss 0.32361865043640137
iteration 1982: loss 0.32321977615356445
iteration 1983: loss 0.32287484407424927
iteration 1984: loss 0.32261398434638977
iteration 1985: loss 0.32245805859565735
iteration 1986: loss 0.3224487900733948
iteration 1987: loss 0.3226263225078583
iteration 1988: loss 0.32305383682250977
iteration 1989: loss 0.3238315284252167
iteration 1990: loss 0.32503536343574524
iteration 1991: loss 0.32687509059906006
iteration 1992: loss 0.32945629954338074
iteration 1993: loss 0.3331851065158844
iteration 1994: loss 0.3381134271621704
iteration 1995: loss 0.34507524967193604
iteration 1996: loss 0.3537694215774536
iteration 1997: loss 0.3660563826560974
iteration 1998: loss 0.379562109708786
iteration 1999: loss 0.3982676565647125
Accuracy:  0.8772 
Recall:  [(0.9846938775510204, 0.8968401486988847), (0.9400881057268723, 0.9870490286771508), (0.8536821705426356, 0.9483315392895587), (0.7792079207920792, 0.9621026894865525), (0.9541751527494908, 0.7900505902192243), (0.8710762331838565, 0.70508166969147), (0.9321503131524008, 0.8859126984126984), (0.8657587548638133, 0.96529284164859), (0.7813141683778234, 0.8271739130434783), (0.8067393458870169, 0.8496868475991649)] 
Matrix:
 [[ 965    0    0    1    4    3    6    0    0    1]
 [   0 1067    8    8    0    2    3    5   42    0]
 [   9    1  881   13    9   17   56    6   39    1]
 [   2    0   20  787    0  152    0    8   37    4]
 [   3    0    0    0  937    1   13    0    3   25]
 [  45    0    3    5   27  777   18    0    9    8]
 [  42    0    3    0   11    8  893    0    1    0]
 [   1   10   11    0   11    2    0  890   23   80]
 [   4    1    3    4   38  117   18    3  761   25]
 [   5    2    0    0  149   23    1   10    5  814]]
Architecture: [784, 100, 100, 100, 10]
Experiment done!



 FULL RESULTS
####################################################################################################
Architecture: [784, 10]	niter: 300	delta: 0.85
Accuracy:  0.9184 
Recall:  [(0.9795918367346939, 0.9467455621301775), (0.973568281938326, 0.9608695652173913), (0.8827519379844961, 0.9314928425357873), (0.9108910891089109, 0.9019607843137255), (0.9317718940936863, 0.9077380952380952), (0.8632286995515696, 0.9090909090909091), (0.9498956158663883, 0.9276248725790011), (0.914396887159533, 0.9233791748526523), (0.8788501026694046, 0.8708036622583927), (0.8889990089197225, 0.8961038961038961)] 
Matrix:
 [[ 960    0    2    2    0    4    9    1    2    0]
 [   0 1105    2    2    0    2    4    2   18    0]
 [   8    8  911   16   15    1   14   13   40    6]
 [   3    1   23  920    0   24    2   12   17    8]
 [   1    1    4    1  915    0   13    2    8   37]
 [  10    3    3   35    9  770   16    7   31    8]
 [  12    3    3    2   11   13  910    2    2    0]
 [   3   12   21    8    8    0    0  940    3   33]
 [   6   10    6   23    9   25   13   14  856   12]
 [  11    7    3   11   41    8    0   25    6  897]]
####################################################################################################
Architecture: [784, 100, 10]	niter: 360	delta: 0.5927039999999999
Accuracy:  0.9511 
Recall:  [(0.9836734693877551, 0.9620758483033932), (0.9814977973568282, 0.9754816112084063), (0.9486434108527132, 0.956989247311828), (0.9554455445544554, 0.9368932038834952), (0.9490835030549898, 0.9442755825734549), (0.929372197309417, 0.9506880733944955), (0.9624217118997912, 0.9495365602471678), (0.9426070038910506, 0.9528023598820059), (0.9291581108829569, 0.9427083333333334), (0.9236868186323092, 0.9357429718875502)] 
Matrix:
 [[ 964    0    1    1    0    4    6    1    2    1]
 [   0 1114    3    2    0    1    3    2   10    0]
 [   7    3  979    9    7    0    8    8    8    3]
 [   0    0    7  965    1   14    0   12    8    3]
 [   1    2    4    0  932    0   11    2    3   27]
 [   7    2    1   18    3  829   11    3   11    7]
 [   8    3    2    0    9    9  922    0    5    0]
 [   3    8   22    3    4    0    0  969    2   17]
 [   4    4    3   18    7   10    9    8  905    6]
 [   8    6    1   14   24    5    1   12    6  932]]
####################################################################################################
Architecture: [784, 100, 100, 10]	niter: 640	delta: 0.49787135999999993
Accuracy:  0.9584 
Recall:  [(0.9826530612244898, 0.9658976930792377), (0.9859030837004406, 0.9798598949211909), (0.9544573643410853, 0.9507722007722008), (0.9643564356435643, 0.945631067961165), (0.9582484725050916, 0.9533941236068896), (0.9394618834080718, 0.9555302166476625), (0.9634655532359081, 0.9574688796680498), (0.9484435797665369, 0.9701492537313433), (0.9476386036960985, 0.9505664263645726), (0.9345887016848364, 0.9515640766902119)] 
Matrix:
 [[ 963    0    0    1    0    8    5    1    2    0]
 [   0 1119    3    2    0    1    4    0    6    0]
 [   8    3  985    7    5    0    6    7   11    0]
 [   0    0   12  974    0    9    0    6    8    1]
 [   1    1    6    0  941    1    8    2    2   20]
 [   7    1    0   20    1  838    9    1   10    5]
 [   9    3    3    0    8    8  923    1    3    0]
 [   1    8   18    3    3    0    0  975    1   19]
 [   3    2    7   13    3    7    8    5  923    3]
 [   5    5    2   10   26    5    1    7    5  943]]
####################################################################################################
Architecture: [784, 100, 100, 100, 10]	niter: 2000	delta: 0.15
Accuracy:  0.8772 
Recall:  [(0.9846938775510204, 0.8968401486988847), (0.9400881057268723, 0.9870490286771508), (0.8536821705426356, 0.9483315392895587), (0.7792079207920792, 0.9621026894865525), (0.9541751527494908, 0.7900505902192243), (0.8710762331838565, 0.70508166969147), (0.9321503131524008, 0.8859126984126984), (0.8657587548638133, 0.96529284164859), (0.7813141683778234, 0.8271739130434783), (0.8067393458870169, 0.8496868475991649)] 
Matrix:
 [[ 965    0    0    1    4    3    6    0    0    1]
 [   0 1067    8    8    0    2    3    5   42    0]
 [   9    1  881   13    9   17   56    6   39    1]
 [   2    0   20  787    0  152    0    8   37    4]
 [   3    0    0    0  937    1   13    0    3   25]
 [  45    0    3    5   27  777   18    0    9    8]
 [  42    0    3    0   11    8  893    0    1    0]
 [   1   10   11    0   11    2    0  890   23   80]
 [   4    1    3    4   38  117   18    3  761   25]
 [   5    2    0    0  149   23    1   10    5  814]]
####################################################################################################
The most successful model architecture is:  [784, 100, 100, 10]
Showing most problematic image, probability for correct class:  0.39071217
iteration 0: loss 2.3025479316711426
Current accuracy on testset:  0.17829166666666665
iteration 1: loss 2.298879623413086
Current accuracy on testset:  0.22829166666666667
iteration 2: loss 2.2949249744415283
Current accuracy on testset:  0.27420833333333333
iteration 3: loss 2.290112018585205
Current accuracy on testset:  0.3257083333333333
iteration 4: loss 2.283846378326416
Current accuracy on testset:  0.3691041666666667
iteration 5: loss 2.2754945755004883
Current accuracy on testset:  0.3993333333333333
iteration 6: loss 2.264353036880493
Current accuracy on testset:  0.41625
iteration 7: loss 2.24945068359375
Current accuracy on testset:  0.4253125
iteration 8: loss 2.2295641899108887
Current accuracy on testset:  0.4288125
iteration 9: loss 2.203256368637085
Current accuracy on testset:  0.43185416666666665
iteration 10: loss 2.168958902359009
Current accuracy on testset:  0.4379166666666667
iteration 11: loss 2.1251847743988037
Current accuracy on testset:  0.450875
iteration 12: loss 2.0708396434783936
Current accuracy on testset:  0.47010416666666666
iteration 13: loss 2.0054941177368164
Current accuracy on testset:  0.4975833333333333
iteration 14: loss 1.929545521736145
Current accuracy on testset:  0.5305625
iteration 15: loss 1.8443087339401245
Current accuracy on testset:  0.5673958333333333
iteration 16: loss 1.7520626783370972
Current accuracy on testset:  0.6070208333333333
iteration 17: loss 1.6558252573013306
Current accuracy on testset:  0.6448333333333334
iteration 18: loss 1.5588469505310059
Current accuracy on testset:  0.6784791666666666
iteration 19: loss 1.4640772342681885
Current accuracy on testset:  0.7051875
iteration 20: loss 1.3738278150558472
Current accuracy on testset:  0.7270416666666667
iteration 21: loss 1.2896703481674194
Current accuracy on testset:  0.7428333333333333
iteration 22: loss 1.2124727964401245
Current accuracy on testset:  0.7561666666666667
iteration 23: loss 1.1424936056137085
Current accuracy on testset:  0.7661875
iteration 24: loss 1.0795408487319946
Current accuracy on testset:  0.7747916666666667
iteration 25: loss 1.0231305360794067
Current accuracy on testset:  0.7823958333333333
iteration 26: loss 0.9726426005363464
Current accuracy on testset:  0.7894791666666666
iteration 27: loss 0.9274290800094604
Current accuracy on testset:  0.7965208333333333
iteration 28: loss 0.8868743181228638
Current accuracy on testset:  0.8022083333333333
iteration 29: loss 0.8504185080528259
Current accuracy on testset:  0.80775
iteration 30: loss 0.8175629377365112
Current accuracy on testset:  0.813125
iteration 31: loss 0.7878681421279907
Current accuracy on testset:  0.8177291666666666
iteration 32: loss 0.7609484791755676
Current accuracy on testset:  0.8223125
iteration 33: loss 0.7364681363105774
Current accuracy on testset:  0.8264166666666667
iteration 34: loss 0.7141345739364624
Current accuracy on testset:  0.8304166666666667
iteration 35: loss 0.6936931014060974
Current accuracy on testset:  0.8339166666666666
iteration 36: loss 0.6749255657196045
Current accuracy on testset:  0.8366458333333333
iteration 37: loss 0.657651960849762
Current accuracy on testset:  0.8406875
iteration 38: loss 0.6417474746704102
Current accuracy on testset:  0.8434166666666667
iteration 39: loss 0.6272188425064087
Current accuracy on testset:  0.8450416666666667
iteration 40: loss 0.6144629716873169
Current accuracy on testset:  0.8468541666666667
iteration 41: loss 0.6051243543624878
Current accuracy on testset:  0.8362083333333333
iteration 42: loss 0.6056337356567383
Current accuracy on testset:  0.8113958333333333
iteration 43: loss 0.6328719258308411
Current accuracy on testset:  0.7541041666666667
iteration 44: loss 0.7436553835868835
Current accuracy on testset:  0.7010833333333333
iteration 45: loss 0.8453497290611267
Current accuracy on testset:  0.6705416666666667
iteration 46: loss 0.9372760653495789
Current accuracy on testset:  0.7005416666666666
iteration 47: loss 0.8194710612297058
Current accuracy on testset:  0.7039791666666667
iteration 48: loss 0.779352605342865
Current accuracy on testset:  0.7858125
iteration 49: loss 0.6543897986412048
Current accuracy on testset:  0.8055625
iteration 50: loss 0.5999823212623596
Current accuracy on testset:  0.840625
iteration 51: loss 0.5598835945129395
Current accuracy on testset:  0.8418125
iteration 52: loss 0.5384942293167114
Current accuracy on testset:  0.8598333333333333
iteration 53: loss 0.5216864943504333
Current accuracy on testset:  0.8557708333333334
iteration 54: loss 0.5105686187744141
Current accuracy on testset:  0.8671666666666666
iteration 55: loss 0.5006983876228333
Current accuracy on testset:  0.8615625
iteration 56: loss 0.4932139217853546
Current accuracy on testset:  0.8703541666666667
iteration 57: loss 0.4863293170928955
Current accuracy on testset:  0.8641458333333333
iteration 58: loss 0.4810314476490021
Current accuracy on testset:  0.8710625
iteration 59: loss 0.4763617217540741
Current accuracy on testset:  0.8642083333333334
iteration 60: loss 0.4731546938419342
Current accuracy on testset:  0.8679791666666666
iteration 61: loss 0.47108855843544006
Current accuracy on testset:  0.8613541666666666
iteration 62: loss 0.47032538056373596
Current accuracy on testset:  0.8601041666666667
iteration 63: loss 0.47175297141075134
Current accuracy on testset:  0.8545208333333333
iteration 64: loss 0.47308510541915894
Current accuracy on testset:  0.8490208333333333
iteration 65: loss 0.4780481159687042
Current accuracy on testset:  0.8480833333333333
iteration 66: loss 0.4784996211528778
Current accuracy on testset:  0.842125
iteration 67: loss 0.48413434624671936
Current accuracy on testset:  0.8458333333333333
iteration 68: loss 0.4795317053794861
Current accuracy on testset:  0.8427916666666667
iteration 69: loss 0.481867253780365
Current accuracy on testset:  0.8489791666666666
iteration 70: loss 0.47188615798950195
Current accuracy on testset:  0.8515
iteration 71: loss 0.4700462520122528
Current accuracy on testset:  0.8573125
iteration 72: loss 0.45700550079345703
Current accuracy on testset:  0.8632083333333334
iteration 73: loss 0.4523821473121643
Current accuracy on testset:  0.8673541666666666
iteration 74: loss 0.43957459926605225
Current accuracy on testset:  0.8725625
iteration 75: loss 0.43405449390411377
Current accuracy on testset:  0.8746875
iteration 76: loss 0.42362990975379944
Current accuracy on testset:  0.88
iteration 77: loss 0.41841837763786316
Current accuracy on testset:  0.8804166666666666
iteration 78: loss 0.4107176959514618
Current accuracy on testset:  0.8853125
iteration 79: loss 0.4063035547733307
Current accuracy on testset:  0.8842291666666666
iteration 80: loss 0.4006630480289459
Current accuracy on testset:  0.8882083333333334
iteration 81: loss 0.3969757556915283
Current accuracy on testset:  0.8869375
iteration 82: loss 0.3927083909511566
Current accuracy on testset:  0.8904791666666667
iteration 83: loss 0.3895733058452606
Current accuracy on testset:  0.8888125
iteration 84: loss 0.3861578106880188
Current accuracy on testset:  0.8919166666666667
iteration 85: loss 0.3834384083747864
Current accuracy on testset:  0.8908541666666666
iteration 86: loss 0.38057947158813477
Current accuracy on testset:  0.8930416666666666
iteration 87: loss 0.37816083431243896
Current accuracy on testset:  0.892375
iteration 88: loss 0.3756815493106842
Current accuracy on testset:  0.8937291666666667
iteration 89: loss 0.37348127365112305
Current accuracy on testset:  0.89325
iteration 90: loss 0.3712574541568756
Current accuracy on testset:  0.8945208333333333
iteration 91: loss 0.36923107504844666
Current accuracy on testset:  0.8941666666666667
iteration 92: loss 0.3671967089176178
Current accuracy on testset:  0.8955208333333333
iteration 93: loss 0.3653101623058319
Current accuracy on testset:  0.8948541666666666
iteration 94: loss 0.36342310905456543
Current accuracy on testset:  0.8960625
iteration 95: loss 0.3616528809070587
Current accuracy on testset:  0.8955416666666667
iteration 96: loss 0.3598811626434326
Current accuracy on testset:  0.8965416666666667
iteration 97: loss 0.35820645093917847
Current accuracy on testset:  0.8961458333333333
iteration 98: loss 0.3565385043621063
Current accuracy on testset:  0.897125
iteration 99: loss 0.3549465239048004
Current accuracy on testset:  0.896625
iteration 100: loss 0.35336753726005554
Current accuracy on testset:  0.8976875
iteration 101: loss 0.35185056924819946
Current accuracy on testset:  0.8972083333333334
iteration 102: loss 0.35034772753715515
Current accuracy on testset:  0.8981041666666667
iteration 103: loss 0.3488970994949341
Current accuracy on testset:  0.897625
iteration 104: loss 0.3474615812301636
Current accuracy on testset:  0.8986458333333334
iteration 105: loss 0.3460710346698761
Current accuracy on testset:  0.897875
iteration 106: loss 0.3446950316429138
Current accuracy on testset:  0.8988958333333333
iteration 107: loss 0.34335392713546753
Current accuracy on testset:  0.8983125
iteration 108: loss 0.34203317761421204
Current accuracy on testset:  0.8992083333333334
iteration 109: loss 0.34074607491493225
Current accuracy on testset:  0.8988958333333333
iteration 110: loss 0.33947888016700745
Current accuracy on testset:  0.8997083333333333
iteration 111: loss 0.33823248744010925
Current accuracy on testset:  0.8993125
iteration 112: loss 0.33701035380363464
Current accuracy on testset:  0.9000416666666666
iteration 113: loss 0.33580833673477173
Current accuracy on testset:  0.8997916666666667
iteration 114: loss 0.33463332056999207
Current accuracy on testset:  0.9003333333333333
iteration 115: loss 0.3334695100784302
Current accuracy on testset:  0.9003333333333333
iteration 116: loss 0.3323400914669037
Current accuracy on testset:  0.9007916666666667
iteration 117: loss 0.33120831847190857
Current accuracy on testset:  0.9008125
iteration 118: loss 0.3301166296005249
Current accuracy on testset:  0.901375
iteration 119: loss 0.3290192484855652
Current accuracy on testset:  0.9013333333333333
iteration 120: loss 0.3279610276222229
Current accuracy on testset:  0.9017083333333333
iteration 121: loss 0.32689404487609863
Current accuracy on testset:  0.9017291666666667
iteration 122: loss 0.32586750388145447
Current accuracy on testset:  0.9022291666666666
iteration 123: loss 0.3248225748538971
Current accuracy on testset:  0.9021666666666667
iteration 124: loss 0.32382258772850037
Current accuracy on testset:  0.9024375
iteration 125: loss 0.3228069841861725
Current accuracy on testset:  0.9023541666666667
iteration 126: loss 0.32183587551116943
Current accuracy on testset:  0.9026041666666667
iteration 127: loss 0.32084232568740845
Current accuracy on testset:  0.9026458333333334
iteration 128: loss 0.3198941648006439
Current accuracy on testset:  0.9028541666666666
iteration 129: loss 0.31891804933547974
Current accuracy on testset:  0.9027291666666667
iteration 130: loss 0.3179938495159149
Current accuracy on testset:  0.90325
iteration 131: loss 0.3170360028743744
Current accuracy on testset:  0.9031666666666667
iteration 132: loss 0.3161316215991974
Current accuracy on testset:  0.9037083333333333
iteration 133: loss 0.31519126892089844
Current accuracy on testset:  0.9034791666666667
iteration 134: loss 0.3143061399459839
Current accuracy on testset:  0.9040416666666666
iteration 135: loss 0.31338047981262207
Current accuracy on testset:  0.90375
iteration 136: loss 0.3125137686729431
Current accuracy on testset:  0.9044375
iteration 137: loss 0.31160223484039307
Current accuracy on testset:  0.904
iteration 138: loss 0.31075146794319153
Current accuracy on testset:  0.90475
iteration 139: loss 0.30984699726104736
Current accuracy on testset:  0.9043333333333333
iteration 140: loss 0.3090077340602875
Current accuracy on testset:  0.9050208333333334
iteration 141: loss 0.3081161677837372
Current accuracy on testset:  0.9046458333333334
iteration 142: loss 0.30728593468666077
Current accuracy on testset:  0.9052291666666666
iteration 143: loss 0.30640873312950134
Current accuracy on testset:  0.9050625
iteration 144: loss 0.30558302998542786
Current accuracy on testset:  0.9056458333333334
iteration 145: loss 0.3047206699848175
Current accuracy on testset:  0.9052916666666667
iteration 146: loss 0.3039039373397827
Current accuracy on testset:  0.9058333333333334
iteration 147: loss 0.3030545711517334
Current accuracy on testset:  0.9056041666666667
iteration 148: loss 0.3022546172142029
Current accuracy on testset:  0.9062916666666667
iteration 149: loss 0.301417738199234
Current accuracy on testset:  0.9059583333333333
iteration 150: loss 0.3006284236907959
Current accuracy on testset:  0.9067083333333333
iteration 151: loss 0.29980209469795227
Current accuracy on testset:  0.9063125
iteration 152: loss 0.29902222752571106
Current accuracy on testset:  0.9070625
iteration 153: loss 0.2982137203216553
Current accuracy on testset:  0.9064166666666666
iteration 154: loss 0.29744312167167664
Current accuracy on testset:  0.9074791666666666
iteration 155: loss 0.2966477572917938
Current accuracy on testset:  0.9067916666666667
iteration 156: loss 0.2958846688270569
Current accuracy on testset:  0.9079375
iteration 157: loss 0.29510271549224854
Current accuracy on testset:  0.9073958333333333
iteration 158: loss 0.2943531572818756
Current accuracy on testset:  0.9083333333333333
iteration 159: loss 0.2935832738876343
Current accuracy on testset:  0.9077916666666667
iteration 160: loss 0.29284369945526123
Current accuracy on testset:  0.9087083333333333
iteration 161: loss 0.2920854985713959
Current accuracy on testset:  0.9080416666666666
iteration 162: loss 0.2913546562194824
Current accuracy on testset:  0.9091458333333333
iteration 163: loss 0.2906126379966736
Current accuracy on testset:  0.908375
iteration 164: loss 0.2898921072483063
Current accuracy on testset:  0.9095208333333333
iteration 165: loss 0.28916460275650024
Current accuracy on testset:  0.9086666666666666
iteration 166: loss 0.28845450282096863
Current accuracy on testset:  0.9096666666666666
iteration 167: loss 0.2877413034439087
Current accuracy on testset:  0.9089791666666667
iteration 168: loss 0.28704357147216797
Current accuracy on testset:  0.9097916666666667
iteration 169: loss 0.28634417057037354
Current accuracy on testset:  0.9093125
iteration 170: loss 0.28565800189971924
Current accuracy on testset:  0.9099791666666667
iteration 171: loss 0.2849714159965515
Current accuracy on testset:  0.9094583333333334
iteration 172: loss 0.2842971980571747
Current accuracy on testset:  0.91025
iteration 173: loss 0.2836216688156128
Current accuracy on testset:  0.9096666666666666
iteration 174: loss 0.282958984375
Current accuracy on testset:  0.910375
iteration 175: loss 0.2822951078414917
Current accuracy on testset:  0.9100208333333333
iteration 176: loss 0.2816436290740967
Current accuracy on testset:  0.9106458333333334
iteration 177: loss 0.2809887230396271
Current accuracy on testset:  0.9104166666666667
iteration 178: loss 0.2803439497947693
Current accuracy on testset:  0.9107916666666667
iteration 179: loss 0.2796987295150757
Current accuracy on testset:  0.9107708333333333
iteration 180: loss 0.27906113862991333
Current accuracy on testset:  0.911125
iteration 181: loss 0.27843040227890015
Current accuracy on testset:  0.9110208333333333
iteration 182: loss 0.2778039574623108
Current accuracy on testset:  0.9113541666666667
iteration 183: loss 0.2771795094013214
Current accuracy on testset:  0.9111875
iteration 184: loss 0.2765643894672394
Current accuracy on testset:  0.9115625
iteration 185: loss 0.27594658732414246
Current accuracy on testset:  0.9113541666666667
iteration 186: loss 0.2753378748893738
Current accuracy on testset:  0.9117291666666667
iteration 187: loss 0.2747304141521454
Current accuracy on testset:  0.9115
iteration 188: loss 0.2741278111934662
Current accuracy on testset:  0.9118333333333334
iteration 189: loss 0.2735283672809601
Current accuracy on testset:  0.9117291666666667
iteration 190: loss 0.2729349732398987
Current accuracy on testset:  0.912
iteration 191: loss 0.27234604954719543
Current accuracy on testset:  0.9119583333333333
iteration 192: loss 0.27175918221473694
Current accuracy on testset:  0.9121666666666667
iteration 193: loss 0.2711784243583679
Current accuracy on testset:  0.91225
iteration 194: loss 0.2706012427806854
Current accuracy on testset:  0.9122291666666666
iteration 195: loss 0.2700284719467163
Current accuracy on testset:  0.9124791666666666
iteration 196: loss 0.2694588005542755
Current accuracy on testset:  0.9125
iteration 197: loss 0.26889362931251526
Current accuracy on testset:  0.9126666666666666
iteration 198: loss 0.2683316767215729
Current accuracy on testset:  0.9126666666666666
iteration 199: loss 0.267773300409317
Current accuracy on testset:  0.9128541666666666
iteration 200: loss 0.26721668243408203
Current accuracy on testset:  0.9129583333333333
iteration 201: loss 0.2666640877723694
Current accuracy on testset:  0.9128541666666666
iteration 202: loss 0.2661134898662567
Current accuracy on testset:  0.9131875
iteration 203: loss 0.2655663788318634
Current accuracy on testset:  0.913
iteration 204: loss 0.26502102613449097
Current accuracy on testset:  0.9135208333333333
iteration 205: loss 0.26447805762290955
Current accuracy on testset:  0.913125
iteration 206: loss 0.2639387845993042
Current accuracy on testset:  0.9137083333333333
iteration 207: loss 0.2634020745754242
Current accuracy on testset:  0.9134583333333334
iteration 208: loss 0.2628682553768158
Current accuracy on testset:  0.9138958333333334
iteration 209: loss 0.2623376250267029
Current accuracy on testset:  0.9137083333333333
iteration 210: loss 0.261811226606369
Current accuracy on testset:  0.9140833333333334
iteration 211: loss 0.26128673553466797
Current accuracy on testset:  0.9140208333333333
iteration 212: loss 0.2607661187648773
Current accuracy on testset:  0.9142291666666666
iteration 213: loss 0.26024678349494934
Current accuracy on testset:  0.9142083333333333
iteration 214: loss 0.2597300112247467
Current accuracy on testset:  0.9145
iteration 215: loss 0.25921493768692017
Current accuracy on testset:  0.9144375
iteration 216: loss 0.258701890707016
Current accuracy on testset:  0.9147708333333333
iteration 217: loss 0.2581915855407715
Current accuracy on testset:  0.9145833333333333
iteration 218: loss 0.25768446922302246
Current accuracy on testset:  0.9149583333333333
iteration 219: loss 0.25717929005622864
Current accuracy on testset:  0.9149166666666667
iteration 220: loss 0.25667616724967957
Current accuracy on testset:  0.915125
iteration 221: loss 0.25617486238479614
Current accuracy on testset:  0.9153125
iteration 222: loss 0.2556760609149933
Current accuracy on testset:  0.9153333333333333
iteration 223: loss 0.25517839193344116
Current accuracy on testset:  0.9154166666666667
iteration 224: loss 0.25468218326568604
Current accuracy on testset:  0.9154583333333334
iteration 225: loss 0.2541866600513458
Current accuracy on testset:  0.9155625
iteration 226: loss 0.25369328260421753
Current accuracy on testset:  0.9158125
iteration 227: loss 0.253200888633728
Current accuracy on testset:  0.9157083333333333
iteration 228: loss 0.25271016359329224
Current accuracy on testset:  0.9159166666666667
iteration 229: loss 0.25222134590148926
Current accuracy on testset:  0.9159375
iteration 230: loss 0.2517339587211609
Current accuracy on testset:  0.9161041666666667
iteration 231: loss 0.25124868750572205
Current accuracy on testset:  0.9160833333333334
iteration 232: loss 0.25076574087142944
Current accuracy on testset:  0.9161875
iteration 233: loss 0.25028470158576965
Current accuracy on testset:  0.9161875
iteration 234: loss 0.24980542063713074
Current accuracy on testset:  0.9163125
iteration 235: loss 0.24932768940925598
Current accuracy on testset:  0.9164375
iteration 236: loss 0.2488514482975006
Current accuracy on testset:  0.9163958333333333
iteration 237: loss 0.24837693572044373
Current accuracy on testset:  0.9165625
iteration 238: loss 0.24790456891059875
Current accuracy on testset:  0.9167083333333333
iteration 239: loss 0.24743369221687317
Current accuracy on testset:  0.9167291666666667
iteration 240: loss 0.2469649761915207
Current accuracy on testset:  0.9167916666666667
iteration 241: loss 0.24649809300899506
Current accuracy on testset:  0.917
iteration 242: loss 0.24603262543678284
Current accuracy on testset:  0.9170833333333334
iteration 243: loss 0.24556976556777954
Current accuracy on testset:  0.9171875
iteration 244: loss 0.24510805308818817
Current accuracy on testset:  0.9174166666666667
iteration 245: loss 0.24464759230613708
Current accuracy on testset:  0.9175208333333333
iteration 246: loss 0.24418878555297852
Current accuracy on testset:  0.917625
iteration 247: loss 0.24373136460781097
Current accuracy on testset:  0.9177708333333333
iteration 248: loss 0.24327419698238373
Current accuracy on testset:  0.917875
iteration 249: loss 0.24281825125217438
Current accuracy on testset:  0.9178541666666666
iteration 250: loss 0.24236468970775604
Current accuracy on testset:  0.9180208333333333
iteration 251: loss 0.24191240966320038
Current accuracy on testset:  0.9180208333333333
iteration 252: loss 0.24146178364753723
Current accuracy on testset:  0.918125
iteration 253: loss 0.24101251363754272
Current accuracy on testset:  0.9182916666666666
iteration 254: loss 0.24056464433670044
Current accuracy on testset:  0.9182916666666666
iteration 255: loss 0.2401190549135208
Current accuracy on testset:  0.9185625
iteration 256: loss 0.23967672884464264
Current accuracy on testset:  0.9185
iteration 257: loss 0.2392364740371704
Current accuracy on testset:  0.9187291666666667
iteration 258: loss 0.23879733681678772
Current accuracy on testset:  0.9188541666666666
iteration 259: loss 0.23835866153240204
Current accuracy on testset:  0.9189166666666667
iteration 260: loss 0.23792101442813873
Current accuracy on testset:  0.9192083333333333
iteration 261: loss 0.2374836653470993
Current accuracy on testset:  0.9192916666666666
iteration 262: loss 0.23704828321933746
Current accuracy on testset:  0.9193541666666667
iteration 263: loss 0.23661506175994873
Current accuracy on testset:  0.9193958333333333
iteration 264: loss 0.23618309199810028
Current accuracy on testset:  0.9195833333333333
iteration 265: loss 0.23575206100940704
Current accuracy on testset:  0.9196875
iteration 266: loss 0.23532164096832275
Current accuracy on testset:  0.9196666666666666
iteration 267: loss 0.2348928451538086
Current accuracy on testset:  0.91975
iteration 268: loss 0.23446537554264069
Current accuracy on testset:  0.9197916666666667
iteration 269: loss 0.23403941094875336
Current accuracy on testset:  0.9199166666666667
iteration 270: loss 0.23361481726169586
Current accuracy on testset:  0.9198958333333334
iteration 271: loss 0.23319128155708313
Current accuracy on testset:  0.9199791666666667
iteration 272: loss 0.23276908695697784
Current accuracy on testset:  0.9201041666666666
iteration 273: loss 0.2323485016822815
Current accuracy on testset:  0.9202083333333333
iteration 274: loss 0.23192936182022095
Current accuracy on testset:  0.9202708333333334
iteration 275: loss 0.2315121293067932
Current accuracy on testset:  0.9203333333333333
iteration 276: loss 0.23109683394432068
Current accuracy on testset:  0.9204375
iteration 277: loss 0.23068276047706604
Current accuracy on testset:  0.9204166666666667
iteration 278: loss 0.2302704155445099
Current accuracy on testset:  0.9205833333333333
iteration 279: loss 0.2298598289489746
Current accuracy on testset:  0.9205833333333333
iteration 280: loss 0.22945131361484528
Current accuracy on testset:  0.9207291666666667
iteration 281: loss 0.22904379665851593
Current accuracy on testset:  0.9207916666666667
iteration 282: loss 0.2286376804113388
Current accuracy on testset:  0.9208541666666666
iteration 283: loss 0.22823311388492584
Current accuracy on testset:  0.9209166666666667
iteration 284: loss 0.2278304547071457
Current accuracy on testset:  0.9211458333333333
iteration 285: loss 0.22742979228496552
Current accuracy on testset:  0.9212083333333333
iteration 286: loss 0.22702986001968384
Current accuracy on testset:  0.9213541666666667
iteration 287: loss 0.22663156688213348
Current accuracy on testset:  0.9213958333333333
iteration 288: loss 0.226234570145607
Current accuracy on testset:  0.9215208333333333
iteration 289: loss 0.2258385568857193
Current accuracy on testset:  0.9215416666666667
iteration 290: loss 0.2254437953233719
Current accuracy on testset:  0.921625
iteration 291: loss 0.225050151348114
Current accuracy on testset:  0.9216666666666666
iteration 292: loss 0.22465787827968597
Current accuracy on testset:  0.9216875
iteration 293: loss 0.22426724433898926
Current accuracy on testset:  0.9218125
iteration 294: loss 0.22387871146202087
Current accuracy on testset:  0.9218958333333334
iteration 295: loss 0.22349131107330322
Current accuracy on testset:  0.9219375
iteration 296: loss 0.22310423851013184
Current accuracy on testset:  0.9220208333333333
iteration 297: loss 0.22271813452243805
Current accuracy on testset:  0.922
iteration 298: loss 0.22233286499977112
Current accuracy on testset:  0.9220625
iteration 299: loss 0.22194863855838776
Current accuracy on testset:  0.9221041666666666
Accuracy:  0.9221041666666666 
Recall:  [(0.9656408094435076, 0.9563674321503132), (0.9690644800596347, 0.9468317552804079), (0.9005501481168007, 0.9148753224419605), (0.8849094567404426, 0.9139650872817955), (0.9264579759862779, 0.9160483358066568), (0.8780262854507724, 0.8853754940711462), (0.9526714345814051, 0.9359371771027072), (0.9491357043512816, 0.933919843597263), (0.8818982177367404, 0.900460425345319), (0.9020846493998737, 0.9074348654945986)] 
Matrix:
 [[4581    0   25    6   13   31   26    7   46    9]
 [   1 5200   32   30    3   22    3   16   51    8]
 [  32   49 4256   60   58   16   65   84   91   15]
 [   9   22  102 4398    1  213   23   60   97   45]
 [   8   22   30    3 4321    2   59   12   31  176]
 [  53   27   32  143   50 3808   87   19   69   49]
 [  29   12   45    2   56   55 4529    1   25    0]
 [  13   30   64   11   40    6    2 4777    9   81]
 [  35  100   53  110   21  119   42   16 4107   54]
 [  29   30   13   49  154   29    3  123   35 4284]]
iteration 0: loss 2.3033103942871094
iteration 1: loss 2.3029139041900635
iteration 2: loss 2.3028550148010254
iteration 3: loss 2.3021674156188965
iteration 4: loss 2.302502393722534
iteration 5: loss 2.3020901679992676
iteration 6: loss 2.3021774291992188
iteration 7: loss 2.301792860031128
iteration 8: loss 2.3015859127044678
iteration 9: loss 2.302014112472534
iteration 10: loss 2.301123857498169
iteration 11: loss 2.301388740539551
iteration 12: loss 2.301098346710205
iteration 13: loss 2.3005337715148926
iteration 14: loss 2.300704002380371
iteration 15: loss 2.300294876098633
iteration 16: loss 2.300657272338867
iteration 17: loss 2.2998616695404053
iteration 18: loss 2.2998733520507812
iteration 19: loss 2.299713611602783
iteration 20: loss 2.299927234649658
iteration 21: loss 2.299290418624878
iteration 22: loss 2.2992124557495117
iteration 23: loss 2.298841714859009
iteration 24: loss 2.2987263202667236
iteration 25: loss 2.2987940311431885
iteration 26: loss 2.2986552715301514
iteration 27: loss 2.29853892326355
iteration 28: loss 2.2977747917175293
iteration 29: loss 2.298062801361084
iteration 30: loss 2.297872543334961
iteration 31: loss 2.2974894046783447
iteration 32: loss 2.297664165496826
iteration 33: loss 2.296570301055908
iteration 34: loss 2.296443462371826
iteration 35: loss 2.2967216968536377
iteration 36: loss 2.2955658435821533
iteration 37: loss 2.2970428466796875
iteration 38: loss 2.2960205078125
iteration 39: loss 2.2959649562835693
iteration 40: loss 2.2961814403533936
iteration 41: loss 2.296229362487793
iteration 42: loss 2.294985055923462
iteration 43: loss 2.294928789138794
iteration 44: loss 2.294523000717163
iteration 45: loss 2.2957277297973633
iteration 46: loss 2.2953689098358154
iteration 47: loss 2.295106887817383
iteration 48: loss 2.2934296131134033
iteration 49: loss 2.293576955795288
iteration 50: loss 2.293034553527832
iteration 51: loss 2.2931675910949707
iteration 52: loss 2.2931065559387207
iteration 53: loss 2.2929091453552246
iteration 54: loss 2.2923755645751953
iteration 55: loss 2.292426586151123
iteration 56: loss 2.292558193206787
iteration 57: loss 2.2920453548431396
iteration 58: loss 2.29069447517395
iteration 59: loss 2.2910544872283936
iteration 60: loss 2.2903482913970947
iteration 61: loss 2.2900538444519043
iteration 62: loss 2.2897496223449707
iteration 63: loss 2.290142059326172
iteration 64: loss 2.2888216972351074
iteration 65: loss 2.2899341583251953
iteration 66: loss 2.290473699569702
iteration 67: loss 2.289058208465576
iteration 68: loss 2.2882182598114014
iteration 69: loss 2.2877066135406494
iteration 70: loss 2.287879228591919
iteration 71: loss 2.287799596786499
iteration 72: loss 2.2872469425201416
iteration 73: loss 2.2873291969299316
iteration 74: loss 2.285118341445923
iteration 75: loss 2.2875702381134033
iteration 76: loss 2.28444766998291
iteration 77: loss 2.285374164581299
iteration 78: loss 2.2849464416503906
iteration 79: loss 2.285428762435913
iteration 80: loss 2.284485340118408
iteration 81: loss 2.283379554748535
iteration 82: loss 2.2852747440338135
iteration 83: loss 2.281446695327759
iteration 84: loss 2.2827036380767822
iteration 85: loss 2.2843806743621826
iteration 86: loss 2.2816359996795654
iteration 87: loss 2.281073808670044
iteration 88: loss 2.280081033706665
iteration 89: loss 2.279606342315674
iteration 90: loss 2.2822766304016113
iteration 91: loss 2.281447649002075
iteration 92: loss 2.278576374053955
iteration 93: loss 2.278829574584961
iteration 94: loss 2.2793307304382324
iteration 95: loss 2.279825448989868
iteration 96: loss 2.277390718460083
iteration 97: loss 2.276463747024536
iteration 98: loss 2.2777087688446045
iteration 99: loss 2.275934934616089
iteration 100: loss 2.2744479179382324
iteration 101: loss 2.274653673171997
iteration 102: loss 2.2748329639434814
iteration 103: loss 2.275829553604126
iteration 104: loss 2.2722787857055664
iteration 105: loss 2.272731304168701
iteration 106: loss 2.273329734802246
iteration 107: loss 2.2703065872192383
iteration 108: loss 2.2718639373779297
iteration 109: loss 2.270197629928589
iteration 110: loss 2.270543098449707
iteration 111: loss 2.2687978744506836
iteration 112: loss 2.269037961959839
iteration 113: loss 2.2678866386413574
iteration 114: loss 2.269995927810669
iteration 115: loss 2.2660226821899414
iteration 116: loss 2.2667577266693115
iteration 117: loss 2.265807628631592
iteration 118: loss 2.2664849758148193
iteration 119: loss 2.2636771202087402
iteration 120: loss 2.2635886669158936
iteration 121: loss 2.2614939212799072
iteration 122: loss 2.2646572589874268
iteration 123: loss 2.2628753185272217
iteration 124: loss 2.262148857116699
iteration 125: loss 2.2596421241760254
iteration 126: loss 2.260831117630005
iteration 127: loss 2.2552833557128906
iteration 128: loss 2.259096145629883
iteration 129: loss 2.2594239711761475
iteration 130: loss 2.256021022796631
iteration 131: loss 2.2552237510681152
iteration 132: loss 2.2557830810546875
iteration 133: loss 2.255950927734375
iteration 134: loss 2.254776954650879
iteration 135: loss 2.2535645961761475
iteration 136: loss 2.2500743865966797
iteration 137: loss 2.244814157485962
iteration 138: loss 2.2475438117980957
iteration 139: loss 2.2470767498016357
iteration 140: loss 2.240173101425171
iteration 141: loss 2.245286464691162
iteration 142: loss 2.245985984802246
iteration 143: loss 2.2423255443573
iteration 144: loss 2.2399685382843018
iteration 145: loss 2.2404849529266357
iteration 146: loss 2.241481304168701
iteration 147: loss 2.241685390472412
iteration 148: loss 2.2385447025299072
iteration 149: loss 2.239811897277832
iteration 150: loss 2.2360410690307617
iteration 151: loss 2.234733819961548
iteration 152: loss 2.237978935241699
iteration 153: loss 2.229844093322754
iteration 154: loss 2.2328827381134033
iteration 155: loss 2.2344133853912354
iteration 156: loss 2.2264015674591064
iteration 157: loss 2.2297744750976562
iteration 158: loss 2.231673002243042
iteration 159: loss 2.2275266647338867
iteration 160: loss 2.227867603302002
iteration 161: loss 2.2236828804016113
iteration 162: loss 2.2190170288085938
iteration 163: loss 2.2228925228118896
iteration 164: loss 2.213409423828125
iteration 165: loss 2.2217509746551514
iteration 166: loss 2.216996192932129
iteration 167: loss 2.213700294494629
iteration 168: loss 2.2126142978668213
iteration 169: loss 2.214522123336792
iteration 170: loss 2.2068872451782227
iteration 171: loss 2.205968141555786
iteration 172: loss 2.2057912349700928
iteration 173: loss 2.206838846206665
iteration 174: loss 2.202756643295288
iteration 175: loss 2.2025604248046875
iteration 176: loss 2.1978988647460938
iteration 177: loss 2.20367169380188
iteration 178: loss 2.1926956176757812
iteration 179: loss 2.188722610473633
iteration 180: loss 2.1986913681030273
iteration 181: loss 2.192793846130371
iteration 182: loss 2.193906784057617
iteration 183: loss 2.2022321224212646
iteration 184: loss 2.1884422302246094
iteration 185: loss 2.1847195625305176
iteration 186: loss 2.1861298084259033
iteration 187: loss 2.1832642555236816
iteration 188: loss 2.1703858375549316
iteration 189: loss 2.1829028129577637
iteration 190: loss 2.1786949634552
iteration 191: loss 2.177460193634033
iteration 192: loss 2.169504165649414
iteration 193: loss 2.1680989265441895
iteration 194: loss 2.164666175842285
iteration 195: loss 2.167505979537964
iteration 196: loss 2.1603009700775146
iteration 197: loss 2.155747890472412
iteration 198: loss 2.1592869758605957
iteration 199: loss 2.14955735206604
iteration 200: loss 2.153519630432129
iteration 201: loss 2.153285264968872
iteration 202: loss 2.149169445037842
iteration 203: loss 2.1440346240997314
iteration 204: loss 2.1473517417907715
iteration 205: loss 2.146737813949585
iteration 206: loss 2.137237071990967
iteration 207: loss 2.131822347640991
iteration 208: loss 2.149113178253174
iteration 209: loss 2.135982036590576
iteration 210: loss 2.140259027481079
iteration 211: loss 2.1217756271362305
iteration 212: loss 2.1316628456115723
iteration 213: loss 2.130500078201294
iteration 214: loss 2.1203606128692627
iteration 215: loss 2.1249332427978516
iteration 216: loss 2.1293015480041504
iteration 217: loss 2.096879005432129
iteration 218: loss 2.1138179302215576
iteration 219: loss 2.097917079925537
iteration 220: loss 2.1022088527679443
iteration 221: loss 2.1135246753692627
iteration 222: loss 2.1107964515686035
iteration 223: loss 2.109318256378174
iteration 224: loss 2.1074910163879395
iteration 225: loss 2.089937448501587
iteration 226: loss 2.0793557167053223
iteration 227: loss 2.076063871383667
iteration 228: loss 2.075327157974243
iteration 229: loss 2.089934825897217
iteration 230: loss 2.0786359310150146
iteration 231: loss 2.0778400897979736
iteration 232: loss 2.077824831008911
iteration 233: loss 2.0715386867523193
iteration 234: loss 2.0758056640625
iteration 235: loss 2.0658657550811768
iteration 236: loss 2.0448496341705322
iteration 237: loss 2.0492751598358154
iteration 238: loss 2.044936180114746
iteration 239: loss 2.0379600524902344
iteration 240: loss 2.0200905799865723
iteration 241: loss 2.0399856567382812
iteration 242: loss 2.0179407596588135
iteration 243: loss 2.038360834121704
iteration 244: loss 2.027667760848999
iteration 245: loss 2.0389657020568848
iteration 246: loss 2.038886547088623
iteration 247: loss 2.0485541820526123
iteration 248: loss 2.0018210411071777
iteration 249: loss 2.028324604034424
iteration 250: loss 2.0234134197235107
iteration 251: loss 2.0141029357910156
iteration 252: loss 1.9974576234817505
iteration 253: loss 1.9852735996246338
iteration 254: loss 1.9982646703720093
iteration 255: loss 1.9828544855117798
iteration 256: loss 1.987169623374939
iteration 257: loss 1.9773732423782349
iteration 258: loss 1.9533500671386719
iteration 259: loss 1.9987238645553589
iteration 260: loss 1.9816938638687134
iteration 261: loss 1.9619214534759521
iteration 262: loss 1.9564180374145508
iteration 263: loss 1.9573874473571777
iteration 264: loss 1.9519190788269043
iteration 265: loss 1.9570772647857666
iteration 266: loss 1.937227487564087
iteration 267: loss 1.923843264579773
iteration 268: loss 1.9287532567977905
iteration 269: loss 1.9120821952819824
iteration 270: loss 1.927773118019104
iteration 271: loss 1.9134403467178345
iteration 272: loss 1.9243459701538086
iteration 273: loss 1.9142985343933105
iteration 274: loss 1.9062561988830566
iteration 275: loss 1.905375599861145
iteration 276: loss 1.8980799913406372
iteration 277: loss 1.8781746625900269
iteration 278: loss 1.8934941291809082
iteration 279: loss 1.885468602180481
iteration 280: loss 1.8870326280593872
iteration 281: loss 1.8905388116836548
iteration 282: loss 1.9020092487335205
iteration 283: loss 1.8802133798599243
iteration 284: loss 1.862646460533142
iteration 285: loss 1.865705966949463
iteration 286: loss 1.869767189025879
iteration 287: loss 1.8513658046722412
iteration 288: loss 1.833394169807434
iteration 289: loss 1.8348585367202759
iteration 290: loss 1.8205058574676514
iteration 291: loss 1.843351125717163
iteration 292: loss 1.8105144500732422
iteration 293: loss 1.7876492738723755
iteration 294: loss 1.828873634338379
iteration 295: loss 1.7935112714767456
iteration 296: loss 1.8127422332763672
iteration 297: loss 1.8303207159042358
iteration 298: loss 1.8111716508865356
iteration 299: loss 1.7951487302780151
iteration 300: loss 1.7853467464447021
iteration 301: loss 1.7755365371704102
iteration 302: loss 1.7911996841430664
iteration 303: loss 1.775922417640686
iteration 304: loss 1.8047771453857422
iteration 305: loss 1.7772499322891235
iteration 306: loss 1.7546592950820923
iteration 307: loss 1.779294490814209
iteration 308: loss 1.7369180917739868
iteration 309: loss 1.7471932172775269
iteration 310: loss 1.7197829484939575
iteration 311: loss 1.7234655618667603
iteration 312: loss 1.7249208688735962
iteration 313: loss 1.7160708904266357
iteration 314: loss 1.7214853763580322
iteration 315: loss 1.7129700183868408
iteration 316: loss 1.6996190547943115
iteration 317: loss 1.7031669616699219
iteration 318: loss 1.6981618404388428
iteration 319: loss 1.6904373168945312
iteration 320: loss 1.6566401720046997
iteration 321: loss 1.7154202461242676
iteration 322: loss 1.6371387243270874
iteration 323: loss 1.6872451305389404
iteration 324: loss 1.6689989566802979
iteration 325: loss 1.6800060272216797
iteration 326: loss 1.6538082361221313
iteration 327: loss 1.6519182920455933
iteration 328: loss 1.654636263847351
iteration 329: loss 1.6560206413269043
iteration 330: loss 1.5861585140228271
iteration 331: loss 1.6049681901931763
iteration 332: loss 1.600947618484497
iteration 333: loss 1.6208183765411377
iteration 334: loss 1.612572193145752
iteration 335: loss 1.5818488597869873
iteration 336: loss 1.5870552062988281
iteration 337: loss 1.5794939994812012
iteration 338: loss 1.579270362854004
iteration 339: loss 1.5841279029846191
iteration 340: loss 1.5565646886825562
iteration 341: loss 1.5785086154937744
iteration 342: loss 1.5604445934295654
iteration 343: loss 1.5658663511276245
iteration 344: loss 1.529131531715393
iteration 345: loss 1.57038152217865
iteration 346: loss 1.5646740198135376
iteration 347: loss 1.5468047857284546
iteration 348: loss 1.533191204071045
iteration 349: loss 1.5410467386245728
iteration 350: loss 1.5134443044662476
iteration 351: loss 1.5198287963867188
iteration 352: loss 1.5209660530090332
iteration 353: loss 1.5562342405319214
iteration 354: loss 1.5124058723449707
iteration 355: loss 1.4837782382965088
iteration 356: loss 1.5147572755813599
iteration 357: loss 1.500910758972168
iteration 358: loss 1.4487591981887817
iteration 359: loss 1.463136076927185
iteration 360: loss 1.4531162977218628
iteration 361: loss 1.4213776588439941
iteration 362: loss 1.4514354467391968
iteration 363: loss 1.4669514894485474
iteration 364: loss 1.4596755504608154
iteration 365: loss 1.4288626909255981
iteration 366: loss 1.4053205251693726
iteration 367: loss 1.4294333457946777
iteration 368: loss 1.420214295387268
iteration 369: loss 1.4261209964752197
iteration 370: loss 1.4383386373519897
iteration 371: loss 1.4597817659378052
iteration 372: loss 1.4284641742706299
iteration 373: loss 1.43184232711792
iteration 374: loss 1.434043288230896
iteration 375: loss 1.4533063173294067
iteration 376: loss 1.4104249477386475
iteration 377: loss 1.364260196685791
iteration 378: loss 1.4289066791534424
iteration 379: loss 1.3938416242599487
iteration 380: loss 1.3624414205551147
iteration 381: loss 1.3876734972000122
iteration 382: loss 1.3638408184051514
iteration 383: loss 1.3449296951293945
iteration 384: loss 1.3669042587280273
iteration 385: loss 1.3947705030441284
iteration 386: loss 1.3608050346374512
iteration 387: loss 1.3629322052001953
iteration 388: loss 1.3526725769042969
iteration 389: loss 1.3258527517318726
iteration 390: loss 1.36935555934906
iteration 391: loss 1.3370424509048462
iteration 392: loss 1.3010259866714478
iteration 393: loss 1.3170125484466553
iteration 394: loss 1.2901769876480103
iteration 395: loss 1.286520004272461
iteration 396: loss 1.3421752452850342
iteration 397: loss 1.3143229484558105
iteration 398: loss 1.308990478515625
iteration 399: loss 1.315700650215149
iteration 400: loss 1.309920072555542
iteration 401: loss 1.2721861600875854
iteration 402: loss 1.2261916399002075
iteration 403: loss 1.258037805557251
iteration 404: loss 1.3042702674865723
iteration 405: loss 1.261090636253357
iteration 406: loss 1.241742730140686
iteration 407: loss 1.2635555267333984
iteration 408: loss 1.2613407373428345
iteration 409: loss 1.2278200387954712
iteration 410: loss 1.29636549949646
iteration 411: loss 1.2351351976394653
iteration 412: loss 1.2875202894210815
iteration 413: loss 1.2598081827163696
iteration 414: loss 1.249235987663269
iteration 415: loss 1.249135971069336
iteration 416: loss 1.2233121395111084
iteration 417: loss 1.2079746723175049
iteration 418: loss 1.1926631927490234
iteration 419: loss 1.180238127708435
iteration 420: loss 1.239174723625183
iteration 421: loss 1.1776281595230103
iteration 422: loss 1.2329875230789185
iteration 423: loss 1.2051985263824463
iteration 424: loss 1.1485834121704102
iteration 425: loss 1.194197177886963
iteration 426: loss 1.160097360610962
iteration 427: loss 1.2084397077560425
iteration 428: loss 1.225976586341858
iteration 429: loss 1.1946650743484497
iteration 430: loss 1.1853809356689453
iteration 431: loss 1.1713073253631592
iteration 432: loss 1.1795750856399536
iteration 433: loss 1.1486425399780273
iteration 434: loss 1.1771270036697388
iteration 435: loss 1.1971439123153687
iteration 436: loss 1.1308156251907349
iteration 437: loss 1.1752641201019287
iteration 438: loss 1.167554259300232
iteration 439: loss 1.1545186042785645
iteration 440: loss 1.1489027738571167
iteration 441: loss 1.1338073015213013
iteration 442: loss 1.1505078077316284
iteration 443: loss 1.1219173669815063
iteration 444: loss 1.1550787687301636
iteration 445: loss 1.0848695039749146
iteration 446: loss 1.0757676362991333
iteration 447: loss 1.1356195211410522
iteration 448: loss 1.115034818649292
iteration 449: loss 1.109848141670227
iteration 450: loss 1.0770331621170044
iteration 451: loss 1.099583387374878
iteration 452: loss 1.1342761516571045
iteration 453: loss 1.0856845378875732
iteration 454: loss 1.1006795167922974
iteration 455: loss 1.1107032299041748
iteration 456: loss 1.1073864698410034
iteration 457: loss 1.0871306657791138
iteration 458: loss 1.0783251523971558
iteration 459: loss 1.0599159002304077
iteration 460: loss 1.0958797931671143
iteration 461: loss 1.0677424669265747
iteration 462: loss 1.060543179512024
iteration 463: loss 1.0793108940124512
iteration 464: loss 1.0421342849731445
iteration 465: loss 1.0807905197143555
iteration 466: loss 1.0815147161483765
iteration 467: loss 1.024091124534607
iteration 468: loss 1.076012372970581
iteration 469: loss 1.0196211338043213
iteration 470: loss 1.0504064559936523
iteration 471: loss 1.073193907737732
iteration 472: loss 1.0203464031219482
iteration 473: loss 1.0563660860061646
iteration 474: loss 1.066726803779602
iteration 475: loss 1.0183697938919067
iteration 476: loss 1.1172317266464233
iteration 477: loss 1.023318886756897
iteration 478: loss 0.983306884765625
iteration 479: loss 1.0041530132293701
iteration 480: loss 1.0200855731964111
iteration 481: loss 0.9824569225311279
iteration 482: loss 1.0203720331192017
iteration 483: loss 1.0054553747177124
iteration 484: loss 0.9722145795822144
iteration 485: loss 0.9635644555091858
iteration 486: loss 0.9799204468727112
iteration 487: loss 1.0049735307693481
iteration 488: loss 0.9764237999916077
iteration 489: loss 0.9926947355270386
iteration 490: loss 1.0046241283416748
iteration 491: loss 1.0271755456924438
iteration 492: loss 1.0049920082092285
iteration 493: loss 0.9904154539108276
iteration 494: loss 0.9429778456687927
iteration 495: loss 0.9587311148643494
iteration 496: loss 0.9911842346191406
iteration 497: loss 0.9676228165626526
iteration 498: loss 0.9874197244644165
iteration 499: loss 0.9381684064865112
iteration 500: loss 0.94196617603302
iteration 501: loss 0.9103217124938965
iteration 502: loss 1.0081011056900024
iteration 503: loss 1.0123215913772583
iteration 504: loss 0.9608692526817322
iteration 505: loss 0.9659876823425293
iteration 506: loss 0.9286692142486572
iteration 507: loss 0.9144107699394226
iteration 508: loss 0.9723339080810547
iteration 509: loss 0.9446050524711609
iteration 510: loss 0.95040363073349
iteration 511: loss 0.9570422172546387
iteration 512: loss 0.9268065690994263
iteration 513: loss 0.9623485803604126
iteration 514: loss 0.9050832986831665
iteration 515: loss 0.9371262192726135
iteration 516: loss 0.9413549900054932
iteration 517: loss 0.9194691777229309
iteration 518: loss 0.927110493183136
iteration 519: loss 0.925493061542511
iteration 520: loss 0.9099950790405273
iteration 521: loss 0.9181655645370483
iteration 522: loss 0.9153078198432922
iteration 523: loss 0.941190242767334
iteration 524: loss 0.8989930748939514
iteration 525: loss 0.9116593599319458
iteration 526: loss 0.9095146059989929
iteration 527: loss 0.876591682434082
iteration 528: loss 0.894612729549408
iteration 529: loss 0.918927788734436
iteration 530: loss 0.8789234161376953
iteration 531: loss 0.8702027797698975
iteration 532: loss 0.9250079393386841
iteration 533: loss 0.8854133486747742
iteration 534: loss 0.8751845359802246
iteration 535: loss 0.8737708926200867
iteration 536: loss 0.9230523109436035
iteration 537: loss 0.8783178925514221
iteration 538: loss 0.9053167700767517
iteration 539: loss 0.8974286317825317
iteration 540: loss 0.9048822522163391
iteration 541: loss 0.8834714293479919
iteration 542: loss 0.8739863634109497
iteration 543: loss 0.8776218295097351
iteration 544: loss 0.8812590837478638
iteration 545: loss 0.8976136445999146
iteration 546: loss 0.8278580904006958
iteration 547: loss 0.8979548811912537
iteration 548: loss 0.8806234002113342
iteration 549: loss 0.8597378730773926
iteration 550: loss 0.9305604696273804
iteration 551: loss 0.8652148246765137
iteration 552: loss 0.8706567287445068
iteration 553: loss 0.884859561920166
iteration 554: loss 0.8472493886947632
iteration 555: loss 0.8627280592918396
iteration 556: loss 0.8775206804275513
iteration 557: loss 0.8486959934234619
iteration 558: loss 0.868805468082428
iteration 559: loss 0.9091910123825073
iteration 560: loss 0.8527102470397949
iteration 561: loss 0.8608846664428711
iteration 562: loss 0.8722294569015503
iteration 563: loss 0.8467273116111755
iteration 564: loss 0.8876792192459106
iteration 565: loss 0.8405168056488037
iteration 566: loss 0.8304868340492249
iteration 567: loss 0.8120386004447937
iteration 568: loss 0.7848191261291504
iteration 569: loss 0.891866147518158
iteration 570: loss 0.919737696647644
iteration 571: loss 0.8286802172660828
iteration 572: loss 0.8629342913627625
iteration 573: loss 0.8247522711753845
iteration 574: loss 0.7864538431167603
iteration 575: loss 0.8539330363273621
iteration 576: loss 0.8356369733810425
iteration 577: loss 0.8198782205581665
iteration 578: loss 0.721410870552063
iteration 579: loss 0.8172788619995117
iteration 580: loss 0.8279019594192505
iteration 581: loss 0.7775825262069702
iteration 582: loss 0.8126580119132996
iteration 583: loss 0.8120548725128174
iteration 584: loss 0.8441544771194458
iteration 585: loss 0.7797242403030396
iteration 586: loss 0.8284170627593994
iteration 587: loss 0.8606814742088318
iteration 588: loss 0.8355792760848999
iteration 589: loss 0.7451735138893127
iteration 590: loss 0.8101142644882202
iteration 591: loss 0.7986226081848145
iteration 592: loss 0.7604665160179138
iteration 593: loss 0.7279264330863953
iteration 594: loss 0.7730979323387146
iteration 595: loss 0.7822914123535156
iteration 596: loss 0.8257941603660583
iteration 597: loss 0.747515857219696
iteration 598: loss 0.7871468663215637
iteration 599: loss 0.8187239766120911
iteration 600: loss 0.7699525952339172
iteration 601: loss 0.6945816278457642
iteration 602: loss 0.777109682559967
iteration 603: loss 0.7621150016784668
iteration 604: loss 0.7616851329803467
iteration 605: loss 0.7437900900840759
iteration 606: loss 0.8256348967552185
iteration 607: loss 0.7966828942298889
iteration 608: loss 0.806806743144989
iteration 609: loss 0.7547021508216858
iteration 610: loss 0.7294352650642395
iteration 611: loss 0.7504043579101562
iteration 612: loss 0.7629313468933105
iteration 613: loss 0.7721121311187744
iteration 614: loss 0.7796972990036011
iteration 615: loss 0.7613775730133057
iteration 616: loss 0.7479954361915588
iteration 617: loss 0.7850490212440491
iteration 618: loss 0.779460608959198
iteration 619: loss 0.776742696762085
iteration 620: loss 0.6641432642936707
iteration 621: loss 0.691449761390686
iteration 622: loss 0.7587848901748657
iteration 623: loss 0.7511136531829834
iteration 624: loss 0.7199699282646179
iteration 625: loss 0.7373933792114258
iteration 626: loss 0.7096701264381409
iteration 627: loss 0.7309300303459167
iteration 628: loss 0.6730669140815735
iteration 629: loss 0.7656527757644653
iteration 630: loss 0.7656198740005493
iteration 631: loss 0.7491968274116516
iteration 632: loss 0.7816144227981567
iteration 633: loss 0.7682607769966125
iteration 634: loss 0.6827887296676636
iteration 635: loss 0.7402434945106506
iteration 636: loss 0.7303184270858765
iteration 637: loss 0.7677322030067444
iteration 638: loss 0.7502686381340027
iteration 639: loss 0.6793769001960754
iteration 640: loss 0.7286481857299805
iteration 641: loss 0.7308670282363892
iteration 642: loss 0.7112029194831848
iteration 643: loss 0.6948765516281128
iteration 644: loss 0.6886050701141357
iteration 645: loss 0.7227523326873779
iteration 646: loss 0.7223044633865356
iteration 647: loss 0.7155708074569702
iteration 648: loss 0.7146499156951904
iteration 649: loss 0.7476759552955627
iteration 650: loss 0.8162034153938293
iteration 651: loss 0.7375503778457642
iteration 652: loss 0.6990061402320862
iteration 653: loss 0.7227009534835815
iteration 654: loss 0.7179241180419922
iteration 655: loss 0.7478049397468567
iteration 656: loss 0.706750750541687
iteration 657: loss 0.703237771987915
iteration 658: loss 0.7034920454025269
iteration 659: loss 0.6896576881408691
iteration 660: loss 0.6965385675430298
iteration 661: loss 0.694300651550293
iteration 662: loss 0.7899138927459717
iteration 663: loss 0.7304525971412659
iteration 664: loss 0.7103565335273743
iteration 665: loss 0.7843183875083923
iteration 666: loss 0.6865532994270325
iteration 667: loss 0.7078167200088501
iteration 668: loss 0.6898040175437927
iteration 669: loss 0.6958022713661194
iteration 670: loss 0.7396591901779175
iteration 671: loss 0.7215339541435242
iteration 672: loss 0.6873748302459717
iteration 673: loss 0.7221112847328186
iteration 674: loss 0.7312219142913818
iteration 675: loss 0.6517603397369385
iteration 676: loss 0.7426414489746094
iteration 677: loss 0.7100604772567749
iteration 678: loss 0.6982626914978027
iteration 679: loss 0.7135851979255676
iteration 680: loss 0.6692460775375366
iteration 681: loss 0.7183228731155396
iteration 682: loss 0.7153186798095703
iteration 683: loss 0.6507823467254639
iteration 684: loss 0.7571366429328918
iteration 685: loss 0.7040605545043945
iteration 686: loss 0.6696892976760864
iteration 687: loss 0.7010276913642883
iteration 688: loss 0.6965112090110779
iteration 689: loss 0.6952294111251831
iteration 690: loss 0.7329725623130798
iteration 691: loss 0.6968317627906799
iteration 692: loss 0.609081506729126
iteration 693: loss 0.6928837299346924
iteration 694: loss 0.6954091787338257
iteration 695: loss 0.6689280867576599
iteration 696: loss 0.6500648856163025
iteration 697: loss 0.6924327611923218
iteration 698: loss 0.6701672673225403
iteration 699: loss 0.628084659576416
iteration 700: loss 0.7176077961921692
iteration 701: loss 0.6483851075172424
iteration 702: loss 0.674994945526123
iteration 703: loss 0.6919752359390259
iteration 704: loss 0.6417729258537292
iteration 705: loss 0.6529170274734497
iteration 706: loss 0.6315773725509644
iteration 707: loss 0.6791388392448425
iteration 708: loss 0.6880208253860474
iteration 709: loss 0.6653361320495605
iteration 710: loss 0.6816902160644531
iteration 711: loss 0.6951096653938293
iteration 712: loss 0.6751652956008911
iteration 713: loss 0.650863528251648
iteration 714: loss 0.631173312664032
iteration 715: loss 0.6737557053565979
iteration 716: loss 0.7483140826225281
iteration 717: loss 0.6490811109542847
iteration 718: loss 0.6086403131484985
iteration 719: loss 0.6197033524513245
iteration 720: loss 0.6270521879196167
iteration 721: loss 0.6181289553642273
iteration 722: loss 0.6087173819541931
iteration 723: loss 0.675891101360321
iteration 724: loss 0.6813352108001709
iteration 725: loss 0.6759185791015625
iteration 726: loss 0.6581634283065796
iteration 727: loss 0.6074357032775879
iteration 728: loss 0.6597252488136292
iteration 729: loss 0.6817530393600464
iteration 730: loss 0.6522690653800964
iteration 731: loss 0.6520699858665466
iteration 732: loss 0.6826619505882263
iteration 733: loss 0.6063380241394043
iteration 734: loss 0.6819515824317932
iteration 735: loss 0.6584193706512451
iteration 736: loss 0.6340787410736084
iteration 737: loss 0.621914267539978
iteration 738: loss 0.6473867893218994
iteration 739: loss 0.6437297463417053
iteration 740: loss 0.5968648791313171
iteration 741: loss 0.6064059138298035
iteration 742: loss 0.6602668762207031
iteration 743: loss 0.6671871542930603
iteration 744: loss 0.7025219798088074
iteration 745: loss 0.7083903551101685
iteration 746: loss 0.6716120839118958
iteration 747: loss 0.6092714071273804
iteration 748: loss 0.5666553378105164
iteration 749: loss 0.6261478662490845
iteration 750: loss 0.67613285779953
iteration 751: loss 0.6704266667366028
iteration 752: loss 0.6007101535797119
iteration 753: loss 0.6839848756790161
iteration 754: loss 0.6902134418487549
iteration 755: loss 0.6623156070709229
iteration 756: loss 0.5829483866691589
iteration 757: loss 0.6178149580955505
iteration 758: loss 0.5931099057197571
iteration 759: loss 0.6054502725601196
iteration 760: loss 0.7028588056564331
iteration 761: loss 0.6257650256156921
iteration 762: loss 0.612073540687561
iteration 763: loss 0.6199977397918701
iteration 764: loss 0.6910519599914551
iteration 765: loss 0.6437990069389343
iteration 766: loss 0.6018826961517334
iteration 767: loss 0.598526656627655
iteration 768: loss 0.6138410568237305
iteration 769: loss 0.5963291525840759
iteration 770: loss 0.5595107078552246
iteration 771: loss 0.6007353067398071
iteration 772: loss 0.5881490111351013
iteration 773: loss 0.6452425122261047
iteration 774: loss 0.570616602897644
iteration 775: loss 0.6517627239227295
iteration 776: loss 0.612406313419342
iteration 777: loss 0.5929562449455261
iteration 778: loss 0.6365058422088623
iteration 779: loss 0.6280105710029602
iteration 780: loss 0.6185296177864075
iteration 781: loss 0.5373623371124268
iteration 782: loss 0.6130744814872742
iteration 783: loss 0.6073824167251587
iteration 784: loss 0.6186246275901794
iteration 785: loss 0.5770337581634521
iteration 786: loss 0.6277812719345093
iteration 787: loss 0.6230395436286926
iteration 788: loss 0.5848273038864136
iteration 789: loss 0.6172541379928589
iteration 790: loss 0.6328449845314026
iteration 791: loss 0.620178759098053
iteration 792: loss 0.5974357724189758
iteration 793: loss 0.598404049873352
iteration 794: loss 0.5734996199607849
iteration 795: loss 0.6338075995445251
iteration 796: loss 0.580786406993866
iteration 797: loss 0.5965389013290405
iteration 798: loss 0.6230195760726929
iteration 799: loss 0.5967423915863037
iteration 800: loss 0.583906352519989
iteration 801: loss 0.5635219812393188
iteration 802: loss 0.5892754793167114
iteration 803: loss 0.6066595315933228
iteration 804: loss 0.6096082925796509
iteration 805: loss 0.5864905118942261
iteration 806: loss 0.6096180081367493
iteration 807: loss 0.5827585458755493
iteration 808: loss 0.5674875974655151
iteration 809: loss 0.5643318295478821
iteration 810: loss 0.6196619272232056
iteration 811: loss 0.5476289391517639
iteration 812: loss 0.5879663228988647
iteration 813: loss 0.6672267913818359
iteration 814: loss 0.6245406270027161
iteration 815: loss 0.6265795826911926
iteration 816: loss 0.5448065400123596
iteration 817: loss 0.6235785484313965
iteration 818: loss 0.5796027183532715
iteration 819: loss 0.6026080250740051
iteration 820: loss 0.5797533988952637
iteration 821: loss 0.5444305539131165
iteration 822: loss 0.6479818820953369
iteration 823: loss 0.5378124713897705
iteration 824: loss 0.543503999710083
iteration 825: loss 0.5280443429946899
iteration 826: loss 0.53372722864151
iteration 827: loss 0.5280919671058655
iteration 828: loss 0.5870963335037231
iteration 829: loss 0.5728980898857117
iteration 830: loss 0.6155913472175598
iteration 831: loss 0.5908352732658386
iteration 832: loss 0.6715400815010071
iteration 833: loss 0.6083559989929199
iteration 834: loss 0.5656605958938599
iteration 835: loss 0.5987255573272705
iteration 836: loss 0.588916003704071
iteration 837: loss 0.5633772015571594
iteration 838: loss 0.5407751798629761
iteration 839: loss 0.5751009583473206
iteration 840: loss 0.5678175687789917
iteration 841: loss 0.5548349022865295
iteration 842: loss 0.5798287987709045
iteration 843: loss 0.5252627730369568
iteration 844: loss 0.6289910078048706
iteration 845: loss 0.565056562423706
iteration 846: loss 0.5077604651451111
iteration 847: loss 0.5672925710678101
iteration 848: loss 0.5466322302818298
iteration 849: loss 0.6078168749809265
iteration 850: loss 0.5983276963233948
iteration 851: loss 0.6193684935569763
iteration 852: loss 0.5712651610374451
iteration 853: loss 0.6336802244186401
iteration 854: loss 0.5675077438354492
iteration 855: loss 0.5974535346031189
iteration 856: loss 0.5820581912994385
iteration 857: loss 0.5876725912094116
iteration 858: loss 0.5633447170257568
iteration 859: loss 0.5406789183616638
iteration 860: loss 0.5792924761772156
iteration 861: loss 0.5515936017036438
iteration 862: loss 0.6069519519805908
iteration 863: loss 0.5480941534042358
iteration 864: loss 0.594476580619812
iteration 865: loss 0.5924364328384399
iteration 866: loss 0.5899719595909119
iteration 867: loss 0.5482352375984192
iteration 868: loss 0.6055830121040344
iteration 869: loss 0.5664209723472595
iteration 870: loss 0.5215832591056824
iteration 871: loss 0.5521953105926514
iteration 872: loss 0.5619799494743347
iteration 873: loss 0.5850068926811218
iteration 874: loss 0.5467185974121094
iteration 875: loss 0.5981115698814392
iteration 876: loss 0.5496684312820435
iteration 877: loss 0.5605742335319519
iteration 878: loss 0.5638477802276611
iteration 879: loss 0.5478353500366211
iteration 880: loss 0.5406619906425476
iteration 881: loss 0.5741201043128967
iteration 882: loss 0.5703892707824707
iteration 883: loss 0.5188601613044739
iteration 884: loss 0.5741780400276184
iteration 885: loss 0.5228606462478638
iteration 886: loss 0.5496593117713928
iteration 887: loss 0.5837308764457703
iteration 888: loss 0.4943368434906006
iteration 889: loss 0.5302630662918091
iteration 890: loss 0.5317267775535583
iteration 891: loss 0.5002166628837585
iteration 892: loss 0.5299756526947021
iteration 893: loss 0.4998864531517029
iteration 894: loss 0.5308109521865845
iteration 895: loss 0.5591067671775818
iteration 896: loss 0.5232328772544861
iteration 897: loss 0.564342200756073
iteration 898: loss 0.5959901809692383
iteration 899: loss 0.5545504689216614
iteration 900: loss 0.4839927852153778
iteration 901: loss 0.4940854609012604
iteration 902: loss 0.5893509387969971
iteration 903: loss 0.5856775641441345
iteration 904: loss 0.5471804141998291
iteration 905: loss 0.5562431812286377
iteration 906: loss 0.5556878447532654
iteration 907: loss 0.5513383746147156
iteration 908: loss 0.564918577671051
iteration 909: loss 0.4735463261604309
iteration 910: loss 0.5271033048629761
iteration 911: loss 0.50650554895401
iteration 912: loss 0.532355546951294
iteration 913: loss 0.6011217832565308
iteration 914: loss 0.5624200701713562
iteration 915: loss 0.5190965533256531
iteration 916: loss 0.5420712232589722
iteration 917: loss 0.5748862028121948
iteration 918: loss 0.5611852407455444
iteration 919: loss 0.5268174409866333
iteration 920: loss 0.5518038272857666
iteration 921: loss 0.5698468685150146
iteration 922: loss 0.5232053399085999
iteration 923: loss 0.5737655758857727
iteration 924: loss 0.4731264114379883
iteration 925: loss 0.5254486203193665
iteration 926: loss 0.5120009779930115
iteration 927: loss 0.5396612882614136
iteration 928: loss 0.5592186450958252
iteration 929: loss 0.5028600692749023
iteration 930: loss 0.47610583901405334
iteration 931: loss 0.5075636506080627
iteration 932: loss 0.5587646961212158
iteration 933: loss 0.5810774564743042
iteration 934: loss 0.5703386664390564
iteration 935: loss 0.5278089642524719
iteration 936: loss 0.6046913266181946
iteration 937: loss 0.5071248412132263
iteration 938: loss 0.520927906036377
iteration 939: loss 0.4819629490375519
iteration 940: loss 0.5234066843986511
iteration 941: loss 0.5109931826591492
iteration 942: loss 0.5275124311447144
iteration 943: loss 0.5490080714225769
iteration 944: loss 0.6021731495857239
iteration 945: loss 0.6463727951049805
iteration 946: loss 0.5485203862190247
iteration 947: loss 0.5682336091995239
iteration 948: loss 0.5110572576522827
iteration 949: loss 0.5221131443977356
iteration 950: loss 0.543891429901123
iteration 951: loss 0.46285226941108704
iteration 952: loss 0.4814566969871521
iteration 953: loss 0.4675472676753998
iteration 954: loss 0.4894390404224396
iteration 955: loss 0.47812023758888245
iteration 956: loss 0.540052056312561
iteration 957: loss 0.581612229347229
iteration 958: loss 0.5413317084312439
iteration 959: loss 0.5951282382011414
iteration 960: loss 0.585767924785614
iteration 961: loss 0.5414800643920898
iteration 962: loss 0.5176855325698853
iteration 963: loss 0.5244728922843933
iteration 964: loss 0.4935099184513092
iteration 965: loss 0.5173472762107849
iteration 966: loss 0.5385002493858337
iteration 967: loss 0.5298115611076355
iteration 968: loss 0.4707942306995392
iteration 969: loss 0.5144515037536621
iteration 970: loss 0.5444191694259644
iteration 971: loss 0.5211082100868225
iteration 972: loss 0.4677967131137848
iteration 973: loss 0.5218561887741089
iteration 974: loss 0.5345746278762817
iteration 975: loss 0.4604226350784302
iteration 976: loss 0.5085227489471436
iteration 977: loss 0.5225006341934204
iteration 978: loss 0.5036391615867615
iteration 979: loss 0.5412364602088928
iteration 980: loss 0.5430843830108643
iteration 981: loss 0.49001049995422363
iteration 982: loss 0.5119216442108154
iteration 983: loss 0.5151087641716003
iteration 984: loss 0.5324583053588867
iteration 985: loss 0.46463698148727417
iteration 986: loss 0.4645555913448334
iteration 987: loss 0.45361092686653137
iteration 988: loss 0.4999777376651764
iteration 989: loss 0.5423172116279602
iteration 990: loss 0.46945640444755554
iteration 991: loss 0.5346421003341675
iteration 992: loss 0.5361530780792236
iteration 993: loss 0.5531619787216187
iteration 994: loss 0.4566633701324463
iteration 995: loss 0.46192502975463867
iteration 996: loss 0.5068178772926331
iteration 997: loss 0.5037556886672974
iteration 998: loss 0.5093128681182861
iteration 999: loss 0.49418559670448303
iteration 1000: loss 0.4876314103603363
iteration 1001: loss 0.5317944288253784
iteration 1002: loss 0.5066486597061157
iteration 1003: loss 0.4900541305541992
iteration 1004: loss 0.5097534656524658
iteration 1005: loss 0.5556809306144714
iteration 1006: loss 0.5308148264884949
iteration 1007: loss 0.4780859351158142
iteration 1008: loss 0.49205002188682556
iteration 1009: loss 0.511873185634613
iteration 1010: loss 0.5164867043495178
iteration 1011: loss 0.5218844413757324
iteration 1012: loss 0.558745801448822
iteration 1013: loss 0.5010894536972046
iteration 1014: loss 0.4839635193347931
iteration 1015: loss 0.4771731495857239
iteration 1016: loss 0.45016995072364807
iteration 1017: loss 0.543601930141449
iteration 1018: loss 0.4911145865917206
iteration 1019: loss 0.5113836526870728
iteration 1020: loss 0.5550432205200195
iteration 1021: loss 0.4780723750591278
iteration 1022: loss 0.51445072889328
iteration 1023: loss 0.4741467833518982
iteration 1024: loss 0.5091190338134766
iteration 1025: loss 0.5004680752754211
iteration 1026: loss 0.49233904480934143
iteration 1027: loss 0.44594821333885193
iteration 1028: loss 0.4473938047885895
iteration 1029: loss 0.5597848296165466
iteration 1030: loss 0.5034535527229309
iteration 1031: loss 0.49704307317733765
iteration 1032: loss 0.47074398398399353
iteration 1033: loss 0.4895578920841217
iteration 1034: loss 0.47755441069602966
iteration 1035: loss 0.5440303087234497
iteration 1036: loss 0.48891088366508484
iteration 1037: loss 0.498476505279541
iteration 1038: loss 0.5058197975158691
iteration 1039: loss 0.4447433650493622
iteration 1040: loss 0.5127474665641785
iteration 1041: loss 0.4852046072483063
iteration 1042: loss 0.4222915470600128
iteration 1043: loss 0.47056519985198975
iteration 1044: loss 0.49188563227653503
iteration 1045: loss 0.509787380695343
iteration 1046: loss 0.447811484336853
iteration 1047: loss 0.4717216491699219
iteration 1048: loss 0.49001532793045044
iteration 1049: loss 0.5240267515182495
iteration 1050: loss 0.46899694204330444
iteration 1051: loss 0.4930103123188019
iteration 1052: loss 0.5381720066070557
iteration 1053: loss 0.5470998287200928
iteration 1054: loss 0.4740511178970337
iteration 1055: loss 0.4724995493888855
iteration 1056: loss 0.4519657790660858
iteration 1057: loss 0.5680100917816162
iteration 1058: loss 0.44009360671043396
iteration 1059: loss 0.4739261567592621
iteration 1060: loss 0.4871982932090759
iteration 1061: loss 0.5307360887527466
iteration 1062: loss 0.5076795816421509
iteration 1063: loss 0.49370694160461426
iteration 1064: loss 0.5009849667549133
iteration 1065: loss 0.5004894137382507
iteration 1066: loss 0.4867668151855469
iteration 1067: loss 0.4960789680480957
iteration 1068: loss 0.4655301868915558
iteration 1069: loss 0.47505709528923035
iteration 1070: loss 0.44002848863601685
iteration 1071: loss 0.4607395529747009
iteration 1072: loss 0.4489653706550598
iteration 1073: loss 0.5091699361801147
iteration 1074: loss 0.43470633029937744
iteration 1075: loss 0.49809563159942627
iteration 1076: loss 0.5043604373931885
iteration 1077: loss 0.5653673410415649
iteration 1078: loss 0.5074961185455322
iteration 1079: loss 0.4870836138725281
iteration 1080: loss 0.4828853905200958
iteration 1081: loss 0.44924551248550415
iteration 1082: loss 0.49487099051475525
iteration 1083: loss 0.46252039074897766
iteration 1084: loss 0.4595656096935272
iteration 1085: loss 0.4792592525482178
iteration 1086: loss 0.4955519437789917
iteration 1087: loss 0.5172938108444214
iteration 1088: loss 0.4470604956150055
iteration 1089: loss 0.42448145151138306
iteration 1090: loss 0.5104753971099854
iteration 1091: loss 0.481792151927948
iteration 1092: loss 0.4998786449432373
iteration 1093: loss 0.48714733123779297
iteration 1094: loss 0.4896993637084961
iteration 1095: loss 0.45442089438438416
iteration 1096: loss 0.4699770510196686
iteration 1097: loss 0.46390777826309204
iteration 1098: loss 0.5442686676979065
iteration 1099: loss 0.4503028988838196
iteration 1100: loss 0.5017403364181519
iteration 1101: loss 0.4510287046432495
iteration 1102: loss 0.44438713788986206
iteration 1103: loss 0.5109379291534424
iteration 1104: loss 0.477474182844162
iteration 1105: loss 0.4826124310493469
iteration 1106: loss 0.49696671962738037
iteration 1107: loss 0.47509798407554626
iteration 1108: loss 0.5088663697242737
iteration 1109: loss 0.5162012577056885
iteration 1110: loss 0.5074305534362793
iteration 1111: loss 0.413952112197876
iteration 1112: loss 0.43758368492126465
iteration 1113: loss 0.42330387234687805
iteration 1114: loss 0.49395641684532166
iteration 1115: loss 0.5004335641860962
iteration 1116: loss 0.45292940735816956
iteration 1117: loss 0.5026392936706543
iteration 1118: loss 0.48013854026794434
iteration 1119: loss 0.47661924362182617
iteration 1120: loss 0.4273715019226074
iteration 1121: loss 0.4283819794654846
iteration 1122: loss 0.4372483789920807
iteration 1123: loss 0.43047967553138733
iteration 1124: loss 0.47083768248558044
iteration 1125: loss 0.47551819682121277
iteration 1126: loss 0.48819780349731445
iteration 1127: loss 0.4611181318759918
iteration 1128: loss 0.4915491044521332
iteration 1129: loss 0.40861254930496216
iteration 1130: loss 0.4329773187637329
iteration 1131: loss 0.4689537286758423
iteration 1132: loss 0.455230176448822
iteration 1133: loss 0.47771167755126953
iteration 1134: loss 0.4982249140739441
iteration 1135: loss 0.5116735696792603
iteration 1136: loss 0.5067779421806335
iteration 1137: loss 0.5523578524589539
iteration 1138: loss 0.49926382303237915
iteration 1139: loss 0.41342049837112427
iteration 1140: loss 0.4988000988960266
iteration 1141: loss 0.5130340456962585
iteration 1142: loss 0.4602302014827728
iteration 1143: loss 0.42865288257598877
iteration 1144: loss 0.5359585285186768
iteration 1145: loss 0.4403154253959656
iteration 1146: loss 0.47195932269096375
iteration 1147: loss 0.4797806739807129
iteration 1148: loss 0.48196566104888916
iteration 1149: loss 0.540228545665741
iteration 1150: loss 0.4361582398414612
iteration 1151: loss 0.44280627369880676
iteration 1152: loss 0.49018701910972595
iteration 1153: loss 0.5304218530654907
iteration 1154: loss 0.468975692987442
iteration 1155: loss 0.4429135322570801
iteration 1156: loss 0.5146015882492065
iteration 1157: loss 0.36388465762138367
iteration 1158: loss 0.44761934876441956
iteration 1159: loss 0.4990253746509552
iteration 1160: loss 0.4864289462566376
iteration 1161: loss 0.44354912638664246
iteration 1162: loss 0.5092073678970337
iteration 1163: loss 0.5111641883850098
iteration 1164: loss 0.4397730827331543
iteration 1165: loss 0.4939655065536499
iteration 1166: loss 0.5148497223854065
iteration 1167: loss 0.49145328998565674
iteration 1168: loss 0.46326467394828796
iteration 1169: loss 0.4923155903816223
iteration 1170: loss 0.4305696487426758
iteration 1171: loss 0.4927791953086853
iteration 1172: loss 0.43695297837257385
iteration 1173: loss 0.5060306787490845
iteration 1174: loss 0.39864441752433777
iteration 1175: loss 0.40571048855781555
iteration 1176: loss 0.42516034841537476
iteration 1177: loss 0.4649278223514557
iteration 1178: loss 0.40704023838043213
iteration 1179: loss 0.4581837058067322
iteration 1180: loss 0.39021655917167664
iteration 1181: loss 0.46661415696144104
iteration 1182: loss 0.4127674698829651
iteration 1183: loss 0.43297508358955383
iteration 1184: loss 0.4370015263557434
iteration 1185: loss 0.4084562659263611
iteration 1186: loss 0.46079400181770325
iteration 1187: loss 0.4293704032897949
iteration 1188: loss 0.5140050649642944
iteration 1189: loss 0.4810539782047272
iteration 1190: loss 0.4607594907283783
iteration 1191: loss 0.46101996302604675
iteration 1192: loss 0.5211395621299744
iteration 1193: loss 0.49912014603614807
iteration 1194: loss 0.3969276547431946
iteration 1195: loss 0.47900697588920593
iteration 1196: loss 0.4962489604949951
iteration 1197: loss 0.4601558446884155
iteration 1198: loss 0.4458775222301483
iteration 1199: loss 0.47446945309638977
iteration 1200: loss 0.43135887384414673
iteration 1201: loss 0.47507211565971375
iteration 1202: loss 0.5209272503852844
iteration 1203: loss 0.4738799035549164
iteration 1204: loss 0.44157874584198
iteration 1205: loss 0.4380526840686798
iteration 1206: loss 0.4282681941986084
iteration 1207: loss 0.43171122670173645
iteration 1208: loss 0.49297499656677246
iteration 1209: loss 0.5023266077041626
iteration 1210: loss 0.46421119570732117
iteration 1211: loss 0.4888846278190613
iteration 1212: loss 0.4403810203075409
iteration 1213: loss 0.46028950810432434
iteration 1214: loss 0.4229825735092163
iteration 1215: loss 0.40769487619400024
iteration 1216: loss 0.41103577613830566
iteration 1217: loss 0.5182265043258667
iteration 1218: loss 0.44611552357673645
iteration 1219: loss 0.557340681552887
iteration 1220: loss 0.45569077134132385
iteration 1221: loss 0.4498111605644226
iteration 1222: loss 0.45068180561065674
iteration 1223: loss 0.39050859212875366
iteration 1224: loss 0.49722519516944885
iteration 1225: loss 0.41586199402809143
iteration 1226: loss 0.4439815282821655
iteration 1227: loss 0.41559961438179016
iteration 1228: loss 0.47732287645339966
iteration 1229: loss 0.4846813380718231
iteration 1230: loss 0.4544537663459778
iteration 1231: loss 0.4928983747959137
iteration 1232: loss 0.4406638443470001
iteration 1233: loss 0.5041226744651794
iteration 1234: loss 0.44503483176231384
iteration 1235: loss 0.385275274515152
iteration 1236: loss 0.5122805833816528
iteration 1237: loss 0.4945710599422455
iteration 1238: loss 0.4113960564136505
iteration 1239: loss 0.42274996638298035
iteration 1240: loss 0.42607375979423523
iteration 1241: loss 0.40327227115631104
iteration 1242: loss 0.49226298928260803
iteration 1243: loss 0.46130532026290894
iteration 1244: loss 0.42605510354042053
iteration 1245: loss 0.4952279329299927
iteration 1246: loss 0.4747016429901123
iteration 1247: loss 0.41458725929260254
iteration 1248: loss 0.40318503975868225
iteration 1249: loss 0.4256853759288788
iteration 1250: loss 0.4834190011024475
iteration 1251: loss 0.3803565800189972
iteration 1252: loss 0.4888656735420227
iteration 1253: loss 0.43844059109687805
iteration 1254: loss 0.424047589302063
iteration 1255: loss 0.40429025888442993
iteration 1256: loss 0.4343719184398651
iteration 1257: loss 0.44518914818763733
iteration 1258: loss 0.4354097843170166
iteration 1259: loss 0.46225637197494507
iteration 1260: loss 0.47302407026290894
iteration 1261: loss 0.451421320438385
iteration 1262: loss 0.45759284496307373
iteration 1263: loss 0.5092513561248779
iteration 1264: loss 0.4518253803253174
iteration 1265: loss 0.4858716130256653
iteration 1266: loss 0.40880903601646423
iteration 1267: loss 0.4570191204547882
iteration 1268: loss 0.4259744882583618
iteration 1269: loss 0.46141645312309265
iteration 1270: loss 0.5145142674446106
iteration 1271: loss 0.40278854966163635
iteration 1272: loss 0.3468582332134247
iteration 1273: loss 0.4025975167751312
iteration 1274: loss 0.4117316007614136
iteration 1275: loss 0.3770161271095276
iteration 1276: loss 0.46830376982688904
iteration 1277: loss 0.47356098890304565
iteration 1278: loss 0.4805111885070801
iteration 1279: loss 0.4592674672603607
iteration 1280: loss 0.4410945773124695
iteration 1281: loss 0.43020743131637573
iteration 1282: loss 0.4236796796321869
iteration 1283: loss 0.3760574758052826
iteration 1284: loss 0.43251940608024597
iteration 1285: loss 0.46343794465065
iteration 1286: loss 0.4135347306728363
iteration 1287: loss 0.3915463984012604
iteration 1288: loss 0.4146973192691803
iteration 1289: loss 0.3902544379234314
iteration 1290: loss 0.45507103204727173
iteration 1291: loss 0.41207218170166016
iteration 1292: loss 0.4140337407588959
iteration 1293: loss 0.4730117917060852
iteration 1294: loss 0.4356609284877777
iteration 1295: loss 0.44320571422576904
iteration 1296: loss 0.4217996299266815
iteration 1297: loss 0.4037207365036011
iteration 1298: loss 0.4437694549560547
iteration 1299: loss 0.4173021912574768
iteration 1300: loss 0.4890649914741516
iteration 1301: loss 0.380018025636673
iteration 1302: loss 0.4267740249633789
iteration 1303: loss 0.49113231897354126
iteration 1304: loss 0.39421480894088745
iteration 1305: loss 0.46802666783332825
iteration 1306: loss 0.39566555619239807
iteration 1307: loss 0.44340983033180237
iteration 1308: loss 0.463412344455719
iteration 1309: loss 0.48137572407722473
iteration 1310: loss 0.42069172859191895
iteration 1311: loss 0.3998487591743469
iteration 1312: loss 0.45827049016952515
iteration 1313: loss 0.49836885929107666
iteration 1314: loss 0.42954060435295105
iteration 1315: loss 0.4598104953765869
iteration 1316: loss 0.4177590012550354
iteration 1317: loss 0.3905473053455353
iteration 1318: loss 0.4721336364746094
iteration 1319: loss 0.45039623975753784
iteration 1320: loss 0.4442090392112732
iteration 1321: loss 0.4140162467956543
iteration 1322: loss 0.4461786448955536
iteration 1323: loss 0.49151191115379333
iteration 1324: loss 0.38639816641807556
iteration 1325: loss 0.38484853506088257
iteration 1326: loss 0.465788871049881
iteration 1327: loss 0.4215468764305115
iteration 1328: loss 0.44563451409339905
iteration 1329: loss 0.42711034417152405
iteration 1330: loss 0.40125420689582825
iteration 1331: loss 0.3880886137485504
iteration 1332: loss 0.41048499941825867
iteration 1333: loss 0.3709006905555725
iteration 1334: loss 0.4166325330734253
iteration 1335: loss 0.4310862720012665
iteration 1336: loss 0.3787009119987488
iteration 1337: loss 0.38149622082710266
iteration 1338: loss 0.4518106281757355
iteration 1339: loss 0.3954464793205261
iteration 1340: loss 0.37461376190185547
iteration 1341: loss 0.3816981613636017
iteration 1342: loss 0.4946605861186981
iteration 1343: loss 0.4259137511253357
iteration 1344: loss 0.4316014349460602
iteration 1345: loss 0.5274548530578613
iteration 1346: loss 0.4412097930908203
iteration 1347: loss 0.40779203176498413
iteration 1348: loss 0.4009566903114319
iteration 1349: loss 0.4098995625972748
iteration 1350: loss 0.39997628331184387
iteration 1351: loss 0.44458258152008057
iteration 1352: loss 0.38980603218078613
iteration 1353: loss 0.3797357678413391
iteration 1354: loss 0.4197768568992615
iteration 1355: loss 0.46745267510414124
iteration 1356: loss 0.45732712745666504
iteration 1357: loss 0.40660181641578674
iteration 1358: loss 0.4334763288497925
iteration 1359: loss 0.3764853775501251
iteration 1360: loss 0.4691373407840729
iteration 1361: loss 0.43544116616249084
iteration 1362: loss 0.42411741614341736
iteration 1363: loss 0.4435614049434662
iteration 1364: loss 0.4770715832710266
iteration 1365: loss 0.4441102147102356
iteration 1366: loss 0.44631290435791016
iteration 1367: loss 0.44102442264556885
iteration 1368: loss 0.45049354434013367
iteration 1369: loss 0.4040396213531494
iteration 1370: loss 0.3809100389480591
iteration 1371: loss 0.4246878921985626
iteration 1372: loss 0.4417978525161743
iteration 1373: loss 0.40777307748794556
iteration 1374: loss 0.4316287636756897
iteration 1375: loss 0.37925344705581665
iteration 1376: loss 0.36998313665390015
iteration 1377: loss 0.3978792726993561
iteration 1378: loss 0.4127819836139679
iteration 1379: loss 0.4607490301132202
iteration 1380: loss 0.39135023951530457
iteration 1381: loss 0.40390411019325256
iteration 1382: loss 0.4020218253135681
iteration 1383: loss 0.3953668773174286
iteration 1384: loss 0.5096971988677979
iteration 1385: loss 0.5057470798492432
iteration 1386: loss 0.3907165229320526
iteration 1387: loss 0.406827449798584
iteration 1388: loss 0.4279736280441284
iteration 1389: loss 0.43238869309425354
iteration 1390: loss 0.39388373494148254
iteration 1391: loss 0.45450982451438904
iteration 1392: loss 0.3994559049606323
iteration 1393: loss 0.46540307998657227
iteration 1394: loss 0.4435585141181946
iteration 1395: loss 0.4628487229347229
iteration 1396: loss 0.38391754031181335
iteration 1397: loss 0.47121185064315796
iteration 1398: loss 0.3247296214103699
iteration 1399: loss 0.4416007101535797
iteration 1400: loss 0.36942750215530396
iteration 1401: loss 0.49638622999191284
iteration 1402: loss 0.5368257761001587
iteration 1403: loss 0.40587320923805237
iteration 1404: loss 0.38610273599624634
iteration 1405: loss 0.5193483233451843
iteration 1406: loss 0.41102054715156555
iteration 1407: loss 0.42090433835983276
iteration 1408: loss 0.42827457189559937
iteration 1409: loss 0.400166392326355
iteration 1410: loss 0.4427339434623718
iteration 1411: loss 0.4713042080402374
iteration 1412: loss 0.3734695613384247
iteration 1413: loss 0.35740095376968384
iteration 1414: loss 0.43706604838371277
iteration 1415: loss 0.43025147914886475
iteration 1416: loss 0.40174686908721924
iteration 1417: loss 0.4276352822780609
iteration 1418: loss 0.38210225105285645
iteration 1419: loss 0.4733525216579437
iteration 1420: loss 0.4348808825016022
iteration 1421: loss 0.4559924900531769
iteration 1422: loss 0.42140933871269226
iteration 1423: loss 0.3995771110057831
iteration 1424: loss 0.36961668729782104
iteration 1425: loss 0.35470888018608093
iteration 1426: loss 0.38391831517219543
iteration 1427: loss 0.39438918232917786
iteration 1428: loss 0.37165820598602295
iteration 1429: loss 0.3822740316390991
iteration 1430: loss 0.414585679769516
iteration 1431: loss 0.45204782485961914
iteration 1432: loss 0.40027594566345215
iteration 1433: loss 0.49110105633735657
iteration 1434: loss 0.41694876551628113
iteration 1435: loss 0.5013590455055237
iteration 1436: loss 0.37670597434043884
iteration 1437: loss 0.4071548283100128
iteration 1438: loss 0.37723296880722046
iteration 1439: loss 0.4097689092159271
iteration 1440: loss 0.43046849966049194
iteration 1441: loss 0.45967745780944824
iteration 1442: loss 0.4354247748851776
iteration 1443: loss 0.40936747193336487
iteration 1444: loss 0.41041791439056396
iteration 1445: loss 0.3645065128803253
iteration 1446: loss 0.4268612861633301
iteration 1447: loss 0.4359898269176483
iteration 1448: loss 0.40421757102012634
iteration 1449: loss 0.3880058825016022
iteration 1450: loss 0.4589875340461731
iteration 1451: loss 0.43820375204086304
iteration 1452: loss 0.4834809899330139
iteration 1453: loss 0.473031222820282
iteration 1454: loss 0.4101523756980896
iteration 1455: loss 0.3471575379371643
iteration 1456: loss 0.45805639028549194
iteration 1457: loss 0.44724389910697937
iteration 1458: loss 0.4414152503013611
iteration 1459: loss 0.40843603014945984
iteration 1460: loss 0.40731117129325867
iteration 1461: loss 0.4795409142971039
iteration 1462: loss 0.3361159861087799
iteration 1463: loss 0.4505510628223419
iteration 1464: loss 0.44353896379470825
iteration 1465: loss 0.32697102427482605
iteration 1466: loss 0.3863712549209595
iteration 1467: loss 0.39233943819999695
iteration 1468: loss 0.41517174243927
iteration 1469: loss 0.3570888638496399
iteration 1470: loss 0.3809874653816223
iteration 1471: loss 0.3937033414840698
iteration 1472: loss 0.3908832371234894
iteration 1473: loss 0.3513522744178772
iteration 1474: loss 0.417523592710495
iteration 1475: loss 0.4019257128238678
iteration 1476: loss 0.4014418125152588
iteration 1477: loss 0.40206781029701233
iteration 1478: loss 0.377152681350708
iteration 1479: loss 0.4146176874637604
iteration 1480: loss 0.38045448064804077
iteration 1481: loss 0.3718003034591675
iteration 1482: loss 0.43917036056518555
iteration 1483: loss 0.43406468629837036
iteration 1484: loss 0.3911725580692291
iteration 1485: loss 0.4288371503353119
iteration 1486: loss 0.38940349221229553
iteration 1487: loss 0.39423802495002747
iteration 1488: loss 0.4436972141265869
iteration 1489: loss 0.4697912335395813
iteration 1490: loss 0.3516900837421417
iteration 1491: loss 0.3871105909347534
iteration 1492: loss 0.4409559965133667
iteration 1493: loss 0.4009030759334564
iteration 1494: loss 0.40273821353912354
iteration 1495: loss 0.39695754647254944
iteration 1496: loss 0.46579715609550476
iteration 1497: loss 0.41340574622154236
iteration 1498: loss 0.4406769573688507
iteration 1499: loss 0.3512425720691681
iteration 1500: loss 0.3736792504787445
iteration 1501: loss 0.3982143998146057
iteration 1502: loss 0.4081099331378937
iteration 1503: loss 0.469789057970047
iteration 1504: loss 0.4170253574848175
iteration 1505: loss 0.4334125816822052
iteration 1506: loss 0.3777511417865753
iteration 1507: loss 0.3486354649066925
iteration 1508: loss 0.3823792338371277
iteration 1509: loss 0.4345428943634033
iteration 1510: loss 0.355247437953949
iteration 1511: loss 0.4147108197212219
iteration 1512: loss 0.42583316564559937
iteration 1513: loss 0.43011122941970825
iteration 1514: loss 0.4012242555618286
iteration 1515: loss 0.4754126965999603
iteration 1516: loss 0.4540445804595947
iteration 1517: loss 0.36771607398986816
iteration 1518: loss 0.46612271666526794
iteration 1519: loss 0.4135514497756958
iteration 1520: loss 0.3841286897659302
iteration 1521: loss 0.43019184470176697
iteration 1522: loss 0.39396706223487854
iteration 1523: loss 0.3941672742366791
iteration 1524: loss 0.41942769289016724
iteration 1525: loss 0.45710670948028564
iteration 1526: loss 0.4177168011665344
iteration 1527: loss 0.4010409712791443
iteration 1528: loss 0.401790589094162
iteration 1529: loss 0.4257182478904724
iteration 1530: loss 0.32905468344688416
iteration 1531: loss 0.42610323429107666
iteration 1532: loss 0.36417898535728455
iteration 1533: loss 0.39847996830940247
iteration 1534: loss 0.4665383994579315
iteration 1535: loss 0.44176149368286133
iteration 1536: loss 0.3968905806541443
iteration 1537: loss 0.446698933839798
iteration 1538: loss 0.4670898914337158
iteration 1539: loss 0.3813477158546448
iteration 1540: loss 0.4352043569087982
iteration 1541: loss 0.3479408025741577
iteration 1542: loss 0.5592922568321228
iteration 1543: loss 0.4173571467399597
iteration 1544: loss 0.38287732005119324
iteration 1545: loss 0.4017286002635956
iteration 1546: loss 0.35537171363830566
iteration 1547: loss 0.3589048683643341
iteration 1548: loss 0.397009015083313
iteration 1549: loss 0.4094957113265991
iteration 1550: loss 0.35612979531288147
iteration 1551: loss 0.4533923864364624
iteration 1552: loss 0.3779379427433014
iteration 1553: loss 0.3797990679740906
iteration 1554: loss 0.4068129360675812
iteration 1555: loss 0.3588292896747589
iteration 1556: loss 0.41194769740104675
iteration 1557: loss 0.42439043521881104
iteration 1558: loss 0.39879047870635986
iteration 1559: loss 0.3901470899581909
iteration 1560: loss 0.4226556420326233
iteration 1561: loss 0.37791457772254944
iteration 1562: loss 0.33392831683158875
iteration 1563: loss 0.42480695247650146
iteration 1564: loss 0.44315680861473083
iteration 1565: loss 0.40744882822036743
iteration 1566: loss 0.39305928349494934
iteration 1567: loss 0.3588396906852722
iteration 1568: loss 0.37592625617980957
iteration 1569: loss 0.44840654730796814
iteration 1570: loss 0.3732787072658539
iteration 1571: loss 0.41235122084617615
iteration 1572: loss 0.42418497800827026
iteration 1573: loss 0.3253757059574127
iteration 1574: loss 0.34742069244384766
iteration 1575: loss 0.414091020822525
iteration 1576: loss 0.4355492889881134
iteration 1577: loss 0.3773781657218933
iteration 1578: loss 0.44473540782928467
iteration 1579: loss 0.41840434074401855
iteration 1580: loss 0.4538813531398773
iteration 1581: loss 0.39111602306365967
iteration 1582: loss 0.40952736139297485
iteration 1583: loss 0.4438841938972473
iteration 1584: loss 0.38344407081604004
iteration 1585: loss 0.3556872010231018
iteration 1586: loss 0.39401936531066895
iteration 1587: loss 0.42958569526672363
iteration 1588: loss 0.38853228092193604
iteration 1589: loss 0.42067378759384155
iteration 1590: loss 0.35337892174720764
iteration 1591: loss 0.37836453318595886
iteration 1592: loss 0.407695472240448
iteration 1593: loss 0.35965484380722046
iteration 1594: loss 0.4399898648262024
iteration 1595: loss 0.3830345869064331
iteration 1596: loss 0.38779258728027344
iteration 1597: loss 0.3534030616283417
iteration 1598: loss 0.36509427428245544
iteration 1599: loss 0.4084266126155853
iteration 1600: loss 0.4313035309314728
iteration 1601: loss 0.3718208372592926
iteration 1602: loss 0.36607056856155396
iteration 1603: loss 0.38534924387931824
iteration 1604: loss 0.44359830021858215
iteration 1605: loss 0.39756983518600464
iteration 1606: loss 0.4168177545070648
iteration 1607: loss 0.36627236008644104
iteration 1608: loss 0.34711551666259766
iteration 1609: loss 0.414298951625824
iteration 1610: loss 0.4597088694572449
iteration 1611: loss 0.3245493173599243
iteration 1612: loss 0.3855752646923065
iteration 1613: loss 0.37694066762924194
iteration 1614: loss 0.4316217601299286
iteration 1615: loss 0.34011369943618774
iteration 1616: loss 0.36601951718330383
iteration 1617: loss 0.3905000686645508
iteration 1618: loss 0.3569200932979584
iteration 1619: loss 0.36886534094810486
iteration 1620: loss 0.3919697701931
iteration 1621: loss 0.3979989290237427
iteration 1622: loss 0.34623628854751587
iteration 1623: loss 0.38813164830207825
iteration 1624: loss 0.4471578299999237
iteration 1625: loss 0.4062153995037079
iteration 1626: loss 0.4122897982597351
iteration 1627: loss 0.45176607370376587
iteration 1628: loss 0.3633078634738922
iteration 1629: loss 0.3851170539855957
iteration 1630: loss 0.42274561524391174
iteration 1631: loss 0.3984985947608948
iteration 1632: loss 0.37634438276290894
iteration 1633: loss 0.377269446849823
iteration 1634: loss 0.3898989260196686
iteration 1635: loss 0.3792639672756195
iteration 1636: loss 0.3669483959674835
iteration 1637: loss 0.3547877073287964
iteration 1638: loss 0.4083499610424042
iteration 1639: loss 0.39739885926246643
iteration 1640: loss 0.46715351939201355
iteration 1641: loss 0.394680917263031
iteration 1642: loss 0.4457888901233673
iteration 1643: loss 0.429641455411911
iteration 1644: loss 0.48446235060691833
iteration 1645: loss 0.42187538743019104
iteration 1646: loss 0.3540552258491516
iteration 1647: loss 0.41649651527404785
iteration 1648: loss 0.44896459579467773
iteration 1649: loss 0.3627215325832367
iteration 1650: loss 0.3743886351585388
iteration 1651: loss 0.4204584062099457
iteration 1652: loss 0.38008031249046326
iteration 1653: loss 0.390763521194458
iteration 1654: loss 0.3479400873184204
iteration 1655: loss 0.3467770218849182
iteration 1656: loss 0.3731461465358734
iteration 1657: loss 0.4124157130718231
iteration 1658: loss 0.37638506293296814
iteration 1659: loss 0.46280357241630554
iteration 1660: loss 0.3648928701877594
iteration 1661: loss 0.3763825595378876
iteration 1662: loss 0.377323180437088
iteration 1663: loss 0.35168349742889404
iteration 1664: loss 0.34420984983444214
iteration 1665: loss 0.35408246517181396
iteration 1666: loss 0.3725234270095825
iteration 1667: loss 0.4172315001487732
iteration 1668: loss 0.32121753692626953
iteration 1669: loss 0.41245660185813904
iteration 1670: loss 0.46923571825027466
iteration 1671: loss 0.3316255807876587
iteration 1672: loss 0.42936721444129944
iteration 1673: loss 0.3668895661830902
iteration 1674: loss 0.41347211599349976
iteration 1675: loss 0.39032483100891113
iteration 1676: loss 0.37191110849380493
iteration 1677: loss 0.35918352007865906
iteration 1678: loss 0.4205350875854492
iteration 1679: loss 0.3864307999610901
iteration 1680: loss 0.3283982574939728
iteration 1681: loss 0.2921895980834961
iteration 1682: loss 0.3669445514678955
iteration 1683: loss 0.41912853717803955
iteration 1684: loss 0.36532339453697205
iteration 1685: loss 0.3673999011516571
iteration 1686: loss 0.3548277020454407
iteration 1687: loss 0.42470040917396545
iteration 1688: loss 0.3778083920478821
iteration 1689: loss 0.4160524904727936
iteration 1690: loss 0.3574030101299286
iteration 1691: loss 0.41848549246788025
iteration 1692: loss 0.3889751136302948
iteration 1693: loss 0.3775840401649475
iteration 1694: loss 0.4200563430786133
iteration 1695: loss 0.344570130109787
iteration 1696: loss 0.42182591557502747
iteration 1697: loss 0.35162216424942017
iteration 1698: loss 0.33108213543891907
iteration 1699: loss 0.35852736234664917
iteration 1700: loss 0.391139417886734
iteration 1701: loss 0.39864253997802734
iteration 1702: loss 0.38103848695755005
iteration 1703: loss 0.39482569694519043
iteration 1704: loss 0.40386763215065
iteration 1705: loss 0.4035851061344147
iteration 1706: loss 0.41165027022361755
iteration 1707: loss 0.41016530990600586
iteration 1708: loss 0.34049755334854126
iteration 1709: loss 0.42280417680740356
iteration 1710: loss 0.3876306414604187
iteration 1711: loss 0.41343310475349426
iteration 1712: loss 0.35198044776916504
iteration 1713: loss 0.3921148180961609
iteration 1714: loss 0.31766343116760254
iteration 1715: loss 0.55097496509552
iteration 1716: loss 0.4006333649158478
iteration 1717: loss 0.38480180501937866
iteration 1718: loss 0.4065917134284973
iteration 1719: loss 0.35401591658592224
iteration 1720: loss 0.4307589530944824
iteration 1721: loss 0.3898225426673889
iteration 1722: loss 0.4292220175266266
iteration 1723: loss 0.3241709768772125
iteration 1724: loss 0.33009496331214905
iteration 1725: loss 0.5085363388061523
iteration 1726: loss 0.3695595860481262
iteration 1727: loss 0.34941375255584717
iteration 1728: loss 0.3659653961658478
iteration 1729: loss 0.4527255892753601
iteration 1730: loss 0.3764416575431824
iteration 1731: loss 0.3804877698421478
iteration 1732: loss 0.380407452583313
iteration 1733: loss 0.40058863162994385
iteration 1734: loss 0.4376614987850189
iteration 1735: loss 0.3631381690502167
iteration 1736: loss 0.36342906951904297
iteration 1737: loss 0.40155768394470215
iteration 1738: loss 0.39252835512161255
iteration 1739: loss 0.3878606855869293
iteration 1740: loss 0.4030289053916931
iteration 1741: loss 0.3614569902420044
iteration 1742: loss 0.39195239543914795
iteration 1743: loss 0.40099799633026123
iteration 1744: loss 0.3992117941379547
iteration 1745: loss 0.3384714722633362
iteration 1746: loss 0.39982718229293823
iteration 1747: loss 0.3674778640270233
iteration 1748: loss 0.39763733744621277
iteration 1749: loss 0.39635011553764343
iteration 1750: loss 0.34055691957473755
iteration 1751: loss 0.4048033654689789
iteration 1752: loss 0.3901619017124176
iteration 1753: loss 0.44188836216926575
iteration 1754: loss 0.4304475784301758
iteration 1755: loss 0.3976798355579376
iteration 1756: loss 0.3839591145515442
iteration 1757: loss 0.3637038469314575
iteration 1758: loss 0.3783983588218689
iteration 1759: loss 0.3079792261123657
iteration 1760: loss 0.4184727966785431
iteration 1761: loss 0.39323392510414124
iteration 1762: loss 0.3460773825645447
iteration 1763: loss 0.45291003584861755
iteration 1764: loss 0.4205103814601898
iteration 1765: loss 0.39222753047943115
iteration 1766: loss 0.3523758351802826
iteration 1767: loss 0.3401985764503479
iteration 1768: loss 0.34194305539131165
iteration 1769: loss 0.4169239401817322
iteration 1770: loss 0.4010787904262543
iteration 1771: loss 0.33487218618392944
iteration 1772: loss 0.4208628237247467
iteration 1773: loss 0.3807332217693329
iteration 1774: loss 0.40800169110298157
iteration 1775: loss 0.38434502482414246
iteration 1776: loss 0.3989183306694031
iteration 1777: loss 0.3639633059501648
iteration 1778: loss 0.3465439975261688
iteration 1779: loss 0.3911137282848358
iteration 1780: loss 0.4185936748981476
iteration 1781: loss 0.38471266627311707
iteration 1782: loss 0.31987905502319336
iteration 1783: loss 0.3369848430156708
iteration 1784: loss 0.3336052894592285
iteration 1785: loss 0.3431236445903778
iteration 1786: loss 0.3821465075016022
iteration 1787: loss 0.3435083031654358
iteration 1788: loss 0.37428611516952515
iteration 1789: loss 0.38719043135643005
iteration 1790: loss 0.394521564245224
iteration 1791: loss 0.3893362581729889
iteration 1792: loss 0.3208450675010681
iteration 1793: loss 0.4018194079399109
iteration 1794: loss 0.35943156480789185
iteration 1795: loss 0.3901834785938263
iteration 1796: loss 0.34619206190109253
iteration 1797: loss 0.41559886932373047
iteration 1798: loss 0.3659428656101227
iteration 1799: loss 0.40339434146881104
iteration 1800: loss 0.39435702562332153
iteration 1801: loss 0.3663256764411926
iteration 1802: loss 0.4802001714706421
iteration 1803: loss 0.3565007150173187
iteration 1804: loss 0.33957889676094055
iteration 1805: loss 0.41772881150245667
iteration 1806: loss 0.41260528564453125
iteration 1807: loss 0.3772580325603485
iteration 1808: loss 0.37593701481819153
iteration 1809: loss 0.45070168375968933
iteration 1810: loss 0.42296040058135986
iteration 1811: loss 0.3058719038963318
iteration 1812: loss 0.3788398504257202
iteration 1813: loss 0.33920642733573914
iteration 1814: loss 0.36824363470077515
iteration 1815: loss 0.4001537561416626
iteration 1816: loss 0.39809533953666687
iteration 1817: loss 0.36531567573547363
iteration 1818: loss 0.4217483699321747
iteration 1819: loss 0.38057249784469604
iteration 1820: loss 0.3940650522708893
iteration 1821: loss 0.44263598322868347
iteration 1822: loss 0.37462118268013
iteration 1823: loss 0.3751493990421295
iteration 1824: loss 0.3131834864616394
iteration 1825: loss 0.3670790493488312
iteration 1826: loss 0.4185604453086853
iteration 1827: loss 0.3483750522136688
iteration 1828: loss 0.40569597482681274
iteration 1829: loss 0.3967094123363495
iteration 1830: loss 0.370021253824234
iteration 1831: loss 0.36298438906669617
iteration 1832: loss 0.381047785282135
iteration 1833: loss 0.399250864982605
iteration 1834: loss 0.39808130264282227
iteration 1835: loss 0.351978063583374
iteration 1836: loss 0.3687816560268402
iteration 1837: loss 0.34747594594955444
iteration 1838: loss 0.3798537254333496
iteration 1839: loss 0.34867867827415466
iteration 1840: loss 0.34921857714653015
iteration 1841: loss 0.3668656647205353
iteration 1842: loss 0.3678281009197235
iteration 1843: loss 0.3879172205924988
iteration 1844: loss 0.3675215244293213
iteration 1845: loss 0.3689741790294647
iteration 1846: loss 0.41059547662734985
iteration 1847: loss 0.4449748992919922
iteration 1848: loss 0.3571232259273529
iteration 1849: loss 0.312267005443573
iteration 1850: loss 0.3707442581653595
iteration 1851: loss 0.48591944575309753
iteration 1852: loss 0.3807579576969147
iteration 1853: loss 0.3843899071216583
iteration 1854: loss 0.2984859049320221
iteration 1855: loss 0.4002127945423126
iteration 1856: loss 0.3549216091632843
iteration 1857: loss 0.3876335024833679
iteration 1858: loss 0.4460689425468445
iteration 1859: loss 0.42904427647590637
iteration 1860: loss 0.41790246963500977
iteration 1861: loss 0.3554663360118866
iteration 1862: loss 0.32791388034820557
iteration 1863: loss 0.3714980483055115
iteration 1864: loss 0.34359312057495117
iteration 1865: loss 0.3845943808555603
iteration 1866: loss 0.3707227408885956
iteration 1867: loss 0.36147934198379517
iteration 1868: loss 0.37739384174346924
iteration 1869: loss 0.40312325954437256
iteration 1870: loss 0.39275312423706055
iteration 1871: loss 0.32977887988090515
iteration 1872: loss 0.35248878598213196
iteration 1873: loss 0.32549920678138733
iteration 1874: loss 0.37304267287254333
iteration 1875: loss 0.39297643303871155
iteration 1876: loss 0.397174209356308
iteration 1877: loss 0.396596759557724
iteration 1878: loss 0.4339483082294464
iteration 1879: loss 0.3546167314052582
iteration 1880: loss 0.35839495062828064
iteration 1881: loss 0.363985538482666
iteration 1882: loss 0.38976243138313293
iteration 1883: loss 0.3896937072277069
iteration 1884: loss 0.36946842074394226
iteration 1885: loss 0.37310507893562317
iteration 1886: loss 0.4001559615135193
iteration 1887: loss 0.3414043188095093
iteration 1888: loss 0.3183160126209259
iteration 1889: loss 0.3706837296485901
iteration 1890: loss 0.3542174994945526
iteration 1891: loss 0.4223432242870331
iteration 1892: loss 0.3800305426120758
iteration 1893: loss 0.32824942469596863
iteration 1894: loss 0.33210572600364685
iteration 1895: loss 0.33282116055488586
iteration 1896: loss 0.34258756041526794
iteration 1897: loss 0.33597975969314575
iteration 1898: loss 0.3722626566886902
iteration 1899: loss 0.38839346170425415
iteration 1900: loss 0.36365807056427
iteration 1901: loss 0.35736867785453796
iteration 1902: loss 0.427253782749176
iteration 1903: loss 0.3340209424495697
iteration 1904: loss 0.37580248713493347
iteration 1905: loss 0.39459359645843506
iteration 1906: loss 0.415066123008728
iteration 1907: loss 0.3807425796985626
iteration 1908: loss 0.40451470017433167
iteration 1909: loss 0.35088440775871277
iteration 1910: loss 0.3920825123786926
iteration 1911: loss 0.36537888646125793
iteration 1912: loss 0.3301340937614441
iteration 1913: loss 0.3792053461074829
iteration 1914: loss 0.3894347846508026
iteration 1915: loss 0.3132219612598419
iteration 1916: loss 0.3282844126224518
iteration 1917: loss 0.3321359157562256
iteration 1918: loss 0.3906971514225006
iteration 1919: loss 0.4280712306499481
iteration 1920: loss 0.3318888247013092
iteration 1921: loss 0.3734091520309448
iteration 1922: loss 0.4110639691352844
iteration 1923: loss 0.35152044892311096
iteration 1924: loss 0.4127984941005707
iteration 1925: loss 0.3624892234802246
iteration 1926: loss 0.35468772053718567
iteration 1927: loss 0.3652228116989136
iteration 1928: loss 0.38055291771888733
iteration 1929: loss 0.37352561950683594
iteration 1930: loss 0.33186978101730347
iteration 1931: loss 0.3865210711956024
iteration 1932: loss 0.3646720051765442
iteration 1933: loss 0.3611733019351959
iteration 1934: loss 0.3218856751918793
iteration 1935: loss 0.3477582633495331
iteration 1936: loss 0.3326856195926666
iteration 1937: loss 0.3367789089679718
iteration 1938: loss 0.3788975179195404
iteration 1939: loss 0.4902443289756775
iteration 1940: loss 0.3171568810939789
iteration 1941: loss 0.3801209032535553
iteration 1942: loss 0.39792582392692566
iteration 1943: loss 0.3901069164276123
iteration 1944: loss 0.40308496356010437
iteration 1945: loss 0.40220922231674194
iteration 1946: loss 0.3179291784763336
iteration 1947: loss 0.349380761384964
iteration 1948: loss 0.353923499584198
iteration 1949: loss 0.366128534078598
iteration 1950: loss 0.435112327337265
iteration 1951: loss 0.36098209023475647
iteration 1952: loss 0.34925171732902527
iteration 1953: loss 0.36341527104377747
iteration 1954: loss 0.3805651366710663
iteration 1955: loss 0.3180813193321228
iteration 1956: loss 0.3362526595592499
iteration 1957: loss 0.36469021439552307
iteration 1958: loss 0.3515206277370453
iteration 1959: loss 0.4003244638442993
iteration 1960: loss 0.3509371876716614
iteration 1961: loss 0.3176705837249756
iteration 1962: loss 0.3659804165363312
iteration 1963: loss 0.40492963790893555
iteration 1964: loss 0.3481597304344177
iteration 1965: loss 0.33718341588974
iteration 1966: loss 0.3626936674118042
iteration 1967: loss 0.3247145116329193
iteration 1968: loss 0.45039114356040955
iteration 1969: loss 0.39310553669929504
iteration 1970: loss 0.4062550961971283
iteration 1971: loss 0.3452188968658447
iteration 1972: loss 0.36116310954093933
iteration 1973: loss 0.3999345302581787
iteration 1974: loss 0.29770806431770325
iteration 1975: loss 0.3611271381378174
iteration 1976: loss 0.3938869833946228
iteration 1977: loss 0.33552199602127075
iteration 1978: loss 0.35726577043533325
iteration 1979: loss 0.33003613352775574
iteration 1980: loss 0.33853673934936523
iteration 1981: loss 0.36462870240211487
iteration 1982: loss 0.41335996985435486
iteration 1983: loss 0.35319411754608154
iteration 1984: loss 0.300780713558197
iteration 1985: loss 0.3747968375682831
iteration 1986: loss 0.418474018573761
iteration 1987: loss 0.4062623381614685
iteration 1988: loss 0.33711904287338257
iteration 1989: loss 0.3617146611213684
iteration 1990: loss 0.35862085223197937
iteration 1991: loss 0.36750805377960205
iteration 1992: loss 0.32600632309913635
iteration 1993: loss 0.3420315980911255
iteration 1994: loss 0.3957225978374481
iteration 1995: loss 0.4059915542602539
iteration 1996: loss 0.32397565245628357
iteration 1997: loss 0.38213667273521423
iteration 1998: loss 0.3520262837409973
iteration 1999: loss 0.3192223906517029
iteration 2000: loss 0.36730289459228516
iteration 2001: loss 0.34289607405662537
iteration 2002: loss 0.381000816822052
iteration 2003: loss 0.33926138281822205
iteration 2004: loss 0.35940301418304443
iteration 2005: loss 0.3055802285671234
iteration 2006: loss 0.3616963028907776
iteration 2007: loss 0.3293566405773163
iteration 2008: loss 0.3811536431312561
iteration 2009: loss 0.35477474331855774
iteration 2010: loss 0.44114312529563904
iteration 2011: loss 0.40010589361190796
iteration 2012: loss 0.34922900795936584
iteration 2013: loss 0.3472979664802551
iteration 2014: loss 0.28239110112190247
iteration 2015: loss 0.35889294743537903
iteration 2016: loss 0.35475820302963257
iteration 2017: loss 0.33459845185279846
iteration 2018: loss 0.37763112783432007
iteration 2019: loss 0.3257831037044525
iteration 2020: loss 0.33747491240501404
iteration 2021: loss 0.334900438785553
iteration 2022: loss 0.3844354450702667
iteration 2023: loss 0.39919742941856384
iteration 2024: loss 0.34099119901657104
iteration 2025: loss 0.3492095172405243
iteration 2026: loss 0.3233173191547394
iteration 2027: loss 0.3549751937389374
iteration 2028: loss 0.36596158146858215
iteration 2029: loss 0.3325751721858978
iteration 2030: loss 0.4133104085922241
iteration 2031: loss 0.3438636362552643
iteration 2032: loss 0.32340219616889954
iteration 2033: loss 0.4082181751728058
iteration 2034: loss 0.3976231813430786
iteration 2035: loss 0.33011308312416077
iteration 2036: loss 0.39179661870002747
iteration 2037: loss 0.3755978047847748
iteration 2038: loss 0.35054007172584534
iteration 2039: loss 0.3550663888454437
iteration 2040: loss 0.4257301986217499
iteration 2041: loss 0.35865601897239685
iteration 2042: loss 0.39428630471229553
iteration 2043: loss 0.38320785760879517
iteration 2044: loss 0.3551303446292877
iteration 2045: loss 0.34069037437438965
iteration 2046: loss 0.3842328190803528
iteration 2047: loss 0.3672294020652771
iteration 2048: loss 0.3892487585544586
iteration 2049: loss 0.35467442870140076
iteration 2050: loss 0.289284348487854
iteration 2051: loss 0.41088971495628357
iteration 2052: loss 0.5241209268569946
iteration 2053: loss 0.43302279710769653
iteration 2054: loss 0.4007987380027771
iteration 2055: loss 0.426424503326416
iteration 2056: loss 0.37809649109840393
iteration 2057: loss 0.3636397123336792
iteration 2058: loss 0.4157716631889343
iteration 2059: loss 0.32180875539779663
iteration 2060: loss 0.4002118408679962
iteration 2061: loss 0.36703142523765564
iteration 2062: loss 0.3646221160888672
iteration 2063: loss 0.3762425184249878
iteration 2064: loss 0.35652413964271545
iteration 2065: loss 0.3285999298095703
iteration 2066: loss 0.35972848534584045
iteration 2067: loss 0.34649696946144104
iteration 2068: loss 0.4298432469367981
iteration 2069: loss 0.3791564106941223
iteration 2070: loss 0.3714081346988678
iteration 2071: loss 0.3027842938899994
iteration 2072: loss 0.36953791975975037
iteration 2073: loss 0.3062213659286499
iteration 2074: loss 0.33204567432403564
iteration 2075: loss 0.348388135433197
iteration 2076: loss 0.41689664125442505
iteration 2077: loss 0.37582144141197205
iteration 2078: loss 0.3541257679462433
iteration 2079: loss 0.39898839592933655
iteration 2080: loss 0.32191935181617737
iteration 2081: loss 0.39173945784568787
iteration 2082: loss 0.3856264054775238
iteration 2083: loss 0.3856029808521271
iteration 2084: loss 0.3459700345993042
iteration 2085: loss 0.38240957260131836
iteration 2086: loss 0.3447727560997009
iteration 2087: loss 0.29748421907424927
iteration 2088: loss 0.33611899614334106
iteration 2089: loss 0.3912487030029297
iteration 2090: loss 0.434490442276001
iteration 2091: loss 0.3136197328567505
iteration 2092: loss 0.3569469451904297
iteration 2093: loss 0.34392133355140686
iteration 2094: loss 0.36612609028816223
iteration 2095: loss 0.34325864911079407
iteration 2096: loss 0.3034048080444336
iteration 2097: loss 0.36786457896232605
iteration 2098: loss 0.3778718113899231
iteration 2099: loss 0.41658300161361694
iteration 2100: loss 0.3532429039478302
iteration 2101: loss 0.3563859462738037
iteration 2102: loss 0.40016916394233704
iteration 2103: loss 0.29179900884628296
iteration 2104: loss 0.3879643380641937
iteration 2105: loss 0.378073513507843
iteration 2106: loss 0.40171995759010315
iteration 2107: loss 0.3472672402858734
iteration 2108: loss 0.3490043878555298
iteration 2109: loss 0.2610257565975189
iteration 2110: loss 0.3420274555683136
iteration 2111: loss 0.32310640811920166
iteration 2112: loss 0.32095640897750854
iteration 2113: loss 0.35045504570007324
iteration 2114: loss 0.3424292504787445
iteration 2115: loss 0.35003137588500977
iteration 2116: loss 0.3184230625629425
iteration 2117: loss 0.3601958751678467
iteration 2118: loss 0.35792189836502075
iteration 2119: loss 0.3307388424873352
iteration 2120: loss 0.3846144378185272
iteration 2121: loss 0.3714263439178467
iteration 2122: loss 0.36858701705932617
iteration 2123: loss 0.32257118821144104
iteration 2124: loss 0.3257114589214325
iteration 2125: loss 0.3603067398071289
iteration 2126: loss 0.32352373003959656
iteration 2127: loss 0.3267747461795807
iteration 2128: loss 0.3544352948665619
iteration 2129: loss 0.3193656802177429
iteration 2130: loss 0.35664674639701843
iteration 2131: loss 0.3525751531124115
iteration 2132: loss 0.3874850571155548
iteration 2133: loss 0.3379829525947571
iteration 2134: loss 0.39266136288642883
iteration 2135: loss 0.33796563744544983
iteration 2136: loss 0.35934990644454956
iteration 2137: loss 0.40627121925354004
iteration 2138: loss 0.39103564620018005
iteration 2139: loss 0.3037717640399933
iteration 2140: loss 0.32931941747665405
iteration 2141: loss 0.376910537481308
iteration 2142: loss 0.3235968053340912
iteration 2143: loss 0.366778165102005
iteration 2144: loss 0.3927857279777527
iteration 2145: loss 0.3245042860507965
iteration 2146: loss 0.4076346457004547
iteration 2147: loss 0.43450677394866943
iteration 2148: loss 0.3790459632873535
iteration 2149: loss 0.33554789423942566
iteration 2150: loss 0.33927494287490845
iteration 2151: loss 0.30851835012435913
iteration 2152: loss 0.34733131527900696
iteration 2153: loss 0.4361513555049896
iteration 2154: loss 0.3371724784374237
iteration 2155: loss 0.3402862548828125
iteration 2156: loss 0.36721739172935486
iteration 2157: loss 0.3540259003639221
iteration 2158: loss 0.34653306007385254
iteration 2159: loss 0.3360157310962677
iteration 2160: loss 0.3451531231403351
iteration 2161: loss 0.3606179356575012
iteration 2162: loss 0.32915952801704407
iteration 2163: loss 0.309171199798584
iteration 2164: loss 0.3609355390071869
iteration 2165: loss 0.3116845488548279
iteration 2166: loss 0.3644460439682007
iteration 2167: loss 0.2925739586353302
iteration 2168: loss 0.35533469915390015
iteration 2169: loss 0.3598857820034027
iteration 2170: loss 0.29185232520103455
iteration 2171: loss 0.4463103711605072
iteration 2172: loss 0.3488771319389343
iteration 2173: loss 0.3518213927745819
iteration 2174: loss 0.37674427032470703
iteration 2175: loss 0.38472235202789307
iteration 2176: loss 0.3721277415752411
iteration 2177: loss 0.3729192912578583
iteration 2178: loss 0.3536837100982666
iteration 2179: loss 0.30989009141921997
iteration 2180: loss 0.3291245400905609
iteration 2181: loss 0.37090519070625305
iteration 2182: loss 0.33969342708587646
iteration 2183: loss 0.3734821081161499
iteration 2184: loss 0.40289223194122314
iteration 2185: loss 0.3178761899471283
iteration 2186: loss 0.3758501708507538
iteration 2187: loss 0.2958797514438629
iteration 2188: loss 0.38135039806365967
iteration 2189: loss 0.32287833094596863
iteration 2190: loss 0.3571547865867615
iteration 2191: loss 0.40818914771080017
iteration 2192: loss 0.41563329100608826
iteration 2193: loss 0.33484697341918945
iteration 2194: loss 0.34081554412841797
iteration 2195: loss 0.3723941743373871
iteration 2196: loss 0.3640412986278534
iteration 2197: loss 0.40129780769348145
iteration 2198: loss 0.36807337403297424
iteration 2199: loss 0.3937976062297821
iteration 2200: loss 0.38059109449386597
iteration 2201: loss 0.34241679310798645
iteration 2202: loss 0.33785679936408997
iteration 2203: loss 0.3794976472854614
iteration 2204: loss 0.35517844557762146
iteration 2205: loss 0.3997417092323303
iteration 2206: loss 0.2643768787384033
iteration 2207: loss 0.348917692899704
iteration 2208: loss 0.3693777322769165
iteration 2209: loss 0.32990917563438416
iteration 2210: loss 0.40189123153686523
iteration 2211: loss 0.37135398387908936
iteration 2212: loss 0.33819565176963806
iteration 2213: loss 0.3358686864376068
iteration 2214: loss 0.2980867326259613
iteration 2215: loss 0.34222885966300964
iteration 2216: loss 0.4339161515235901
iteration 2217: loss 0.353097528219223
iteration 2218: loss 0.387004017829895
iteration 2219: loss 0.3602609634399414
iteration 2220: loss 0.330390065908432
iteration 2221: loss 0.31076285243034363
iteration 2222: loss 0.3324815630912781
iteration 2223: loss 0.42006242275238037
iteration 2224: loss 0.3267776370048523
iteration 2225: loss 0.3647031784057617
iteration 2226: loss 0.3156612813472748
iteration 2227: loss 0.29138752818107605
iteration 2228: loss 0.33592289686203003
iteration 2229: loss 0.3635633587837219
iteration 2230: loss 0.3426072299480438
iteration 2231: loss 0.3834759294986725
iteration 2232: loss 0.3143567740917206
iteration 2233: loss 0.3090363144874573
iteration 2234: loss 0.3633597493171692
iteration 2235: loss 0.3404315710067749
iteration 2236: loss 0.2832046449184418
iteration 2237: loss 0.3370932936668396
iteration 2238: loss 0.3366350829601288
iteration 2239: loss 0.3707756996154785
iteration 2240: loss 0.34096017479896545
iteration 2241: loss 0.33454450964927673
iteration 2242: loss 0.35899558663368225
iteration 2243: loss 0.286293089389801
iteration 2244: loss 0.3256131708621979
iteration 2245: loss 0.30825433135032654
iteration 2246: loss 0.3670867383480072
iteration 2247: loss 0.3529883027076721
iteration 2248: loss 0.29383623600006104
iteration 2249: loss 0.28563153743743896
iteration 2250: loss 0.3509363830089569
iteration 2251: loss 0.3352530300617218
iteration 2252: loss 0.2994329035282135
iteration 2253: loss 0.382037490606308
iteration 2254: loss 0.35592859983444214
iteration 2255: loss 0.3455599546432495
iteration 2256: loss 0.33302754163742065
iteration 2257: loss 0.3303089141845703
iteration 2258: loss 0.34647607803344727
iteration 2259: loss 0.2869165539741516
iteration 2260: loss 0.2848074734210968
iteration 2261: loss 0.2911170721054077
iteration 2262: loss 0.36644965410232544
iteration 2263: loss 0.30745095014572144
iteration 2264: loss 0.3197590410709381
iteration 2265: loss 0.40798306465148926
iteration 2266: loss 0.33793410658836365
iteration 2267: loss 0.3484804034233093
iteration 2268: loss 0.3339633345603943
iteration 2269: loss 0.31332337856292725
iteration 2270: loss 0.34577426314353943
iteration 2271: loss 0.36687037348747253
iteration 2272: loss 0.37571245431900024
iteration 2273: loss 0.3492512106895447
iteration 2274: loss 0.39261680841445923
iteration 2275: loss 0.36982840299606323
iteration 2276: loss 0.2847520411014557
iteration 2277: loss 0.3329177498817444
iteration 2278: loss 0.4159148037433624
iteration 2279: loss 0.34303486347198486
iteration 2280: loss 0.30039364099502563
iteration 2281: loss 0.33835163712501526
iteration 2282: loss 0.3202500641345978
iteration 2283: loss 0.3611323833465576
iteration 2284: loss 0.37011611461639404
iteration 2285: loss 0.35869333148002625
iteration 2286: loss 0.31769636273384094
iteration 2287: loss 0.3651795983314514
iteration 2288: loss 0.3125303387641907
iteration 2289: loss 0.3246608376502991
iteration 2290: loss 0.3569543659687042
iteration 2291: loss 0.27936479449272156
iteration 2292: loss 0.2882915139198303
iteration 2293: loss 0.34808236360549927
iteration 2294: loss 0.28683775663375854
iteration 2295: loss 0.38855835795402527
iteration 2296: loss 0.40955427289009094
iteration 2297: loss 0.36937007308006287
iteration 2298: loss 0.3403221070766449
iteration 2299: loss 0.3361145555973053
iteration 2300: loss 0.37276187539100647
iteration 2301: loss 0.3226635754108429
iteration 2302: loss 0.37891605496406555
iteration 2303: loss 0.32299646735191345
iteration 2304: loss 0.3960881233215332
iteration 2305: loss 0.3585396111011505
iteration 2306: loss 0.3488524854183197
iteration 2307: loss 0.339846670627594
iteration 2308: loss 0.3834563195705414
iteration 2309: loss 0.319850891828537
iteration 2310: loss 0.3604791462421417
iteration 2311: loss 0.3412232995033264
iteration 2312: loss 0.3314189910888672
iteration 2313: loss 0.36060017347335815
iteration 2314: loss 0.3499028980731964
iteration 2315: loss 0.34397611021995544
iteration 2316: loss 0.31897494196891785
iteration 2317: loss 0.315491646528244
iteration 2318: loss 0.24590390920639038
iteration 2319: loss 0.423480361700058
iteration 2320: loss 0.3957815170288086
iteration 2321: loss 0.3501831889152527
iteration 2322: loss 0.36546024680137634
iteration 2323: loss 0.3192773759365082
iteration 2324: loss 0.3642953634262085
iteration 2325: loss 0.33952635526657104
iteration 2326: loss 0.33512112498283386
iteration 2327: loss 0.3764501214027405
iteration 2328: loss 0.31731754541397095
iteration 2329: loss 0.29931220412254333
iteration 2330: loss 0.37439072132110596
iteration 2331: loss 0.38253840804100037
iteration 2332: loss 0.4367181360721588
iteration 2333: loss 0.4006175696849823
iteration 2334: loss 0.34026452898979187
iteration 2335: loss 0.3767256438732147
iteration 2336: loss 0.30283695459365845
iteration 2337: loss 0.329201340675354
iteration 2338: loss 0.3352367579936981
iteration 2339: loss 0.3444455862045288
iteration 2340: loss 0.2764964997768402
iteration 2341: loss 0.3390342891216278
iteration 2342: loss 0.33853334188461304
iteration 2343: loss 0.35342419147491455
iteration 2344: loss 0.3848416805267334
iteration 2345: loss 0.33351755142211914
iteration 2346: loss 0.23210527002811432
iteration 2347: loss 0.2956526577472687
iteration 2348: loss 0.31114163994789124
iteration 2349: loss 0.4002982974052429
iteration 2350: loss 0.37299108505249023
iteration 2351: loss 0.3153311014175415
iteration 2352: loss 0.31483590602874756
iteration 2353: loss 0.3522689640522003
iteration 2354: loss 0.3568761646747589
iteration 2355: loss 0.3313218355178833
iteration 2356: loss 0.3577769994735718
iteration 2357: loss 0.34178227186203003
iteration 2358: loss 0.39193835854530334
iteration 2359: loss 0.33645015954971313
iteration 2360: loss 0.3811635673046112
iteration 2361: loss 0.3999377191066742
iteration 2362: loss 0.33638760447502136
iteration 2363: loss 0.29988691210746765
iteration 2364: loss 0.2727179229259491
iteration 2365: loss 0.3347499966621399
iteration 2366: loss 0.2590932250022888
iteration 2367: loss 0.35902056097984314
iteration 2368: loss 0.2921407222747803
iteration 2369: loss 0.27808108925819397
iteration 2370: loss 0.30728355050086975
iteration 2371: loss 0.3005867600440979
iteration 2372: loss 0.3810403048992157
iteration 2373: loss 0.3844219744205475
iteration 2374: loss 0.3043210804462433
iteration 2375: loss 0.3687637746334076
iteration 2376: loss 0.40573009848594666
iteration 2377: loss 0.3194277882575989
iteration 2378: loss 0.36937519907951355
iteration 2379: loss 0.3766389787197113
iteration 2380: loss 0.34631407260894775
iteration 2381: loss 0.3473697006702423
iteration 2382: loss 0.4085272252559662
iteration 2383: loss 0.37683290243148804
iteration 2384: loss 0.3770934045314789
iteration 2385: loss 0.4215155243873596
iteration 2386: loss 0.3400205671787262
iteration 2387: loss 0.30874210596084595
iteration 2388: loss 0.3528105318546295
iteration 2389: loss 0.33661237359046936
iteration 2390: loss 0.3941565155982971
iteration 2391: loss 0.2998652756214142
iteration 2392: loss 0.3244406282901764
iteration 2393: loss 0.38584479689598083
iteration 2394: loss 0.323291152715683
iteration 2395: loss 0.31908756494522095
iteration 2396: loss 0.3660508692264557
iteration 2397: loss 0.31294766068458557
iteration 2398: loss 0.3830338716506958
iteration 2399: loss 0.3417796790599823
iteration 2400: loss 0.3911594748497009
iteration 2401: loss 0.3024817109107971
iteration 2402: loss 0.39903542399406433
iteration 2403: loss 0.3699643611907959
iteration 2404: loss 0.36247876286506653
iteration 2405: loss 0.3536422848701477
iteration 2406: loss 0.36340516805648804
iteration 2407: loss 0.34590595960617065
iteration 2408: loss 0.26590168476104736
iteration 2409: loss 0.3952527344226837
iteration 2410: loss 0.38641127943992615
iteration 2411: loss 0.41891616582870483
iteration 2412: loss 0.2765524983406067
iteration 2413: loss 0.3340156078338623
iteration 2414: loss 0.3572212755680084
iteration 2415: loss 0.35141029953956604
iteration 2416: loss 0.37720605731010437
iteration 2417: loss 0.34619513154029846
iteration 2418: loss 0.35326629877090454
iteration 2419: loss 0.31370115280151367
iteration 2420: loss 0.2863905429840088
iteration 2421: loss 0.3749620020389557
iteration 2422: loss 0.3408814072608948
iteration 2423: loss 0.3915470242500305
iteration 2424: loss 0.32864850759506226
iteration 2425: loss 0.28703773021698
iteration 2426: loss 0.28405532240867615
iteration 2427: loss 0.43671688437461853
iteration 2428: loss 0.29826149344444275
iteration 2429: loss 0.41418540477752686
iteration 2430: loss 0.36811375617980957
iteration 2431: loss 0.36646488308906555
iteration 2432: loss 0.3118915855884552
iteration 2433: loss 0.2761801481246948
iteration 2434: loss 0.37187686562538147
iteration 2435: loss 0.30346444249153137
iteration 2436: loss 0.3175632357597351
iteration 2437: loss 0.330269455909729
iteration 2438: loss 0.302622526884079
iteration 2439: loss 0.2838312089443207
iteration 2440: loss 0.3561903238296509
iteration 2441: loss 0.34466323256492615
iteration 2442: loss 0.40307480096817017
iteration 2443: loss 0.34719258546829224
iteration 2444: loss 0.3315410912036896
iteration 2445: loss 0.3470461666584015
iteration 2446: loss 0.36487746238708496
iteration 2447: loss 0.33808693289756775
iteration 2448: loss 0.40262138843536377
iteration 2449: loss 0.2922624349594116
iteration 2450: loss 0.3307645618915558
iteration 2451: loss 0.4193814694881439
iteration 2452: loss 0.2735164165496826
iteration 2453: loss 0.31957757472991943
iteration 2454: loss 0.4445015788078308
iteration 2455: loss 0.3216507136821747
iteration 2456: loss 0.37862110137939453
iteration 2457: loss 0.3984495997428894
iteration 2458: loss 0.3949469327926636
iteration 2459: loss 0.28305813670158386
iteration 2460: loss 0.36801132559776306
iteration 2461: loss 0.35900411009788513
iteration 2462: loss 0.3049004077911377
iteration 2463: loss 0.36109843850135803
iteration 2464: loss 0.34461772441864014
iteration 2465: loss 0.34152713418006897
iteration 2466: loss 0.3920690715312958
iteration 2467: loss 0.4592779278755188
iteration 2468: loss 0.30820900201797485
iteration 2469: loss 0.32240572571754456
iteration 2470: loss 0.30925437808036804
iteration 2471: loss 0.33709073066711426
iteration 2472: loss 0.36017099022865295
iteration 2473: loss 0.3716731369495392
iteration 2474: loss 0.3574632704257965
iteration 2475: loss 0.407243937253952
iteration 2476: loss 0.31091758608818054
iteration 2477: loss 0.33238646388053894
iteration 2478: loss 0.2716916501522064
iteration 2479: loss 0.3550233542919159
iteration 2480: loss 0.373703271150589
iteration 2481: loss 0.3241938054561615
iteration 2482: loss 0.362809419631958
iteration 2483: loss 0.43942996859550476
iteration 2484: loss 0.334900438785553
iteration 2485: loss 0.32806122303009033
iteration 2486: loss 0.2715948522090912
iteration 2487: loss 0.2758755087852478
iteration 2488: loss 0.3456243574619293
iteration 2489: loss 0.36774757504463196
iteration 2490: loss 0.3895629346370697
iteration 2491: loss 0.37232640385627747
iteration 2492: loss 0.3313615322113037
iteration 2493: loss 0.302348792552948
iteration 2494: loss 0.30543527007102966
iteration 2495: loss 0.31012657284736633
iteration 2496: loss 0.3894236981868744
iteration 2497: loss 0.3031362295150757
iteration 2498: loss 0.39411836862564087
iteration 2499: loss 0.339187353849411
iteration 2500: loss 0.35340872406959534
iteration 2501: loss 0.34286367893218994
iteration 2502: loss 0.37756791710853577
iteration 2503: loss 0.3195217549800873
iteration 2504: loss 0.35787472128868103
iteration 2505: loss 0.3068312704563141
iteration 2506: loss 0.35086145997047424
iteration 2507: loss 0.3836621642112732
iteration 2508: loss 0.32223057746887207
iteration 2509: loss 0.3570931851863861
iteration 2510: loss 0.37044399976730347
iteration 2511: loss 0.353098601102829
iteration 2512: loss 0.36547115445137024
iteration 2513: loss 0.3742671608924866
iteration 2514: loss 0.3370448648929596
iteration 2515: loss 0.39897704124450684
iteration 2516: loss 0.37362074851989746
iteration 2517: loss 0.32955557107925415
iteration 2518: loss 0.3876979947090149
iteration 2519: loss 0.31871581077575684
iteration 2520: loss 0.32958927750587463
iteration 2521: loss 0.33622732758522034
iteration 2522: loss 0.32812637090682983
iteration 2523: loss 0.3086152970790863
iteration 2524: loss 0.38689789175987244
iteration 2525: loss 0.36936989426612854
iteration 2526: loss 0.3549480736255646
iteration 2527: loss 0.3267294466495514
iteration 2528: loss 0.306353360414505
iteration 2529: loss 0.3594488501548767
iteration 2530: loss 0.29907459020614624
iteration 2531: loss 0.33927327394485474
iteration 2532: loss 0.29456743597984314
iteration 2533: loss 0.4182698130607605
iteration 2534: loss 0.3195304870605469
iteration 2535: loss 0.35600876808166504
iteration 2536: loss 0.3101521134376526
iteration 2537: loss 0.32529202103614807
iteration 2538: loss 0.34827113151550293
iteration 2539: loss 0.315131276845932
iteration 2540: loss 0.3764660060405731
iteration 2541: loss 0.3331519365310669
iteration 2542: loss 0.3108619749546051
iteration 2543: loss 0.31716838479042053
iteration 2544: loss 0.26377809047698975
iteration 2545: loss 0.3808772563934326
iteration 2546: loss 0.41731777787208557
iteration 2547: loss 0.28917762637138367
iteration 2548: loss 0.3390335142612457
iteration 2549: loss 0.27439236640930176
iteration 2550: loss 0.39272186160087585
iteration 2551: loss 0.3324706256389618
iteration 2552: loss 0.29160505533218384
iteration 2553: loss 0.30650895833969116
iteration 2554: loss 0.3227148652076721
iteration 2555: loss 0.282254695892334
iteration 2556: loss 0.33185428380966187
iteration 2557: loss 0.37110793590545654
iteration 2558: loss 0.3188888430595398
iteration 2559: loss 0.29564735293388367
iteration 2560: loss 0.2663975656032562
iteration 2561: loss 0.2943849265575409
iteration 2562: loss 0.397183895111084
iteration 2563: loss 0.3400435745716095
iteration 2564: loss 0.4097195565700531
iteration 2565: loss 0.4183713495731354
iteration 2566: loss 0.3190031945705414
iteration 2567: loss 0.38792523741722107
iteration 2568: loss 0.30726590752601624
iteration 2569: loss 0.3400864005088806
iteration 2570: loss 0.25042521953582764
iteration 2571: loss 0.33423304557800293
iteration 2572: loss 0.3142439126968384
iteration 2573: loss 0.34336432814598083
iteration 2574: loss 0.3788195252418518
iteration 2575: loss 0.3478068709373474
iteration 2576: loss 0.3123292028903961
iteration 2577: loss 0.3352338969707489
iteration 2578: loss 0.3892133831977844
iteration 2579: loss 0.4516534209251404
iteration 2580: loss 0.29561614990234375
iteration 2581: loss 0.3283900320529938
iteration 2582: loss 0.26672592759132385
iteration 2583: loss 0.34696364402770996
iteration 2584: loss 0.36606746912002563
iteration 2585: loss 0.30788058042526245
iteration 2586: loss 0.3333205580711365
iteration 2587: loss 0.3652235269546509
iteration 2588: loss 0.36233508586883545
iteration 2589: loss 0.322332501411438
iteration 2590: loss 0.31798991560935974
iteration 2591: loss 0.34582698345184326
iteration 2592: loss 0.3289329409599304
iteration 2593: loss 0.3609217405319214
iteration 2594: loss 0.3715406060218811
iteration 2595: loss 0.3806716203689575
iteration 2596: loss 0.2793215215206146
iteration 2597: loss 0.3217970132827759
iteration 2598: loss 0.3957470953464508
iteration 2599: loss 0.32068556547164917
iteration 2600: loss 0.36322227120399475
iteration 2601: loss 0.3232353627681732
iteration 2602: loss 0.35861968994140625
iteration 2603: loss 0.26188361644744873
iteration 2604: loss 0.30900120735168457
iteration 2605: loss 0.3060189187526703
iteration 2606: loss 0.30442726612091064
iteration 2607: loss 0.3536684215068817
iteration 2608: loss 0.3900124728679657
iteration 2609: loss 0.2761031687259674
iteration 2610: loss 0.30402451753616333
iteration 2611: loss 0.33604350686073303
iteration 2612: loss 0.3063018321990967
iteration 2613: loss 0.34546005725860596
iteration 2614: loss 0.3189297020435333
iteration 2615: loss 0.3299825191497803
iteration 2616: loss 0.33605000376701355
iteration 2617: loss 0.30137813091278076
iteration 2618: loss 0.3274479806423187
iteration 2619: loss 0.3435303568840027
iteration 2620: loss 0.28486907482147217
iteration 2621: loss 0.32179832458496094
iteration 2622: loss 0.39166101813316345
iteration 2623: loss 0.32217398285865784
iteration 2624: loss 0.3916251063346863
iteration 2625: loss 0.38534456491470337
iteration 2626: loss 0.2991468906402588
iteration 2627: loss 0.33701810240745544
iteration 2628: loss 0.3431202471256256
iteration 2629: loss 0.31507861614227295
iteration 2630: loss 0.33516424894332886
iteration 2631: loss 0.38054901361465454
iteration 2632: loss 0.3238127529621124
iteration 2633: loss 0.3042876422405243
iteration 2634: loss 0.4057300090789795
iteration 2635: loss 0.2866787910461426
iteration 2636: loss 0.36252185702323914
iteration 2637: loss 0.28394395112991333
iteration 2638: loss 0.378528892993927
iteration 2639: loss 0.3645632266998291
iteration 2640: loss 0.3503829836845398
iteration 2641: loss 0.30284973978996277
iteration 2642: loss 0.32901328802108765
iteration 2643: loss 0.3194134533405304
iteration 2644: loss 0.3564663529396057
iteration 2645: loss 0.3123959004878998
iteration 2646: loss 0.29123634099960327
iteration 2647: loss 0.3147103190422058
iteration 2648: loss 0.2748101055622101
iteration 2649: loss 0.37972086668014526
iteration 2650: loss 0.34413161873817444
iteration 2651: loss 0.28666549921035767
iteration 2652: loss 0.29575279355049133
iteration 2653: loss 0.4086510241031647
iteration 2654: loss 0.3250514268875122
iteration 2655: loss 0.25185075402259827
iteration 2656: loss 0.3535144329071045
iteration 2657: loss 0.3626914918422699
iteration 2658: loss 0.3052133619785309
iteration 2659: loss 0.2954031229019165
iteration 2660: loss 0.2899075746536255
iteration 2661: loss 0.33275941014289856
iteration 2662: loss 0.3330206573009491
iteration 2663: loss 0.3536902070045471
iteration 2664: loss 0.3314708471298218
iteration 2665: loss 0.3219265043735504
iteration 2666: loss 0.4029120206832886
iteration 2667: loss 0.3723262846469879
iteration 2668: loss 0.3058047294616699
iteration 2669: loss 0.27108344435691833
iteration 2670: loss 0.37659192085266113
iteration 2671: loss 0.3552304804325104
iteration 2672: loss 0.3589724004268646
iteration 2673: loss 0.2859576344490051
iteration 2674: loss 0.28073179721832275
iteration 2675: loss 0.2542763948440552
iteration 2676: loss 0.333879292011261
iteration 2677: loss 0.3281639516353607
iteration 2678: loss 0.341898113489151
iteration 2679: loss 0.3226516544818878
iteration 2680: loss 0.3065626323223114
iteration 2681: loss 0.32747548818588257
iteration 2682: loss 0.42066678404808044
iteration 2683: loss 0.3341040313243866
iteration 2684: loss 0.3794732689857483
iteration 2685: loss 0.3937532901763916
iteration 2686: loss 0.33245912194252014
iteration 2687: loss 0.26294001936912537
iteration 2688: loss 0.3255932629108429
iteration 2689: loss 0.3060015141963959
iteration 2690: loss 0.33119112253189087
iteration 2691: loss 0.35992300510406494
iteration 2692: loss 0.29397517442703247
iteration 2693: loss 0.3367138206958771
iteration 2694: loss 0.3523680567741394
iteration 2695: loss 0.3178749978542328
iteration 2696: loss 0.270671546459198
iteration 2697: loss 0.3452390730381012
iteration 2698: loss 0.33869263529777527
iteration 2699: loss 0.33196407556533813
iteration 2700: loss 0.4138393700122833
iteration 2701: loss 0.32022997736930847
iteration 2702: loss 0.35137665271759033
iteration 2703: loss 0.32053250074386597
iteration 2704: loss 0.35005125403404236
iteration 2705: loss 0.3182036280632019
iteration 2706: loss 0.33800965547561646
iteration 2707: loss 0.33420324325561523
iteration 2708: loss 0.3168921172618866
iteration 2709: loss 0.33358079195022583
iteration 2710: loss 0.30258768796920776
iteration 2711: loss 0.3197578191757202
iteration 2712: loss 0.3504142463207245
iteration 2713: loss 0.3039896786212921
iteration 2714: loss 0.2678864300251007
iteration 2715: loss 0.32695478200912476
iteration 2716: loss 0.299912691116333
iteration 2717: loss 0.3121236562728882
iteration 2718: loss 0.30714333057403564
iteration 2719: loss 0.3567292392253876
iteration 2720: loss 0.29656288027763367
iteration 2721: loss 0.28345319628715515
iteration 2722: loss 0.3147953152656555
iteration 2723: loss 0.3581823706626892
iteration 2724: loss 0.39284366369247437
iteration 2725: loss 0.3010106086730957
iteration 2726: loss 0.3298150300979614
iteration 2727: loss 0.3094142973423004
iteration 2728: loss 0.34315499663352966
iteration 2729: loss 0.29140061140060425
iteration 2730: loss 0.39620441198349
iteration 2731: loss 0.28076136112213135
iteration 2732: loss 0.3199355900287628
iteration 2733: loss 0.34998050332069397
iteration 2734: loss 0.31993407011032104
iteration 2735: loss 0.36358150839805603
iteration 2736: loss 0.35238394141197205
iteration 2737: loss 0.3030836284160614
iteration 2738: loss 0.33834749460220337
iteration 2739: loss 0.285138875246048
iteration 2740: loss 0.3931737542152405
iteration 2741: loss 0.37093237042427063
iteration 2742: loss 0.3597796857357025
iteration 2743: loss 0.3181537985801697
iteration 2744: loss 0.29489558935165405
iteration 2745: loss 0.38504186272621155
iteration 2746: loss 0.31261366605758667
iteration 2747: loss 0.25904580950737
iteration 2748: loss 0.3002265691757202
iteration 2749: loss 0.27706143260002136
iteration 2750: loss 0.30108359456062317
iteration 2751: loss 0.31697365641593933
iteration 2752: loss 0.3441803455352783
iteration 2753: loss 0.2996536195278168
iteration 2754: loss 0.3228975832462311
iteration 2755: loss 0.30564096570014954
iteration 2756: loss 0.36386385560035706
iteration 2757: loss 0.265529066324234
iteration 2758: loss 0.35098743438720703
iteration 2759: loss 0.3142399489879608
iteration 2760: loss 0.32594990730285645
iteration 2761: loss 0.28280505537986755
iteration 2762: loss 0.38157907128334045
iteration 2763: loss 0.35111990571022034
iteration 2764: loss 0.34597060084342957
iteration 2765: loss 0.2864183485507965
iteration 2766: loss 0.36692312359809875
iteration 2767: loss 0.3948631286621094
iteration 2768: loss 0.2939442992210388
iteration 2769: loss 0.348473459482193
iteration 2770: loss 0.35481271147727966
iteration 2771: loss 0.28536301851272583
iteration 2772: loss 0.3172679841518402
iteration 2773: loss 0.3150799870491028
iteration 2774: loss 0.32370060682296753
iteration 2775: loss 0.31124287843704224
iteration 2776: loss 0.34267279505729675
iteration 2777: loss 0.2917521893978119
iteration 2778: loss 0.3061932623386383
iteration 2779: loss 0.2909151017665863
iteration 2780: loss 0.3048509359359741
iteration 2781: loss 0.34851905703544617
iteration 2782: loss 0.40277358889579773
iteration 2783: loss 0.30446040630340576
iteration 2784: loss 0.3394312560558319
iteration 2785: loss 0.3332839906215668
iteration 2786: loss 0.2972462773323059
iteration 2787: loss 0.3171221911907196
iteration 2788: loss 0.30467990040779114
iteration 2789: loss 0.32586297392845154
iteration 2790: loss 0.3549751937389374
iteration 2791: loss 0.35576751828193665
iteration 2792: loss 0.33021479845046997
iteration 2793: loss 0.31561845541000366
iteration 2794: loss 0.3150583505630493
iteration 2795: loss 0.37863948941230774
iteration 2796: loss 0.3105885088443756
iteration 2797: loss 0.28074929118156433
iteration 2798: loss 0.3447164297103882
iteration 2799: loss 0.35251903533935547
iteration 2800: loss 0.3086175322532654
iteration 2801: loss 0.34152811765670776
iteration 2802: loss 0.3032350242137909
iteration 2803: loss 0.26522502303123474
iteration 2804: loss 0.33638298511505127
iteration 2805: loss 0.348375141620636
iteration 2806: loss 0.3084411025047302
iteration 2807: loss 0.3275412321090698
iteration 2808: loss 0.3411801755428314
iteration 2809: loss 0.3223218321800232
iteration 2810: loss 0.294839084148407
iteration 2811: loss 0.3432537019252777
iteration 2812: loss 0.29772382974624634
iteration 2813: loss 0.28902146220207214
iteration 2814: loss 0.37683653831481934
iteration 2815: loss 0.2779088020324707
iteration 2816: loss 0.34805959463119507
iteration 2817: loss 0.4044782817363739
iteration 2818: loss 0.3239695429801941
iteration 2819: loss 0.27560123801231384
iteration 2820: loss 0.3587111234664917
iteration 2821: loss 0.2775150239467621
iteration 2822: loss 0.32413825392723083
iteration 2823: loss 0.3700491487979889
iteration 2824: loss 0.3816668391227722
iteration 2825: loss 0.3480408787727356
iteration 2826: loss 0.3267803490161896
iteration 2827: loss 0.280693918466568
iteration 2828: loss 0.3231240510940552
iteration 2829: loss 0.3246498107910156
iteration 2830: loss 0.3137401342391968
iteration 2831: loss 0.34175196290016174
iteration 2832: loss 0.38241252303123474
iteration 2833: loss 0.33578944206237793
iteration 2834: loss 0.32633131742477417
iteration 2835: loss 0.2756235897541046
iteration 2836: loss 0.3395635485649109
iteration 2837: loss 0.3864937126636505
iteration 2838: loss 0.3805350661277771
iteration 2839: loss 0.2869078516960144
iteration 2840: loss 0.272644579410553
iteration 2841: loss 0.3204973638057709
iteration 2842: loss 0.32076188921928406
iteration 2843: loss 0.35021400451660156
iteration 2844: loss 0.3104103207588196
iteration 2845: loss 0.326410174369812
iteration 2846: loss 0.26920515298843384
iteration 2847: loss 0.3133043050765991
iteration 2848: loss 0.3617320954799652
iteration 2849: loss 0.2714203894138336
iteration 2850: loss 0.34149566292762756
iteration 2851: loss 0.31339231133461
iteration 2852: loss 0.2899819612503052
iteration 2853: loss 0.33459341526031494
iteration 2854: loss 0.3420431613922119
iteration 2855: loss 0.27355334162712097
iteration 2856: loss 0.29697084426879883
iteration 2857: loss 0.31626808643341064
iteration 2858: loss 0.3451518416404724
iteration 2859: loss 0.33357319235801697
iteration 2860: loss 0.35537776350975037
iteration 2861: loss 0.28715723752975464
iteration 2862: loss 0.33812215924263
iteration 2863: loss 0.30656763911247253
iteration 2864: loss 0.30436885356903076
iteration 2865: loss 0.2633365988731384
iteration 2866: loss 0.2773033082485199
iteration 2867: loss 0.33622005581855774
iteration 2868: loss 0.3194129168987274
iteration 2869: loss 0.3682449758052826
iteration 2870: loss 0.3162879943847656
iteration 2871: loss 0.315156489610672
iteration 2872: loss 0.36575713753700256
iteration 2873: loss 0.2672782242298126
iteration 2874: loss 0.34849536418914795
iteration 2875: loss 0.34793442487716675
iteration 2876: loss 0.28299546241760254
iteration 2877: loss 0.33374089002609253
iteration 2878: loss 0.3517872095108032
iteration 2879: loss 0.3390418589115143
iteration 2880: loss 0.3474043011665344
iteration 2881: loss 0.3084554374217987
iteration 2882: loss 0.28030508756637573
iteration 2883: loss 0.34963473677635193
iteration 2884: loss 0.34658464789390564
iteration 2885: loss 0.3962942957878113
iteration 2886: loss 0.3550175428390503
iteration 2887: loss 0.23567619919776917
iteration 2888: loss 0.30715492367744446
iteration 2889: loss 0.2765670120716095
iteration 2890: loss 0.322147399187088
iteration 2891: loss 0.37080758810043335
iteration 2892: loss 0.30259034037590027
iteration 2893: loss 0.3298455476760864
iteration 2894: loss 0.35109415650367737
iteration 2895: loss 0.2940320372581482
iteration 2896: loss 0.38121235370635986
iteration 2897: loss 0.3991098403930664
iteration 2898: loss 0.3251785337924957
iteration 2899: loss 0.2868312895298004
iteration 2900: loss 0.3117089867591858
iteration 2901: loss 0.3355374336242676
iteration 2902: loss 0.31299301981925964
iteration 2903: loss 0.2948458790779114
iteration 2904: loss 0.3241550028324127
iteration 2905: loss 0.25933876633644104
iteration 2906: loss 0.39701026678085327
iteration 2907: loss 0.3277217447757721
iteration 2908: loss 0.33518680930137634
iteration 2909: loss 0.30836769938468933
iteration 2910: loss 0.27668261528015137
iteration 2911: loss 0.2845168709754944
iteration 2912: loss 0.28252625465393066
iteration 2913: loss 0.3173559606075287
iteration 2914: loss 0.2885620892047882
iteration 2915: loss 0.31070056557655334
iteration 2916: loss 0.27147015929222107
iteration 2917: loss 0.3232954442501068
iteration 2918: loss 0.30023902654647827
iteration 2919: loss 0.33038580417633057
iteration 2920: loss 0.26383480429649353
iteration 2921: loss 0.395722895860672
iteration 2922: loss 0.37685516476631165
iteration 2923: loss 0.31881654262542725
iteration 2924: loss 0.32541072368621826
iteration 2925: loss 0.3505322337150574
iteration 2926: loss 0.30179888010025024
iteration 2927: loss 0.31636330485343933
iteration 2928: loss 0.3702417314052582
iteration 2929: loss 0.32087573409080505
iteration 2930: loss 0.3559240400791168
iteration 2931: loss 0.2736300528049469
iteration 2932: loss 0.3773270547389984
iteration 2933: loss 0.34454381465911865
iteration 2934: loss 0.3288770020008087
iteration 2935: loss 0.3273589313030243
iteration 2936: loss 0.32159867882728577
iteration 2937: loss 0.33460885286331177
iteration 2938: loss 0.4024271070957184
iteration 2939: loss 0.3727709949016571
iteration 2940: loss 0.3111864924430847
iteration 2941: loss 0.358854740858078
iteration 2942: loss 0.2940250337123871
iteration 2943: loss 0.31554627418518066
iteration 2944: loss 0.3127087652683258
iteration 2945: loss 0.3038153052330017
iteration 2946: loss 0.30035215616226196
iteration 2947: loss 0.2913287281990051
iteration 2948: loss 0.3423008620738983
iteration 2949: loss 0.3002861440181732
iteration 2950: loss 0.34505289793014526
iteration 2951: loss 0.29910656809806824
iteration 2952: loss 0.4157974123954773
iteration 2953: loss 0.28861016035079956
iteration 2954: loss 0.27615630626678467
iteration 2955: loss 0.24386338889598846
iteration 2956: loss 0.30680230259895325
iteration 2957: loss 0.39414530992507935
iteration 2958: loss 0.3185715675354004
iteration 2959: loss 0.35645151138305664
iteration 2960: loss 0.3035393953323364
iteration 2961: loss 0.34180134534835815
iteration 2962: loss 0.2858821153640747
iteration 2963: loss 0.34950488805770874
iteration 2964: loss 0.3570624887943268
iteration 2965: loss 0.28006139397621155
iteration 2966: loss 0.30885863304138184
iteration 2967: loss 0.3085477352142334
iteration 2968: loss 0.30314067006111145
iteration 2969: loss 0.2631239891052246
iteration 2970: loss 0.41470399498939514
iteration 2971: loss 0.30134856700897217
iteration 2972: loss 0.3142557144165039
iteration 2973: loss 0.27580636739730835
iteration 2974: loss 0.3366951048374176
iteration 2975: loss 0.35231688618659973
iteration 2976: loss 0.3182414174079895
iteration 2977: loss 0.2905392646789551
iteration 2978: loss 0.26142337918281555
iteration 2979: loss 0.3019629120826721
iteration 2980: loss 0.33539316058158875
iteration 2981: loss 0.3329552710056305
iteration 2982: loss 0.3330772817134857
iteration 2983: loss 0.3755013048648834
iteration 2984: loss 0.35758471488952637
iteration 2985: loss 0.3218139111995697
iteration 2986: loss 0.33564674854278564
iteration 2987: loss 0.2630299925804138
iteration 2988: loss 0.33577683568000793
iteration 2989: loss 0.3742966949939728
iteration 2990: loss 0.319205641746521
iteration 2991: loss 0.276382714509964
iteration 2992: loss 0.3297366797924042
iteration 2993: loss 0.3568207025527954
iteration 2994: loss 0.3647645115852356
iteration 2995: loss 0.3196692168712616
iteration 2996: loss 0.3370307981967926
iteration 2997: loss 0.31854620575904846
iteration 2998: loss 0.3430347740650177
iteration 2999: loss 0.2810635566711426
iteration 3000: loss 0.24336275458335876
iteration 3001: loss 0.30017873644828796
iteration 3002: loss 0.3470917344093323
iteration 3003: loss 0.3321693539619446
iteration 3004: loss 0.2849496006965637
iteration 3005: loss 0.3067430555820465
iteration 3006: loss 0.3295820951461792
iteration 3007: loss 0.28759709000587463
iteration 3008: loss 0.29971545934677124
iteration 3009: loss 0.34272482991218567
iteration 3010: loss 0.3465658128261566
iteration 3011: loss 0.31230250000953674
iteration 3012: loss 0.3257450461387634
iteration 3013: loss 0.35716691613197327
iteration 3014: loss 0.33147019147872925
iteration 3015: loss 0.2453368753194809
iteration 3016: loss 0.2863817512989044
iteration 3017: loss 0.3673153221607208
iteration 3018: loss 0.3451433479785919
iteration 3019: loss 0.2880997955799103
iteration 3020: loss 0.30151671171188354
iteration 3021: loss 0.3034476339817047
iteration 3022: loss 0.2990325689315796
iteration 3023: loss 0.356338232755661
iteration 3024: loss 0.26537954807281494
iteration 3025: loss 0.31974998116493225
iteration 3026: loss 0.34056350588798523
iteration 3027: loss 0.3251194953918457
iteration 3028: loss 0.32188981771469116
iteration 3029: loss 0.3380124270915985
iteration 3030: loss 0.4175432324409485
iteration 3031: loss 0.2860051393508911
iteration 3032: loss 0.33478179574012756
iteration 3033: loss 0.3672958314418793
iteration 3034: loss 0.31838569045066833
iteration 3035: loss 0.32814091444015503
iteration 3036: loss 0.3461925983428955
iteration 3037: loss 0.36748212575912476
iteration 3038: loss 0.34466198086738586
iteration 3039: loss 0.34320586919784546
iteration 3040: loss 0.38984182476997375
iteration 3041: loss 0.33927714824676514
iteration 3042: loss 0.3814871907234192
iteration 3043: loss 0.36235252022743225
iteration 3044: loss 0.3313886523246765
iteration 3045: loss 0.36533665657043457
iteration 3046: loss 0.32694852352142334
iteration 3047: loss 0.32062333822250366
iteration 3048: loss 0.36476612091064453
iteration 3049: loss 0.3437948524951935
iteration 3050: loss 0.28393858671188354
iteration 3051: loss 0.26307833194732666
iteration 3052: loss 0.32493630051612854
iteration 3053: loss 0.3320314586162567
iteration 3054: loss 0.32291939854621887
iteration 3055: loss 0.30126267671585083
iteration 3056: loss 0.31870758533477783
iteration 3057: loss 0.25382599234580994
iteration 3058: loss 0.30447012186050415
iteration 3059: loss 0.27901050448417664
iteration 3060: loss 0.39348652958869934
iteration 3061: loss 0.3145953118801117
iteration 3062: loss 0.311376690864563
iteration 3063: loss 0.2983683943748474
iteration 3064: loss 0.30356457829475403
iteration 3065: loss 0.3568810224533081
iteration 3066: loss 0.29881832003593445
iteration 3067: loss 0.28631076216697693
iteration 3068: loss 0.294046014547348
iteration 3069: loss 0.287882536649704
iteration 3070: loss 0.37500637769699097
iteration 3071: loss 0.31353676319122314
iteration 3072: loss 0.33045780658721924
iteration 3073: loss 0.304328978061676
iteration 3074: loss 0.3283557593822479
iteration 3075: loss 0.2770148515701294
iteration 3076: loss 0.27672964334487915
iteration 3077: loss 0.33126088976860046
iteration 3078: loss 0.30436480045318604
iteration 3079: loss 0.3280467689037323
iteration 3080: loss 0.3383614122867584
iteration 3081: loss 0.2856188714504242
iteration 3082: loss 0.3393692076206207
iteration 3083: loss 0.3546432852745056
iteration 3084: loss 0.2989580035209656
iteration 3085: loss 0.35306650400161743
iteration 3086: loss 0.3034592568874359
iteration 3087: loss 0.29056215286254883
iteration 3088: loss 0.2805420160293579
iteration 3089: loss 0.2938849925994873
iteration 3090: loss 0.3659752309322357
iteration 3091: loss 0.3056667149066925
iteration 3092: loss 0.3085760474205017
iteration 3093: loss 0.41038307547569275
iteration 3094: loss 0.2966134250164032
iteration 3095: loss 0.29677122831344604
iteration 3096: loss 0.3168330490589142
iteration 3097: loss 0.31448307633399963
iteration 3098: loss 0.3188629448413849
iteration 3099: loss 0.34705907106399536
iteration 3100: loss 0.32846149802207947
iteration 3101: loss 0.2966932952404022
iteration 3102: loss 0.2716914415359497
iteration 3103: loss 0.3437727987766266
iteration 3104: loss 0.362725168466568
iteration 3105: loss 0.3442533612251282
iteration 3106: loss 0.3194800317287445
iteration 3107: loss 0.3317323923110962
iteration 3108: loss 0.2850976884365082
iteration 3109: loss 0.32248374819755554
iteration 3110: loss 0.37252703309059143
iteration 3111: loss 0.30287450551986694
iteration 3112: loss 0.35923025012016296
iteration 3113: loss 0.3733922243118286
iteration 3114: loss 0.3634387254714966
iteration 3115: loss 0.3959597945213318
iteration 3116: loss 0.3367752432823181
iteration 3117: loss 0.306950181722641
iteration 3118: loss 0.3028372526168823
iteration 3119: loss 0.31615936756134033
iteration 3120: loss 0.264860600233078
iteration 3121: loss 0.36917340755462646
iteration 3122: loss 0.26637691259384155
iteration 3123: loss 0.3636696934700012
iteration 3124: loss 0.27817603945732117
iteration 3125: loss 0.35727450251579285
iteration 3126: loss 0.2965308129787445
iteration 3127: loss 0.2534264922142029
iteration 3128: loss 0.35379758477211
iteration 3129: loss 0.3326331675052643
iteration 3130: loss 0.23679545521736145
iteration 3131: loss 0.3221934735774994
iteration 3132: loss 0.3381217122077942
iteration 3133: loss 0.4061497449874878
iteration 3134: loss 0.3329997658729553
iteration 3135: loss 0.37471550703048706
iteration 3136: loss 0.37119945883750916
iteration 3137: loss 0.3277874290943146
iteration 3138: loss 0.32504498958587646
iteration 3139: loss 0.3280103802680969
iteration 3140: loss 0.3776446282863617
iteration 3141: loss 0.2962096035480499
iteration 3142: loss 0.30452561378479004
iteration 3143: loss 0.28678709268569946
iteration 3144: loss 0.28207963705062866
iteration 3145: loss 0.3303316831588745
iteration 3146: loss 0.35892024636268616
iteration 3147: loss 0.30554285645484924
iteration 3148: loss 0.2709190845489502
iteration 3149: loss 0.3199053704738617
iteration 3150: loss 0.3370324671268463
iteration 3151: loss 0.35433429479599
iteration 3152: loss 0.3094344437122345
iteration 3153: loss 0.35460782051086426
iteration 3154: loss 0.31650906801223755
iteration 3155: loss 0.2996276021003723
iteration 3156: loss 0.3477669060230255
iteration 3157: loss 0.28186339139938354
iteration 3158: loss 0.4007307291030884
iteration 3159: loss 0.3500686585903168
iteration 3160: loss 0.29807448387145996
iteration 3161: loss 0.3167352080345154
iteration 3162: loss 0.2929270565509796
iteration 3163: loss 0.2490856796503067
iteration 3164: loss 0.37697699666023254
iteration 3165: loss 0.2805659770965576
iteration 3166: loss 0.33002611994743347
iteration 3167: loss 0.3473200798034668
iteration 3168: loss 0.3010062873363495
iteration 3169: loss 0.2865806221961975
iteration 3170: loss 0.3429076075553894
iteration 3171: loss 0.2709720730781555
iteration 3172: loss 0.2585483491420746
iteration 3173: loss 0.3227698802947998
iteration 3174: loss 0.36558592319488525
iteration 3175: loss 0.31332120299339294
iteration 3176: loss 0.2980518639087677
iteration 3177: loss 0.2552904784679413
iteration 3178: loss 0.36072778701782227
iteration 3179: loss 0.2609388828277588
iteration 3180: loss 0.3125612437725067
iteration 3181: loss 0.2841176986694336
iteration 3182: loss 0.3157874643802643
iteration 3183: loss 0.3143613934516907
iteration 3184: loss 0.34138330817222595
iteration 3185: loss 0.3247635066509247
iteration 3186: loss 0.2808406352996826
iteration 3187: loss 0.3207662105560303
iteration 3188: loss 0.32299649715423584
iteration 3189: loss 0.29444822669029236
iteration 3190: loss 0.25594937801361084
iteration 3191: loss 0.3141181766986847
iteration 3192: loss 0.30981752276420593
iteration 3193: loss 0.3303923010826111
iteration 3194: loss 0.33179327845573425
iteration 3195: loss 0.2963685393333435
iteration 3196: loss 0.33673548698425293
iteration 3197: loss 0.2810792922973633
iteration 3198: loss 0.31124046444892883
iteration 3199: loss 0.33045291900634766
iteration 3200: loss 0.3013887107372284
iteration 3201: loss 0.3185308575630188
iteration 3202: loss 0.298924058675766
iteration 3203: loss 0.3314451575279236
iteration 3204: loss 0.2966705560684204
iteration 3205: loss 0.2815655767917633
iteration 3206: loss 0.26727598905563354
iteration 3207: loss 0.3650088310241699
iteration 3208: loss 0.35156357288360596
iteration 3209: loss 0.26932328939437866
iteration 3210: loss 0.26865535974502563
iteration 3211: loss 0.33530765771865845
iteration 3212: loss 0.324278324842453
iteration 3213: loss 0.34187033772468567
iteration 3214: loss 0.366914838552475
iteration 3215: loss 0.3374289572238922
iteration 3216: loss 0.30355072021484375
iteration 3217: loss 0.30076080560684204
iteration 3218: loss 0.28505873680114746
iteration 3219: loss 0.3270658850669861
iteration 3220: loss 0.35356655716896057
iteration 3221: loss 0.3187114894390106
iteration 3222: loss 0.297858864068985
iteration 3223: loss 0.29489946365356445
iteration 3224: loss 0.34961679577827454
iteration 3225: loss 0.3675440847873688
iteration 3226: loss 0.2781570553779602
iteration 3227: loss 0.2931296229362488
iteration 3228: loss 0.2889305651187897
iteration 3229: loss 0.3003850281238556
iteration 3230: loss 0.26251220703125
iteration 3231: loss 0.28425830602645874
iteration 3232: loss 0.3249090611934662
iteration 3233: loss 0.28908663988113403
iteration 3234: loss 0.2890316843986511
iteration 3235: loss 0.29120132327079773
iteration 3236: loss 0.31142520904541016
iteration 3237: loss 0.33614322543144226
iteration 3238: loss 0.34617775678634644
iteration 3239: loss 0.24126771092414856
iteration 3240: loss 0.3741777539253235
iteration 3241: loss 0.33458906412124634
iteration 3242: loss 0.3196566104888916
iteration 3243: loss 0.30630990862846375
iteration 3244: loss 0.3000033497810364
iteration 3245: loss 0.2797260880470276
iteration 3246: loss 0.29232683777809143
iteration 3247: loss 0.36242708563804626
iteration 3248: loss 0.32787230610847473
iteration 3249: loss 0.30522915720939636
iteration 3250: loss 0.35218364000320435
iteration 3251: loss 0.3527878224849701
iteration 3252: loss 0.3858029842376709
iteration 3253: loss 0.32374605536460876
iteration 3254: loss 0.25828057527542114
iteration 3255: loss 0.2305854856967926
iteration 3256: loss 0.26199331879615784
iteration 3257: loss 0.292836993932724
iteration 3258: loss 0.33934321999549866
iteration 3259: loss 0.30551397800445557
iteration 3260: loss 0.294439435005188
iteration 3261: loss 0.28804004192352295
iteration 3262: loss 0.3687879741191864
iteration 3263: loss 0.3145984709262848
iteration 3264: loss 0.33604899048805237
iteration 3265: loss 0.2967168688774109
iteration 3266: loss 0.3426721692085266
iteration 3267: loss 0.2815229296684265
iteration 3268: loss 0.3132237195968628
iteration 3269: loss 0.32026407122612
iteration 3270: loss 0.3247452676296234
iteration 3271: loss 0.34804677963256836
iteration 3272: loss 0.3160882592201233
iteration 3273: loss 0.2708189785480499
iteration 3274: loss 0.2849654257297516
iteration 3275: loss 0.3139020800590515
iteration 3276: loss 0.34878605604171753
iteration 3277: loss 0.3459858298301697
iteration 3278: loss 0.2858361303806305
iteration 3279: loss 0.29294365644454956
iteration 3280: loss 0.3313778340816498
iteration 3281: loss 0.3455563187599182
iteration 3282: loss 0.33059990406036377
iteration 3283: loss 0.3151513338088989
iteration 3284: loss 0.301669716835022
iteration 3285: loss 0.3298664689064026
iteration 3286: loss 0.28774142265319824
iteration 3287: loss 0.29440659284591675
iteration 3288: loss 0.2546839416027069
iteration 3289: loss 0.29184359312057495
iteration 3290: loss 0.3045729100704193
iteration 3291: loss 0.3252671957015991
iteration 3292: loss 0.29032573103904724
iteration 3293: loss 0.32461076974868774
iteration 3294: loss 0.3576333522796631
iteration 3295: loss 0.2894206941127777
iteration 3296: loss 0.38238468766212463
iteration 3297: loss 0.27928847074508667
iteration 3298: loss 0.3279324173927307
iteration 3299: loss 0.41104933619499207
iteration 3300: loss 0.2694816589355469
iteration 3301: loss 0.3635925352573395
iteration 3302: loss 0.2694214880466461
iteration 3303: loss 0.27602002024650574
iteration 3304: loss 0.31483641266822815
iteration 3305: loss 0.34952273964881897
iteration 3306: loss 0.30663830041885376
iteration 3307: loss 0.3361065983772278
iteration 3308: loss 0.3715006113052368
iteration 3309: loss 0.3022429347038269
iteration 3310: loss 0.28353551030158997
iteration 3311: loss 0.2738319933414459
iteration 3312: loss 0.2838591933250427
iteration 3313: loss 0.31815090775489807
iteration 3314: loss 0.33276650309562683
iteration 3315: loss 0.2819458246231079
iteration 3316: loss 0.31695905327796936
iteration 3317: loss 0.3294493854045868
iteration 3318: loss 0.2648467421531677
iteration 3319: loss 0.23632435500621796
iteration 3320: loss 0.2841043472290039
iteration 3321: loss 0.2941528856754303
iteration 3322: loss 0.31886038184165955
iteration 3323: loss 0.34187790751457214
iteration 3324: loss 0.4113166034221649
iteration 3325: loss 0.30397582054138184
iteration 3326: loss 0.3207685351371765
iteration 3327: loss 0.26885131001472473
iteration 3328: loss 0.2640335261821747
iteration 3329: loss 0.29921287298202515
iteration 3330: loss 0.2898188829421997
iteration 3331: loss 0.28870928287506104
iteration 3332: loss 0.3106004297733307
iteration 3333: loss 0.3353444039821625
iteration 3334: loss 0.2599722146987915
iteration 3335: loss 0.32759442925453186
iteration 3336: loss 0.2738056778907776
iteration 3337: loss 0.395224392414093
iteration 3338: loss 0.3145272433757782
iteration 3339: loss 0.2750818431377411
iteration 3340: loss 0.3496076464653015
iteration 3341: loss 0.3448519706726074
iteration 3342: loss 0.27679207921028137
iteration 3343: loss 0.25690793991088867
iteration 3344: loss 0.31525757908821106
iteration 3345: loss 0.2781725227832794
iteration 3346: loss 0.38830915093421936
iteration 3347: loss 0.2723396122455597
iteration 3348: loss 0.30286410450935364
iteration 3349: loss 0.38094085454940796
iteration 3350: loss 0.32636573910713196
iteration 3351: loss 0.3279518187046051
iteration 3352: loss 0.3023872375488281
iteration 3353: loss 0.2976692318916321
iteration 3354: loss 0.2902611792087555
iteration 3355: loss 0.31679412722587585
iteration 3356: loss 0.2730141580104828
iteration 3357: loss 0.378431111574173
iteration 3358: loss 0.24597693979740143
iteration 3359: loss 0.3480076193809509
iteration 3360: loss 0.32213470339775085
iteration 3361: loss 0.28866997361183167
iteration 3362: loss 0.2329733967781067
iteration 3363: loss 0.30579817295074463
iteration 3364: loss 0.3351456820964813
iteration 3365: loss 0.3244216740131378
iteration 3366: loss 0.31354644894599915
iteration 3367: loss 0.356814444065094
iteration 3368: loss 0.2757754325866699
iteration 3369: loss 0.30373504757881165
iteration 3370: loss 0.32828661799430847
iteration 3371: loss 0.3169153928756714
iteration 3372: loss 0.32520875334739685
iteration 3373: loss 0.27019232511520386
iteration 3374: loss 0.3431141972541809
iteration 3375: loss 0.3096301555633545
iteration 3376: loss 0.3325446546077728
iteration 3377: loss 0.3350219428539276
iteration 3378: loss 0.3030554950237274
iteration 3379: loss 0.2748869061470032
iteration 3380: loss 0.24333816766738892
iteration 3381: loss 0.38070252537727356
iteration 3382: loss 0.33065319061279297
iteration 3383: loss 0.3053441047668457
iteration 3384: loss 0.2740755081176758
iteration 3385: loss 0.3127548396587372
iteration 3386: loss 0.2909387946128845
iteration 3387: loss 0.27439427375793457
iteration 3388: loss 0.3930571973323822
iteration 3389: loss 0.36712294816970825
iteration 3390: loss 0.33390796184539795
iteration 3391: loss 0.3648381531238556
iteration 3392: loss 0.32399195432662964
iteration 3393: loss 0.28839388489723206
iteration 3394: loss 0.40361520648002625
iteration 3395: loss 0.3440406918525696
iteration 3396: loss 0.3008275032043457
iteration 3397: loss 0.297957181930542
iteration 3398: loss 0.3040173351764679
iteration 3399: loss 0.3091292083263397
iteration 3400: loss 0.28762006759643555
iteration 3401: loss 0.28829312324523926
iteration 3402: loss 0.39391905069351196
iteration 3403: loss 0.3201897144317627
iteration 3404: loss 0.3663952052593231
iteration 3405: loss 0.31611013412475586
iteration 3406: loss 0.4224817752838135
iteration 3407: loss 0.3337549567222595
iteration 3408: loss 0.3233080804347992
iteration 3409: loss 0.3264733552932739
iteration 3410: loss 0.29185962677001953
iteration 3411: loss 0.3077298104763031
iteration 3412: loss 0.3327293395996094
iteration 3413: loss 0.24884046614170074
iteration 3414: loss 0.28050848841667175
iteration 3415: loss 0.3135905861854553
iteration 3416: loss 0.30967411398887634
iteration 3417: loss 0.36255311965942383
iteration 3418: loss 0.3508027195930481
iteration 3419: loss 0.3782098889350891
iteration 3420: loss 0.34772244095802307
iteration 3421: loss 0.234241783618927
iteration 3422: loss 0.30215173959732056
iteration 3423: loss 0.34130755066871643
iteration 3424: loss 0.3084714412689209
iteration 3425: loss 0.2942841351032257
iteration 3426: loss 0.3764396011829376
iteration 3427: loss 0.29613351821899414
iteration 3428: loss 0.25973841547966003
iteration 3429: loss 0.26659509539604187
iteration 3430: loss 0.3538600504398346
iteration 3431: loss 0.3424450159072876
iteration 3432: loss 0.37315481901168823
iteration 3433: loss 0.3262287974357605
iteration 3434: loss 0.2984420359134674
iteration 3435: loss 0.32185301184654236
iteration 3436: loss 0.3023788034915924
iteration 3437: loss 0.38039135932922363
iteration 3438: loss 0.2925330400466919
iteration 3439: loss 0.28628578782081604
iteration 3440: loss 0.27465763688087463
iteration 3441: loss 0.28315743803977966
iteration 3442: loss 0.2729165256023407
iteration 3443: loss 0.3014148771762848
iteration 3444: loss 0.30458611249923706
iteration 3445: loss 0.37796440720558167
iteration 3446: loss 0.3173871338367462
iteration 3447: loss 0.291942834854126
iteration 3448: loss 0.29262539744377136
iteration 3449: loss 0.34013694524765015
iteration 3450: loss 0.3363472521305084
iteration 3451: loss 0.35125014185905457
iteration 3452: loss 0.3697251081466675
iteration 3453: loss 0.3055466115474701
iteration 3454: loss 0.2884241044521332
iteration 3455: loss 0.3304137587547302
iteration 3456: loss 0.3598717749118805
iteration 3457: loss 0.329572856426239
iteration 3458: loss 0.24462470412254333
iteration 3459: loss 0.25129419565200806
iteration 3460: loss 0.3834705352783203
iteration 3461: loss 0.35026904940605164
iteration 3462: loss 0.312319278717041
iteration 3463: loss 0.296051561832428
iteration 3464: loss 0.3650399446487427
iteration 3465: loss 0.31375786662101746
iteration 3466: loss 0.3292278051376343
iteration 3467: loss 0.32838544249534607
iteration 3468: loss 0.2929259240627289
iteration 3469: loss 0.3032771646976471
iteration 3470: loss 0.33200541138648987
iteration 3471: loss 0.2640095055103302
iteration 3472: loss 0.3093230724334717
iteration 3473: loss 0.30016663670539856
iteration 3474: loss 0.2558943033218384
iteration 3475: loss 0.3420948088169098
iteration 3476: loss 0.30371415615081787
iteration 3477: loss 0.2953642010688782
iteration 3478: loss 0.26890164613723755
iteration 3479: loss 0.2872490882873535
iteration 3480: loss 0.33052411675453186
iteration 3481: loss 0.2874828279018402
iteration 3482: loss 0.32048299908638
iteration 3483: loss 0.3145361840724945
iteration 3484: loss 0.2924942374229431
iteration 3485: loss 0.2879188358783722
iteration 3486: loss 0.3103192448616028
iteration 3487: loss 0.31856629252433777
iteration 3488: loss 0.2815859317779541
iteration 3489: loss 0.2938704490661621
iteration 3490: loss 0.26612091064453125
iteration 3491: loss 0.35198473930358887
iteration 3492: loss 0.33320826292037964
iteration 3493: loss 0.31259605288505554
iteration 3494: loss 0.31107884645462036
iteration 3495: loss 0.3542689085006714
iteration 3496: loss 0.31770238280296326
iteration 3497: loss 0.28110018372535706
iteration 3498: loss 0.2612776458263397
iteration 3499: loss 0.4428798258304596
iteration 3500: loss 0.28449782729148865
iteration 3501: loss 0.3241680860519409
iteration 3502: loss 0.36604779958724976
iteration 3503: loss 0.31365567445755005
iteration 3504: loss 0.2831838130950928
iteration 3505: loss 0.3669755458831787
iteration 3506: loss 0.3016628921031952
iteration 3507: loss 0.41054803133010864
iteration 3508: loss 0.28345295786857605
iteration 3509: loss 0.387703537940979
iteration 3510: loss 0.32510650157928467
iteration 3511: loss 0.3105446398258209
iteration 3512: loss 0.27338647842407227
iteration 3513: loss 0.2685275673866272
iteration 3514: loss 0.31618258357048035
iteration 3515: loss 0.2815761864185333
iteration 3516: loss 0.34631824493408203
iteration 3517: loss 0.3698914051055908
iteration 3518: loss 0.33792832493782043
iteration 3519: loss 0.31326112151145935
iteration 3520: loss 0.2760937213897705
iteration 3521: loss 0.3143247663974762
iteration 3522: loss 0.29067257046699524
iteration 3523: loss 0.2579355537891388
iteration 3524: loss 0.32479673624038696
iteration 3525: loss 0.3010112941265106
iteration 3526: loss 0.356580913066864
iteration 3527: loss 0.30915147066116333
iteration 3528: loss 0.2893941402435303
iteration 3529: loss 0.32021233439445496
iteration 3530: loss 0.3305942118167877
iteration 3531: loss 0.2631867527961731
iteration 3532: loss 0.28773775696754456
iteration 3533: loss 0.25103312730789185
iteration 3534: loss 0.28660866618156433
iteration 3535: loss 0.27356410026550293
iteration 3536: loss 0.29755106568336487
iteration 3537: loss 0.27454042434692383
iteration 3538: loss 0.29074615240097046
iteration 3539: loss 0.31329286098480225
iteration 3540: loss 0.2763558626174927
iteration 3541: loss 0.3141435980796814
iteration 3542: loss 0.3077542781829834
iteration 3543: loss 0.2962413728237152
iteration 3544: loss 0.31003236770629883
iteration 3545: loss 0.33881813287734985
iteration 3546: loss 0.2975970208644867
iteration 3547: loss 0.2888980805873871
iteration 3548: loss 0.3029961884021759
iteration 3549: loss 0.31880006194114685
iteration 3550: loss 0.3400220572948456
iteration 3551: loss 0.3321097493171692
iteration 3552: loss 0.341371089220047
iteration 3553: loss 0.32325708866119385
iteration 3554: loss 0.35466131567955017
iteration 3555: loss 0.35601237416267395
iteration 3556: loss 0.3081672191619873
iteration 3557: loss 0.2707647681236267
iteration 3558: loss 0.23618236184120178
iteration 3559: loss 0.2429245114326477
iteration 3560: loss 0.3498033583164215
iteration 3561: loss 0.3085012435913086
iteration 3562: loss 0.2663242220878601
iteration 3563: loss 0.3104471266269684
iteration 3564: loss 0.28238093852996826
iteration 3565: loss 0.3496817946434021
iteration 3566: loss 0.27047106623649597
iteration 3567: loss 0.24668264389038086
iteration 3568: loss 0.3376336395740509
iteration 3569: loss 0.2606567144393921
iteration 3570: loss 0.40212658047676086
iteration 3571: loss 0.2652691602706909
iteration 3572: loss 0.3298925757408142
iteration 3573: loss 0.2821160852909088
iteration 3574: loss 0.25922614336013794
iteration 3575: loss 0.36817997694015503
iteration 3576: loss 0.3443466126918793
iteration 3577: loss 0.25944262742996216
iteration 3578: loss 0.33329829573631287
iteration 3579: loss 0.33788734674453735
iteration 3580: loss 0.3699297606945038
iteration 3581: loss 0.33909690380096436
iteration 3582: loss 0.3371703624725342
iteration 3583: loss 0.36117658019065857
iteration 3584: loss 0.30517688393592834
iteration 3585: loss 0.3162700831890106
iteration 3586: loss 0.33699867129325867
iteration 3587: loss 0.2988111972808838
iteration 3588: loss 0.28265833854675293
iteration 3589: loss 0.3034072518348694
iteration 3590: loss 0.3331741392612457
iteration 3591: loss 0.3091641068458557
iteration 3592: loss 0.32622647285461426
iteration 3593: loss 0.3072036802768707
iteration 3594: loss 0.2951955497264862
iteration 3595: loss 0.3159140944480896
iteration 3596: loss 0.3534660041332245
iteration 3597: loss 0.2568697929382324
iteration 3598: loss 0.3166022002696991
iteration 3599: loss 0.31754323840141296
iteration 3600: loss 0.305385947227478
iteration 3601: loss 0.24387645721435547
iteration 3602: loss 0.29894569516181946
iteration 3603: loss 0.35526326298713684
iteration 3604: loss 0.3500823378562927
iteration 3605: loss 0.2949813902378082
iteration 3606: loss 0.2756783068180084
iteration 3607: loss 0.2701052129268646
iteration 3608: loss 0.24268534779548645
iteration 3609: loss 0.2993513345718384
iteration 3610: loss 0.30619093775749207
iteration 3611: loss 0.31746408343315125
iteration 3612: loss 0.25372377038002014
iteration 3613: loss 0.26206955313682556
iteration 3614: loss 0.32279229164123535
iteration 3615: loss 0.2990661561489105
iteration 3616: loss 0.30932852625846863
iteration 3617: loss 0.2675802707672119
iteration 3618: loss 0.27716606855392456
iteration 3619: loss 0.2731418013572693
iteration 3620: loss 0.24019074440002441
iteration 3621: loss 0.38682103157043457
iteration 3622: loss 0.27751100063323975
iteration 3623: loss 0.27653419971466064
iteration 3624: loss 0.31003957986831665
iteration 3625: loss 0.3110208213329315
iteration 3626: loss 0.26741325855255127
iteration 3627: loss 0.3084706664085388
iteration 3628: loss 0.329094260931015
iteration 3629: loss 0.28595834970474243
iteration 3630: loss 0.2841147780418396
iteration 3631: loss 0.2578938901424408
iteration 3632: loss 0.33350247144699097
iteration 3633: loss 0.3010210692882538
iteration 3634: loss 0.2996677756309509
iteration 3635: loss 0.29550862312316895
iteration 3636: loss 0.25156620144844055
iteration 3637: loss 0.27777546644210815
iteration 3638: loss 0.311082124710083
iteration 3639: loss 0.3255296051502228
iteration 3640: loss 0.2787622809410095
iteration 3641: loss 0.3082205355167389
iteration 3642: loss 0.30257487297058105
iteration 3643: loss 0.3677721321582794
iteration 3644: loss 0.3106982111930847
iteration 3645: loss 0.3092508316040039
iteration 3646: loss 0.4043789207935333
iteration 3647: loss 0.3337534964084625
iteration 3648: loss 0.2749798595905304
iteration 3649: loss 0.3179018199443817
iteration 3650: loss 0.26388946175575256
iteration 3651: loss 0.2956901788711548
iteration 3652: loss 0.34954366087913513
iteration 3653: loss 0.3301776349544525
iteration 3654: loss 0.2671462893486023
iteration 3655: loss 0.3426752984523773
iteration 3656: loss 0.2569933831691742
iteration 3657: loss 0.2681157886981964
iteration 3658: loss 0.3176719546318054
iteration 3659: loss 0.2789636552333832
iteration 3660: loss 0.3487080931663513
iteration 3661: loss 0.24056458473205566
iteration 3662: loss 0.33317238092422485
iteration 3663: loss 0.2748129069805145
iteration 3664: loss 0.277950257062912
iteration 3665: loss 0.2864218056201935
iteration 3666: loss 0.2757970094680786
iteration 3667: loss 0.3001636862754822
iteration 3668: loss 0.32340988516807556
iteration 3669: loss 0.29152655601501465
iteration 3670: loss 0.36016711592674255
iteration 3671: loss 0.3881090581417084
iteration 3672: loss 0.372993141412735
iteration 3673: loss 0.3115350902080536
iteration 3674: loss 0.26602575182914734
iteration 3675: loss 0.29617300629615784
iteration 3676: loss 0.36689841747283936
iteration 3677: loss 0.29691243171691895
iteration 3678: loss 0.30905506014823914
iteration 3679: loss 0.34554997086524963
iteration 3680: loss 0.2921757400035858
iteration 3681: loss 0.2576490342617035
iteration 3682: loss 0.2616594135761261
iteration 3683: loss 0.33483460545539856
iteration 3684: loss 0.24061962962150574
iteration 3685: loss 0.34383073449134827
iteration 3686: loss 0.3402232229709625
iteration 3687: loss 0.28463178873062134
iteration 3688: loss 0.25182634592056274
iteration 3689: loss 0.3178693354129791
iteration 3690: loss 0.2670421600341797
iteration 3691: loss 0.3601680099964142
iteration 3692: loss 0.2987092435359955
iteration 3693: loss 0.33392733335494995
iteration 3694: loss 0.2806503176689148
iteration 3695: loss 0.29998788237571716
iteration 3696: loss 0.34906330704689026
iteration 3697: loss 0.35317814350128174
iteration 3698: loss 0.27518802881240845
iteration 3699: loss 0.3333350419998169
iteration 3700: loss 0.3331761360168457
iteration 3701: loss 0.28221482038497925
iteration 3702: loss 0.37363430857658386
iteration 3703: loss 0.35183292627334595
iteration 3704: loss 0.2868654727935791
iteration 3705: loss 0.23904328048229218
iteration 3706: loss 0.29944300651550293
iteration 3707: loss 0.3266766369342804
iteration 3708: loss 0.32636216282844543
iteration 3709: loss 0.23868924379348755
iteration 3710: loss 0.26563647389411926
iteration 3711: loss 0.2831951677799225
iteration 3712: loss 0.3050118386745453
iteration 3713: loss 0.31099069118499756
iteration 3714: loss 0.3487357199192047
iteration 3715: loss 0.2984251379966736
iteration 3716: loss 0.3299698233604431
iteration 3717: loss 0.31198158860206604
iteration 3718: loss 0.2949681282043457
iteration 3719: loss 0.2523936927318573
iteration 3720: loss 0.2776801884174347
iteration 3721: loss 0.31317633390426636
iteration 3722: loss 0.3257996737957001
iteration 3723: loss 0.31470751762390137
iteration 3724: loss 0.28657492995262146
iteration 3725: loss 0.3118273615837097
iteration 3726: loss 0.3709639608860016
iteration 3727: loss 0.3166981339454651
iteration 3728: loss 0.2758600413799286
iteration 3729: loss 0.25516560673713684
iteration 3730: loss 0.3293093144893646
iteration 3731: loss 0.2958732545375824
iteration 3732: loss 0.2576945424079895
iteration 3733: loss 0.3391706049442291
iteration 3734: loss 0.30412501096725464
iteration 3735: loss 0.2669382393360138
iteration 3736: loss 0.3384596109390259
iteration 3737: loss 0.2873761057853699
iteration 3738: loss 0.30180326104164124
iteration 3739: loss 0.2778671085834503
iteration 3740: loss 0.31956806778907776
iteration 3741: loss 0.24030748009681702
iteration 3742: loss 0.29541727900505066
iteration 3743: loss 0.2834588289260864
iteration 3744: loss 0.28758829832077026
iteration 3745: loss 0.2991624176502228
iteration 3746: loss 0.3324744999408722
iteration 3747: loss 0.3930843770503998
iteration 3748: loss 0.29671621322631836
iteration 3749: loss 0.23772408068180084
iteration 3750: loss 0.26343557238578796
iteration 3751: loss 0.27527132630348206
iteration 3752: loss 0.3109475076198578
iteration 3753: loss 0.2621106803417206
iteration 3754: loss 0.3734564483165741
iteration 3755: loss 0.2923680245876312
iteration 3756: loss 0.25808975100517273
iteration 3757: loss 0.318013459444046
iteration 3758: loss 0.36726823449134827
iteration 3759: loss 0.30416643619537354
iteration 3760: loss 0.2957720160484314
iteration 3761: loss 0.3183022439479828
iteration 3762: loss 0.2772790789604187
iteration 3763: loss 0.36356204748153687
iteration 3764: loss 0.4054306745529175
iteration 3765: loss 0.3187691271305084
iteration 3766: loss 0.28605154156684875
iteration 3767: loss 0.29324495792388916
iteration 3768: loss 0.29452136158943176
iteration 3769: loss 0.23441646993160248
iteration 3770: loss 0.28397637605667114
iteration 3771: loss 0.31696057319641113
iteration 3772: loss 0.27849647402763367
iteration 3773: loss 0.2950174808502197
iteration 3774: loss 0.2671913206577301
iteration 3775: loss 0.3039645552635193
iteration 3776: loss 0.3075043559074402
iteration 3777: loss 0.3074401617050171
iteration 3778: loss 0.2888091504573822
iteration 3779: loss 0.3185570538043976
iteration 3780: loss 0.3030683994293213
iteration 3781: loss 0.33169835805892944
iteration 3782: loss 0.34610357880592346
iteration 3783: loss 0.28346845507621765
iteration 3784: loss 0.3269621729850769
iteration 3785: loss 0.3521685302257538
iteration 3786: loss 0.31103184819221497
iteration 3787: loss 0.38337817788124084
iteration 3788: loss 0.3505552411079407
iteration 3789: loss 0.27457141876220703
iteration 3790: loss 0.27192676067352295
iteration 3791: loss 0.25633004307746887
iteration 3792: loss 0.2670450210571289
iteration 3793: loss 0.30473870038986206
iteration 3794: loss 0.3051151633262634
iteration 3795: loss 0.3068341016769409
iteration 3796: loss 0.24882572889328003
iteration 3797: loss 0.255684494972229
iteration 3798: loss 0.29545578360557556
iteration 3799: loss 0.3449510335922241
iteration 3800: loss 0.3186737895011902
iteration 3801: loss 0.2728574275970459
iteration 3802: loss 0.30804091691970825
iteration 3803: loss 0.356894314289093
iteration 3804: loss 0.3032841384410858
iteration 3805: loss 0.26299622654914856
iteration 3806: loss 0.2724817991256714
iteration 3807: loss 0.29053327441215515
iteration 3808: loss 0.2614579498767853
iteration 3809: loss 0.28520673513412476
iteration 3810: loss 0.2568724453449249
iteration 3811: loss 0.3875146806240082
iteration 3812: loss 0.289119154214859
iteration 3813: loss 0.2506590187549591
iteration 3814: loss 0.2848041355609894
iteration 3815: loss 0.30736398696899414
iteration 3816: loss 0.327470988035202
iteration 3817: loss 0.31110063195228577
iteration 3818: loss 0.32814115285873413
iteration 3819: loss 0.2702060639858246
iteration 3820: loss 0.34975871443748474
iteration 3821: loss 0.30370843410491943
iteration 3822: loss 0.26890113949775696
iteration 3823: loss 0.3637693524360657
iteration 3824: loss 0.2684709131717682
iteration 3825: loss 0.2764187753200531
iteration 3826: loss 0.29764458537101746
iteration 3827: loss 0.3109276294708252
iteration 3828: loss 0.3978996276855469
iteration 3829: loss 0.317238986492157
iteration 3830: loss 0.3669639527797699
iteration 3831: loss 0.28947409987449646
iteration 3832: loss 0.21806983649730682
iteration 3833: loss 0.2881215512752533
iteration 3834: loss 0.2660794258117676
iteration 3835: loss 0.37411263585090637
iteration 3836: loss 0.3250897526741028
iteration 3837: loss 0.2636251449584961
iteration 3838: loss 0.26991572976112366
iteration 3839: loss 0.2650493085384369
iteration 3840: loss 0.29682424664497375
iteration 3841: loss 0.30483198165893555
iteration 3842: loss 0.31557315587997437
iteration 3843: loss 0.2375723421573639
iteration 3844: loss 0.3012194037437439
iteration 3845: loss 0.3413374722003937
iteration 3846: loss 0.3872743546962738
iteration 3847: loss 0.24636854231357574
iteration 3848: loss 0.2536436915397644
iteration 3849: loss 0.31535983085632324
iteration 3850: loss 0.28726762533187866
iteration 3851: loss 0.26249757409095764
iteration 3852: loss 0.22943292558193207
iteration 3853: loss 0.3476462960243225
iteration 3854: loss 0.3011029362678528
iteration 3855: loss 0.25043490529060364
iteration 3856: loss 0.32673412561416626
iteration 3857: loss 0.28627240657806396
iteration 3858: loss 0.26722288131713867
iteration 3859: loss 0.28290823101997375
iteration 3860: loss 0.2824541926383972
iteration 3861: loss 0.3082382082939148
iteration 3862: loss 0.252010703086853
iteration 3863: loss 0.2974410057067871
iteration 3864: loss 0.32899096608161926
iteration 3865: loss 0.3287881016731262
iteration 3866: loss 0.32322877645492554
iteration 3867: loss 0.2835112512111664
iteration 3868: loss 0.3302903473377228
iteration 3869: loss 0.3092997074127197
iteration 3870: loss 0.28437677025794983
iteration 3871: loss 0.30575302243232727
iteration 3872: loss 0.3503890335559845
iteration 3873: loss 0.27605655789375305
iteration 3874: loss 0.2837827205657959
iteration 3875: loss 0.30737486481666565
iteration 3876: loss 0.29915714263916016
iteration 3877: loss 0.287447988986969
iteration 3878: loss 0.31754907965660095
iteration 3879: loss 0.3487430810928345
iteration 3880: loss 0.2722453474998474
iteration 3881: loss 0.22726242244243622
iteration 3882: loss 0.2666085362434387
iteration 3883: loss 0.348836213350296
iteration 3884: loss 0.31472527980804443
iteration 3885: loss 0.29366928339004517
iteration 3886: loss 0.2704465687274933
iteration 3887: loss 0.28834113478660583
iteration 3888: loss 0.3223898410797119
iteration 3889: loss 0.2851021885871887
iteration 3890: loss 0.37083059549331665
iteration 3891: loss 0.2973354458808899
iteration 3892: loss 0.3650292754173279
iteration 3893: loss 0.27834054827690125
iteration 3894: loss 0.268567830324173
iteration 3895: loss 0.34052518010139465
iteration 3896: loss 0.2844710648059845
iteration 3897: loss 0.349740207195282
iteration 3898: loss 0.24103949964046478
iteration 3899: loss 0.25674837827682495
iteration 3900: loss 0.2597261667251587
iteration 3901: loss 0.3619178533554077
iteration 3902: loss 0.2935996949672699
iteration 3903: loss 0.2688326835632324
iteration 3904: loss 0.2834315299987793
iteration 3905: loss 0.2928708791732788
iteration 3906: loss 0.30036893486976624
iteration 3907: loss 0.30589210987091064
iteration 3908: loss 0.26102399826049805
iteration 3909: loss 0.3349158465862274
iteration 3910: loss 0.22424913942813873
iteration 3911: loss 0.2982936501502991
iteration 3912: loss 0.27468419075012207
iteration 3913: loss 0.27232202887535095
iteration 3914: loss 0.34157794713974
iteration 3915: loss 0.30831870436668396
iteration 3916: loss 0.30128297209739685
iteration 3917: loss 0.25478988885879517
iteration 3918: loss 0.30608901381492615
iteration 3919: loss 0.25701093673706055
iteration 3920: loss 0.2807178497314453
iteration 3921: loss 0.2794940173625946
iteration 3922: loss 0.2710174024105072
iteration 3923: loss 0.3258790969848633
iteration 3924: loss 0.2842859625816345
iteration 3925: loss 0.26488375663757324
iteration 3926: loss 0.23710134625434875
iteration 3927: loss 0.27979007363319397
iteration 3928: loss 0.262998104095459
iteration 3929: loss 0.2863134443759918
iteration 3930: loss 0.3166774809360504
iteration 3931: loss 0.2880052626132965
iteration 3932: loss 0.32293909788131714
iteration 3933: loss 0.3107071816921234
iteration 3934: loss 0.32058054208755493
iteration 3935: loss 0.28450140357017517
iteration 3936: loss 0.30804312229156494
iteration 3937: loss 0.3645135164260864
iteration 3938: loss 0.27003923058509827
iteration 3939: loss 0.27904820442199707
iteration 3940: loss 0.31892111897468567
iteration 3941: loss 0.27901381254196167
iteration 3942: loss 0.24611236155033112
iteration 3943: loss 0.3171892762184143
iteration 3944: loss 0.32077378034591675
iteration 3945: loss 0.25401732325553894
iteration 3946: loss 0.28983402252197266
iteration 3947: loss 0.29968327283859253
iteration 3948: loss 0.34075331687927246
iteration 3949: loss 0.3462398946285248
iteration 3950: loss 0.3259889483451843
iteration 3951: loss 0.3383208215236664
iteration 3952: loss 0.33943071961402893
iteration 3953: loss 0.32165056467056274
iteration 3954: loss 0.314719021320343
iteration 3955: loss 0.2767041325569153
iteration 3956: loss 0.29621031880378723
iteration 3957: loss 0.2585682272911072
iteration 3958: loss 0.36009377241134644
iteration 3959: loss 0.3105699121952057
iteration 3960: loss 0.3560369610786438
iteration 3961: loss 0.3230985105037689
iteration 3962: loss 0.2579943537712097
iteration 3963: loss 0.27978411316871643
iteration 3964: loss 0.3060411810874939
iteration 3965: loss 0.29720646142959595
iteration 3966: loss 0.3251028060913086
iteration 3967: loss 0.3704088032245636
iteration 3968: loss 0.3639000952243805
iteration 3969: loss 0.335658460855484
iteration 3970: loss 0.3068029284477234
iteration 3971: loss 0.2899162471294403
iteration 3972: loss 0.2977983057498932
iteration 3973: loss 0.26848500967025757
iteration 3974: loss 0.3216792941093445
iteration 3975: loss 0.32739073038101196
iteration 3976: loss 0.24935540556907654
iteration 3977: loss 0.28022587299346924
iteration 3978: loss 0.24963028728961945
iteration 3979: loss 0.2751828730106354
iteration 3980: loss 0.2990274131298065
iteration 3981: loss 0.27138498425483704
iteration 3982: loss 0.29184287786483765
iteration 3983: loss 0.28332963585853577
iteration 3984: loss 0.22948437929153442
iteration 3985: loss 0.2734270691871643
iteration 3986: loss 0.2472485452890396
iteration 3987: loss 0.3015053868293762
iteration 3988: loss 0.301295667886734
iteration 3989: loss 0.29149317741394043
iteration 3990: loss 0.2924824059009552
iteration 3991: loss 0.2643783986568451
iteration 3992: loss 0.33865898847579956
iteration 3993: loss 0.3256571888923645
iteration 3994: loss 0.30313557386398315
iteration 3995: loss 0.3032323122024536
iteration 3996: loss 0.33002400398254395
iteration 3997: loss 0.25290557742118835
iteration 3998: loss 0.29621028900146484
iteration 3999: loss 0.26004257798194885
iteration 4000: loss 0.2859308421611786
iteration 4001: loss 0.32741260528564453
iteration 4002: loss 0.2728758454322815
iteration 4003: loss 0.3044423758983612
iteration 4004: loss 0.2539342939853668
iteration 4005: loss 0.21446247398853302
iteration 4006: loss 0.2530824542045593
iteration 4007: loss 0.2882700562477112
iteration 4008: loss 0.3335000276565552
iteration 4009: loss 0.32207173109054565
iteration 4010: loss 0.32282668352127075
iteration 4011: loss 0.2635529935359955
iteration 4012: loss 0.2780912518501282
iteration 4013: loss 0.33544600009918213
iteration 4014: loss 0.2452622801065445
iteration 4015: loss 0.30254510045051575
iteration 4016: loss 0.27318453788757324
iteration 4017: loss 0.34923502802848816
iteration 4018: loss 0.28583502769470215
iteration 4019: loss 0.227968230843544
iteration 4020: loss 0.3201896846294403
iteration 4021: loss 0.24066518247127533
iteration 4022: loss 0.361359566450119
iteration 4023: loss 0.26736557483673096
iteration 4024: loss 0.2626558840274811
iteration 4025: loss 0.30227670073509216
iteration 4026: loss 0.3374003469944
iteration 4027: loss 0.3079661428928375
iteration 4028: loss 0.26911863684654236
iteration 4029: loss 0.33980849385261536
iteration 4030: loss 0.3712542951107025
iteration 4031: loss 0.2787025570869446
iteration 4032: loss 0.3162703216075897
iteration 4033: loss 0.24575060606002808
iteration 4034: loss 0.24182380735874176
iteration 4035: loss 0.2696966826915741
iteration 4036: loss 0.3626765310764313
iteration 4037: loss 0.29266607761383057
iteration 4038: loss 0.2832473814487457
iteration 4039: loss 0.3003696799278259
iteration 4040: loss 0.24082201719284058
iteration 4041: loss 0.2897411584854126
iteration 4042: loss 0.2566191852092743
iteration 4043: loss 0.26504385471343994
iteration 4044: loss 0.3507474958896637
iteration 4045: loss 0.2551521062850952
iteration 4046: loss 0.3254699110984802
iteration 4047: loss 0.2775927186012268
iteration 4048: loss 0.285901814699173
iteration 4049: loss 0.279112309217453
iteration 4050: loss 0.23945999145507812
iteration 4051: loss 0.2818542718887329
iteration 4052: loss 0.2716141939163208
iteration 4053: loss 0.28434059023857117
iteration 4054: loss 0.25822100043296814
iteration 4055: loss 0.27705276012420654
iteration 4056: loss 0.27651381492614746
iteration 4057: loss 0.2949580252170563
iteration 4058: loss 0.26940464973449707
iteration 4059: loss 0.27645912766456604
iteration 4060: loss 0.3220934569835663
iteration 4061: loss 0.2792094945907593
iteration 4062: loss 0.2946457266807556
iteration 4063: loss 0.3255164325237274
iteration 4064: loss 0.28863969445228577
iteration 4065: loss 0.312055379152298
iteration 4066: loss 0.26522472500801086
iteration 4067: loss 0.2933833599090576
iteration 4068: loss 0.2957446575164795
iteration 4069: loss 0.38070839643478394
iteration 4070: loss 0.24182705581188202
iteration 4071: loss 0.3135475814342499
iteration 4072: loss 0.21591073274612427
iteration 4073: loss 0.292108952999115
iteration 4074: loss 0.351152241230011
iteration 4075: loss 0.24232220649719238
iteration 4076: loss 0.3671637177467346
iteration 4077: loss 0.2962054908275604
iteration 4078: loss 0.2766622304916382
iteration 4079: loss 0.34634751081466675
iteration 4080: loss 0.346951961517334
iteration 4081: loss 0.25242146849632263
iteration 4082: loss 0.30557456612586975
iteration 4083: loss 0.3259715735912323
iteration 4084: loss 0.2702390253543854
iteration 4085: loss 0.35758697986602783
iteration 4086: loss 0.32001376152038574
iteration 4087: loss 0.29530537128448486
iteration 4088: loss 0.21392570436000824
iteration 4089: loss 0.2965538501739502
iteration 4090: loss 0.3361404240131378
iteration 4091: loss 0.2998131513595581
iteration 4092: loss 0.32627132534980774
iteration 4093: loss 0.35050296783447266
iteration 4094: loss 0.2817131280899048
iteration 4095: loss 0.288769006729126
iteration 4096: loss 0.2739766240119934
iteration 4097: loss 0.27711835503578186
iteration 4098: loss 0.34014764428138733
iteration 4099: loss 0.2681719958782196
iteration 4100: loss 0.2731877863407135
iteration 4101: loss 0.35176020860671997
iteration 4102: loss 0.2593361437320709
iteration 4103: loss 0.3387150466442108
iteration 4104: loss 0.28753045201301575
iteration 4105: loss 0.2949332594871521
iteration 4106: loss 0.31714850664138794
iteration 4107: loss 0.25534069538116455
iteration 4108: loss 0.2836664021015167
iteration 4109: loss 0.24220776557922363
iteration 4110: loss 0.25245749950408936
iteration 4111: loss 0.34105029702186584
iteration 4112: loss 0.28263652324676514
iteration 4113: loss 0.2945508658885956
iteration 4114: loss 0.27077534794807434
iteration 4115: loss 0.29788801074028015
iteration 4116: loss 0.2255614846944809
iteration 4117: loss 0.28034722805023193
iteration 4118: loss 0.2734871804714203
iteration 4119: loss 0.2425597906112671
iteration 4120: loss 0.293247252702713
iteration 4121: loss 0.2545546889305115
iteration 4122: loss 0.2762385904788971
iteration 4123: loss 0.2700720429420471
iteration 4124: loss 0.32190513610839844
iteration 4125: loss 0.24614325165748596
iteration 4126: loss 0.2723795175552368
iteration 4127: loss 0.2729109823703766
iteration 4128: loss 0.24649211764335632
iteration 4129: loss 0.32968869805336
iteration 4130: loss 0.26870444416999817
iteration 4131: loss 0.2607839107513428
iteration 4132: loss 0.300253689289093
iteration 4133: loss 0.2615658640861511
iteration 4134: loss 0.30591461062431335
iteration 4135: loss 0.28615254163742065
iteration 4136: loss 0.2497255504131317
iteration 4137: loss 0.2597668766975403
iteration 4138: loss 0.22974291443824768
iteration 4139: loss 0.2845759987831116
iteration 4140: loss 0.30968716740608215
iteration 4141: loss 0.2782769799232483
iteration 4142: loss 0.2565509080886841
iteration 4143: loss 0.3030661940574646
iteration 4144: loss 0.30411019921302795
iteration 4145: loss 0.2662358283996582
iteration 4146: loss 0.2658740282058716
iteration 4147: loss 0.20970213413238525
iteration 4148: loss 0.23979924619197845
iteration 4149: loss 0.3415864109992981
iteration 4150: loss 0.2980427145957947
iteration 4151: loss 0.270014226436615
iteration 4152: loss 0.2596929371356964
iteration 4153: loss 0.3025974929332733
iteration 4154: loss 0.34040865302085876
iteration 4155: loss 0.3006954789161682
iteration 4156: loss 0.3237368166446686
iteration 4157: loss 0.34518152475357056
iteration 4158: loss 0.25157174468040466
iteration 4159: loss 0.3294239342212677
iteration 4160: loss 0.3173477351665497
iteration 4161: loss 0.26101410388946533
iteration 4162: loss 0.3040010929107666
iteration 4163: loss 0.2916061282157898
iteration 4164: loss 0.2881913483142853
iteration 4165: loss 0.2493056207895279
iteration 4166: loss 0.3290286064147949
iteration 4167: loss 0.2733568251132965
iteration 4168: loss 0.3011337220668793
iteration 4169: loss 0.23100775480270386
iteration 4170: loss 0.30660492181777954
iteration 4171: loss 0.31289178133010864
iteration 4172: loss 0.294663667678833
iteration 4173: loss 0.29671961069107056
iteration 4174: loss 0.2568383514881134
iteration 4175: loss 0.3012819290161133
iteration 4176: loss 0.26939770579338074
iteration 4177: loss 0.2396962195634842
iteration 4178: loss 0.2396504282951355
iteration 4179: loss 0.23930805921554565
iteration 4180: loss 0.29594892263412476
iteration 4181: loss 0.25668197870254517
iteration 4182: loss 0.28556153178215027
iteration 4183: loss 0.3026793897151947
iteration 4184: loss 0.3150902986526489
iteration 4185: loss 0.333469957113266
iteration 4186: loss 0.29225629568099976
iteration 4187: loss 0.2447381615638733
iteration 4188: loss 0.24122771620750427
iteration 4189: loss 0.28144004940986633
iteration 4190: loss 0.2967016100883484
iteration 4191: loss 0.3229105770587921
iteration 4192: loss 0.22939884662628174
iteration 4193: loss 0.26328468322753906
iteration 4194: loss 0.3017098605632782
iteration 4195: loss 0.39878883957862854
iteration 4196: loss 0.29220885038375854
iteration 4197: loss 0.31300631165504456
iteration 4198: loss 0.28019100427627563
iteration 4199: loss 0.24366533756256104
iteration 4200: loss 0.30678534507751465
iteration 4201: loss 0.34118521213531494
iteration 4202: loss 0.31187716126441956
iteration 4203: loss 0.27534961700439453
iteration 4204: loss 0.26685142517089844
iteration 4205: loss 0.2532779276371002
iteration 4206: loss 0.31189393997192383
iteration 4207: loss 0.2788962423801422
iteration 4208: loss 0.21150624752044678
iteration 4209: loss 0.3196655213832855
iteration 4210: loss 0.2708757221698761
iteration 4211: loss 0.35914576053619385
iteration 4212: loss 0.2643848657608032
iteration 4213: loss 0.2367185801267624
iteration 4214: loss 0.23424571752548218
iteration 4215: loss 0.29556921124458313
iteration 4216: loss 0.29462894797325134
iteration 4217: loss 0.3003491461277008
iteration 4218: loss 0.31045615673065186
iteration 4219: loss 0.3109429180622101
iteration 4220: loss 0.3061586320400238
iteration 4221: loss 0.314405620098114
iteration 4222: loss 0.29100802540779114
iteration 4223: loss 0.29210951924324036
iteration 4224: loss 0.2795071005821228
iteration 4225: loss 0.3703281879425049
iteration 4226: loss 0.2578886151313782
iteration 4227: loss 0.2823931872844696
iteration 4228: loss 0.2558314800262451
iteration 4229: loss 0.2937614619731903
iteration 4230: loss 0.32769083976745605
iteration 4231: loss 0.32841435074806213
iteration 4232: loss 0.28887268900871277
iteration 4233: loss 0.26136231422424316
iteration 4234: loss 0.2841367721557617
iteration 4235: loss 0.25552788376808167
iteration 4236: loss 0.33865174651145935
iteration 4237: loss 0.31497955322265625
iteration 4238: loss 0.25683555006980896
iteration 4239: loss 0.3181268870830536
iteration 4240: loss 0.2907053828239441
iteration 4241: loss 0.22076092660427094
iteration 4242: loss 0.3317980170249939
iteration 4243: loss 0.2821262776851654
iteration 4244: loss 0.28126323223114014
iteration 4245: loss 0.28327369689941406
iteration 4246: loss 0.25652024149894714
iteration 4247: loss 0.31599128246307373
iteration 4248: loss 0.23329922556877136
iteration 4249: loss 0.25300320982933044
iteration 4250: loss 0.3033052384853363
iteration 4251: loss 0.31248635053634644
iteration 4252: loss 0.34400615096092224
iteration 4253: loss 0.27076321840286255
iteration 4254: loss 0.2621159553527832
iteration 4255: loss 0.2534145414829254
iteration 4256: loss 0.29793378710746765
iteration 4257: loss 0.27272507548332214
iteration 4258: loss 0.22138214111328125
iteration 4259: loss 0.3187364935874939
iteration 4260: loss 0.3110216557979584
iteration 4261: loss 0.24592499434947968
iteration 4262: loss 0.31670740246772766
iteration 4263: loss 0.27671438455581665
iteration 4264: loss 0.29653215408325195
iteration 4265: loss 0.25020554661750793
iteration 4266: loss 0.2759001851081848
iteration 4267: loss 0.313591867685318
iteration 4268: loss 0.26034075021743774
iteration 4269: loss 0.3051067888736725
iteration 4270: loss 0.261616587638855
iteration 4271: loss 0.22113755345344543
iteration 4272: loss 0.26235467195510864
iteration 4273: loss 0.3503960371017456
iteration 4274: loss 0.273541122674942
iteration 4275: loss 0.378244549036026
iteration 4276: loss 0.24941247701644897
iteration 4277: loss 0.25216495990753174
iteration 4278: loss 0.31166791915893555
iteration 4279: loss 0.32730889320373535
iteration 4280: loss 0.3510763943195343
iteration 4281: loss 0.35378730297088623
iteration 4282: loss 0.3506345748901367
iteration 4283: loss 0.32917845249176025
iteration 4284: loss 0.32561194896698
iteration 4285: loss 0.3396527171134949
iteration 4286: loss 0.244255930185318
iteration 4287: loss 0.31405922770500183
iteration 4288: loss 0.23419594764709473
iteration 4289: loss 0.19745740294456482
iteration 4290: loss 0.25813406705856323
iteration 4291: loss 0.271680623292923
iteration 4292: loss 0.3560316562652588
iteration 4293: loss 0.2634967863559723
iteration 4294: loss 0.21056562662124634
iteration 4295: loss 0.3559187054634094
iteration 4296: loss 0.29099082946777344
iteration 4297: loss 0.27868521213531494
iteration 4298: loss 0.2810721695423126
iteration 4299: loss 0.30730152130126953
iteration 4300: loss 0.38040587306022644
iteration 4301: loss 0.2878677546977997
iteration 4302: loss 0.28412941098213196
iteration 4303: loss 0.27368059754371643
iteration 4304: loss 0.3121100068092346
iteration 4305: loss 0.27478349208831787
iteration 4306: loss 0.26999813318252563
iteration 4307: loss 0.2979920506477356
iteration 4308: loss 0.3102191388607025
iteration 4309: loss 0.3191644251346588
iteration 4310: loss 0.301496684551239
iteration 4311: loss 0.22768349945545197
iteration 4312: loss 0.25639790296554565
iteration 4313: loss 0.2388431578874588
iteration 4314: loss 0.2998262345790863
iteration 4315: loss 0.26491186022758484
iteration 4316: loss 0.2563585937023163
iteration 4317: loss 0.386689156293869
iteration 4318: loss 0.36519211530685425
iteration 4319: loss 0.2897128462791443
iteration 4320: loss 0.3048344850540161
iteration 4321: loss 0.3070923388004303
iteration 4322: loss 0.2504054307937622
iteration 4323: loss 0.3121383786201477
iteration 4324: loss 0.2932276725769043
iteration 4325: loss 0.32936161756515503
iteration 4326: loss 0.3074747920036316
iteration 4327: loss 0.2877780795097351
iteration 4328: loss 0.28902721405029297
iteration 4329: loss 0.3178400695323944
iteration 4330: loss 0.281384140253067
iteration 4331: loss 0.3357623517513275
iteration 4332: loss 0.25903287529945374
iteration 4333: loss 0.26602354645729065
iteration 4334: loss 0.22906363010406494
iteration 4335: loss 0.28875765204429626
iteration 4336: loss 0.2690221071243286
iteration 4337: loss 0.27520638704299927
iteration 4338: loss 0.2621561586856842
iteration 4339: loss 0.30201438069343567
iteration 4340: loss 0.24862666428089142
iteration 4341: loss 0.2762228846549988
iteration 4342: loss 0.34104928374290466
iteration 4343: loss 0.23779436945915222
iteration 4344: loss 0.23081672191619873
iteration 4345: loss 0.27821609377861023
iteration 4346: loss 0.3030324876308441
iteration 4347: loss 0.2706449031829834
iteration 4348: loss 0.2950578033924103
iteration 4349: loss 0.35840269923210144
iteration 4350: loss 0.37284019589424133
iteration 4351: loss 0.3763006925582886
iteration 4352: loss 0.2722274661064148
iteration 4353: loss 0.24853065609931946
iteration 4354: loss 0.3428829312324524
iteration 4355: loss 0.28456369042396545
iteration 4356: loss 0.29468834400177
iteration 4357: loss 0.33042487502098083
iteration 4358: loss 0.3359774649143219
iteration 4359: loss 0.3098890483379364
iteration 4360: loss 0.26724866032600403
iteration 4361: loss 0.23004846274852753
iteration 4362: loss 0.20953017473220825
iteration 4363: loss 0.29615268111228943
iteration 4364: loss 0.24590426683425903
iteration 4365: loss 0.2567460238933563
iteration 4366: loss 0.21593254804611206
iteration 4367: loss 0.2855888307094574
iteration 4368: loss 0.24345183372497559
iteration 4369: loss 0.30027052760124207
iteration 4370: loss 0.30280742049217224
iteration 4371: loss 0.23665696382522583
iteration 4372: loss 0.3178485631942749
iteration 4373: loss 0.3352343738079071
iteration 4374: loss 0.3448949456214905
iteration 4375: loss 0.3149403929710388
iteration 4376: loss 0.24732983112335205
iteration 4377: loss 0.3062671422958374
iteration 4378: loss 0.2539781928062439
iteration 4379: loss 0.28969550132751465
iteration 4380: loss 0.2651859223842621
iteration 4381: loss 0.27765849232673645
iteration 4382: loss 0.27214765548706055
iteration 4383: loss 0.27657410502433777
iteration 4384: loss 0.32411351799964905
iteration 4385: loss 0.2824453115463257
iteration 4386: loss 0.27656787633895874
iteration 4387: loss 0.2632833421230316
iteration 4388: loss 0.286112904548645
iteration 4389: loss 0.3226017653942108
iteration 4390: loss 0.3179706931114197
iteration 4391: loss 0.24288959801197052
iteration 4392: loss 0.3031551241874695
iteration 4393: loss 0.2898453176021576
iteration 4394: loss 0.3101031184196472
iteration 4395: loss 0.3370512127876282
iteration 4396: loss 0.30121973156929016
iteration 4397: loss 0.26957234740257263
iteration 4398: loss 0.25499677658081055
iteration 4399: loss 0.35456088185310364
iteration 4400: loss 0.2814444601535797
iteration 4401: loss 0.33906981348991394
iteration 4402: loss 0.2729405462741852
iteration 4403: loss 0.35207509994506836
iteration 4404: loss 0.241740420460701
iteration 4405: loss 0.2616904377937317
iteration 4406: loss 0.26820996403694153
iteration 4407: loss 0.2585788369178772
iteration 4408: loss 0.2709217369556427
iteration 4409: loss 0.3431883752346039
iteration 4410: loss 0.2923746109008789
iteration 4411: loss 0.28148093819618225
iteration 4412: loss 0.2714262306690216
iteration 4413: loss 0.2980707287788391
iteration 4414: loss 0.293929785490036
iteration 4415: loss 0.3107839524745941
iteration 4416: loss 0.3191610872745514
iteration 4417: loss 0.32669129967689514
iteration 4418: loss 0.33587896823883057
iteration 4419: loss 0.31229230761528015
iteration 4420: loss 0.3274586796760559
iteration 4421: loss 0.33842065930366516
iteration 4422: loss 0.2912694811820984
iteration 4423: loss 0.3500010073184967
iteration 4424: loss 0.2936287224292755
iteration 4425: loss 0.2643032371997833
iteration 4426: loss 0.26867014169692993
iteration 4427: loss 0.3099089562892914
iteration 4428: loss 0.36261236667633057
iteration 4429: loss 0.26694077253341675
iteration 4430: loss 0.2716650366783142
iteration 4431: loss 0.2574754059314728
iteration 4432: loss 0.28451746702194214
iteration 4433: loss 0.3143211901187897
iteration 4434: loss 0.23845475912094116
iteration 4435: loss 0.31438031792640686
iteration 4436: loss 0.2282848209142685
iteration 4437: loss 0.2459222376346588
iteration 4438: loss 0.29755836725234985
iteration 4439: loss 0.3383529484272003
iteration 4440: loss 0.28973862528800964
iteration 4441: loss 0.29179665446281433
iteration 4442: loss 0.27528196573257446
iteration 4443: loss 0.32929739356040955
iteration 4444: loss 0.31905126571655273
iteration 4445: loss 0.25472089648246765
iteration 4446: loss 0.2420588880777359
iteration 4447: loss 0.2712695300579071
iteration 4448: loss 0.2787964344024658
iteration 4449: loss 0.3393343985080719
iteration 4450: loss 0.25288838148117065
iteration 4451: loss 0.32928624749183655
iteration 4452: loss 0.26747334003448486
iteration 4453: loss 0.2478114515542984
iteration 4454: loss 0.2465951144695282
iteration 4455: loss 0.21887189149856567
iteration 4456: loss 0.2208266258239746
iteration 4457: loss 0.2904944121837616
iteration 4458: loss 0.23525138199329376
iteration 4459: loss 0.2307671308517456
iteration 4460: loss 0.3006942868232727
iteration 4461: loss 0.2716621160507202
iteration 4462: loss 0.2540155351161957
iteration 4463: loss 0.33231911063194275
iteration 4464: loss 0.25713953375816345
iteration 4465: loss 0.3258572518825531
iteration 4466: loss 0.28679776191711426
iteration 4467: loss 0.2824278175830841
iteration 4468: loss 0.28923705220222473
iteration 4469: loss 0.2911359369754791
iteration 4470: loss 0.26237753033638
iteration 4471: loss 0.2425014078617096
iteration 4472: loss 0.33351239562034607
iteration 4473: loss 0.2989577054977417
iteration 4474: loss 0.2603060305118561
iteration 4475: loss 0.2660556435585022
iteration 4476: loss 0.3022158443927765
iteration 4477: loss 0.3007422387599945
iteration 4478: loss 0.2748626470565796
iteration 4479: loss 0.35480955243110657
iteration 4480: loss 0.270332396030426
iteration 4481: loss 0.3484746217727661
iteration 4482: loss 0.2919009327888489
iteration 4483: loss 0.3282099962234497
iteration 4484: loss 0.27314406633377075
iteration 4485: loss 0.24639755487442017
iteration 4486: loss 0.23898564279079437
iteration 4487: loss 0.24980315566062927
iteration 4488: loss 0.2563972771167755
iteration 4489: loss 0.29718685150146484
iteration 4490: loss 0.3119177222251892
iteration 4491: loss 0.24197888374328613
iteration 4492: loss 0.25687533617019653
iteration 4493: loss 0.2945837378501892
iteration 4494: loss 0.24578364193439484
iteration 4495: loss 0.3537246882915497
iteration 4496: loss 0.2880636155605316
iteration 4497: loss 0.29868045449256897
iteration 4498: loss 0.2977261543273926
iteration 4499: loss 0.2982203960418701
iteration 4500: loss 0.2376800924539566
iteration 4501: loss 0.2909547686576843
iteration 4502: loss 0.2557150721549988
iteration 4503: loss 0.2489566057920456
iteration 4504: loss 0.29667800664901733
iteration 4505: loss 0.21625742316246033
iteration 4506: loss 0.2996703088283539
iteration 4507: loss 0.27320489287376404
iteration 4508: loss 0.2766125798225403
iteration 4509: loss 0.2814708650112152
iteration 4510: loss 0.26315465569496155
iteration 4511: loss 0.27326515316963196
iteration 4512: loss 0.3203633725643158
iteration 4513: loss 0.29462867975234985
iteration 4514: loss 0.31994396448135376
iteration 4515: loss 0.2992357313632965
iteration 4516: loss 0.23837114870548248
iteration 4517: loss 0.29541581869125366
iteration 4518: loss 0.32928118109703064
iteration 4519: loss 0.33536309003829956
iteration 4520: loss 0.28710606694221497
iteration 4521: loss 0.3256402909755707
iteration 4522: loss 0.26945844292640686
iteration 4523: loss 0.2796465754508972
iteration 4524: loss 0.37030261754989624
iteration 4525: loss 0.31556183099746704
iteration 4526: loss 0.28508466482162476
iteration 4527: loss 0.3093971610069275
iteration 4528: loss 0.350140243768692
iteration 4529: loss 0.28947529196739197
iteration 4530: loss 0.2709115445613861
iteration 4531: loss 0.29619675874710083
iteration 4532: loss 0.2566140294075012
iteration 4533: loss 0.2567388117313385
iteration 4534: loss 0.2504275143146515
iteration 4535: loss 0.20819947123527527
iteration 4536: loss 0.2754012942314148
iteration 4537: loss 0.23351633548736572
iteration 4538: loss 0.2670692205429077
iteration 4539: loss 0.2765110433101654
iteration 4540: loss 0.3607551157474518
iteration 4541: loss 0.322934091091156
iteration 4542: loss 0.2558757960796356
iteration 4543: loss 0.2927003502845764
iteration 4544: loss 0.25102105736732483
iteration 4545: loss 0.21717676520347595
iteration 4546: loss 0.3423866629600525
iteration 4547: loss 0.32004043459892273
iteration 4548: loss 0.24952447414398193
iteration 4549: loss 0.20687299966812134
iteration 4550: loss 0.2403663992881775
iteration 4551: loss 0.36545681953430176
iteration 4552: loss 0.30976736545562744
iteration 4553: loss 0.272524356842041
iteration 4554: loss 0.2922045588493347
iteration 4555: loss 0.25014030933380127
iteration 4556: loss 0.24391822516918182
iteration 4557: loss 0.29203563928604126
iteration 4558: loss 0.3446055054664612
iteration 4559: loss 0.2947418987751007
iteration 4560: loss 0.30551883578300476
iteration 4561: loss 0.32655760645866394
iteration 4562: loss 0.2923249304294586
iteration 4563: loss 0.28158891201019287
iteration 4564: loss 0.25388839840888977
iteration 4565: loss 0.26092198491096497
iteration 4566: loss 0.28279849886894226
iteration 4567: loss 0.3360421061515808
iteration 4568: loss 0.29882141947746277
iteration 4569: loss 0.25620850920677185
iteration 4570: loss 0.2674484848976135
iteration 4571: loss 0.21441160142421722
iteration 4572: loss 0.30383357405662537
iteration 4573: loss 0.2920314073562622
iteration 4574: loss 0.30894291400909424
iteration 4575: loss 0.3061395287513733
iteration 4576: loss 0.28283998370170593
iteration 4577: loss 0.2836652994155884
iteration 4578: loss 0.2745085656642914
iteration 4579: loss 0.28012022376060486
iteration 4580: loss 0.2927257716655731
iteration 4581: loss 0.26871854066848755
iteration 4582: loss 0.3185632526874542
iteration 4583: loss 0.20819871127605438
iteration 4584: loss 0.2515602111816406
iteration 4585: loss 0.21519936621189117
iteration 4586: loss 0.25571611523628235
iteration 4587: loss 0.27765849232673645
iteration 4588: loss 0.2605970799922943
iteration 4589: loss 0.2940825819969177
iteration 4590: loss 0.2389564961194992
iteration 4591: loss 0.27405765652656555
iteration 4592: loss 0.2833382487297058
iteration 4593: loss 0.27941131591796875
iteration 4594: loss 0.21286000311374664
iteration 4595: loss 0.25980398058891296
iteration 4596: loss 0.28579503297805786
iteration 4597: loss 0.26037710905075073
iteration 4598: loss 0.26066872477531433
iteration 4599: loss 0.2552787959575653
iteration 4600: loss 0.27229318022727966
iteration 4601: loss 0.3261011242866516
iteration 4602: loss 0.225352481007576
iteration 4603: loss 0.2624486982822418
iteration 4604: loss 0.3035534918308258
iteration 4605: loss 0.2846957743167877
iteration 4606: loss 0.26583409309387207
iteration 4607: loss 0.30710741877555847
iteration 4608: loss 0.24002790451049805
iteration 4609: loss 0.3256577253341675
iteration 4610: loss 0.24595759809017181
iteration 4611: loss 0.24073165655136108
iteration 4612: loss 0.3561461865901947
iteration 4613: loss 0.31687429547309875
iteration 4614: loss 0.268320232629776
iteration 4615: loss 0.2542950510978699
iteration 4616: loss 0.3140530288219452
iteration 4617: loss 0.2257402539253235
iteration 4618: loss 0.25111842155456543
iteration 4619: loss 0.2691834568977356
iteration 4620: loss 0.27958381175994873
iteration 4621: loss 0.2501993179321289
iteration 4622: loss 0.2684735059738159
iteration 4623: loss 0.26933836936950684
iteration 4624: loss 0.2869328260421753
iteration 4625: loss 0.300386905670166
iteration 4626: loss 0.29448872804641724
iteration 4627: loss 0.34918564558029175
iteration 4628: loss 0.4177494943141937
iteration 4629: loss 0.2547228932380676
iteration 4630: loss 0.24002598226070404
iteration 4631: loss 0.3327038884162903
iteration 4632: loss 0.33961039781570435
iteration 4633: loss 0.3294849693775177
iteration 4634: loss 0.27385008335113525
iteration 4635: loss 0.24904480576515198
iteration 4636: loss 0.24330224096775055
iteration 4637: loss 0.3126230835914612
iteration 4638: loss 0.21065086126327515
iteration 4639: loss 0.2496454268693924
iteration 4640: loss 0.25834742188453674
iteration 4641: loss 0.2427205741405487
iteration 4642: loss 0.25601086020469666
iteration 4643: loss 0.25763827562332153
iteration 4644: loss 0.2984088063240051
iteration 4645: loss 0.2505558133125305
iteration 4646: loss 0.32206854224205017
iteration 4647: loss 0.30674171447753906
iteration 4648: loss 0.20433291792869568
iteration 4649: loss 0.20056143403053284
iteration 4650: loss 0.29926225543022156
iteration 4651: loss 0.3066196143627167
iteration 4652: loss 0.24323171377182007
iteration 4653: loss 0.22927100956439972
iteration 4654: loss 0.2487526386976242
iteration 4655: loss 0.2658841013908386
iteration 4656: loss 0.28356271982192993
iteration 4657: loss 0.32247889041900635
iteration 4658: loss 0.2565820515155792
iteration 4659: loss 0.2367323637008667
iteration 4660: loss 0.29682406783103943
iteration 4661: loss 0.3220786452293396
iteration 4662: loss 0.2669163942337036
iteration 4663: loss 0.22754764556884766
iteration 4664: loss 0.3315380811691284
iteration 4665: loss 0.3153689205646515
iteration 4666: loss 0.28680843114852905
iteration 4667: loss 0.221229687333107
iteration 4668: loss 0.22011925280094147
iteration 4669: loss 0.24193722009658813
iteration 4670: loss 0.2813699543476105
iteration 4671: loss 0.2638222277164459
iteration 4672: loss 0.2771543264389038
iteration 4673: loss 0.2651759088039398
iteration 4674: loss 0.21021175384521484
iteration 4675: loss 0.2937493622303009
iteration 4676: loss 0.32570940256118774
iteration 4677: loss 0.29433679580688477
iteration 4678: loss 0.3023529350757599
iteration 4679: loss 0.26726481318473816
iteration 4680: loss 0.2517869174480438
iteration 4681: loss 0.27484744787216187
iteration 4682: loss 0.2949926257133484
iteration 4683: loss 0.29505765438079834
iteration 4684: loss 0.29262402653694153
iteration 4685: loss 0.31999143958091736
iteration 4686: loss 0.28307491540908813
iteration 4687: loss 0.2557791471481323
iteration 4688: loss 0.28061357140541077
iteration 4689: loss 0.23987694084644318
iteration 4690: loss 0.24555112421512604
iteration 4691: loss 0.22353675961494446
iteration 4692: loss 0.23393496870994568
iteration 4693: loss 0.23057520389556885
iteration 4694: loss 0.322215735912323
iteration 4695: loss 0.28801876306533813
iteration 4696: loss 0.30845579504966736
iteration 4697: loss 0.29024410247802734
iteration 4698: loss 0.22478224337100983
iteration 4699: loss 0.31786230206489563
iteration 4700: loss 0.245676651597023
iteration 4701: loss 0.31395092606544495
iteration 4702: loss 0.26707494258880615
iteration 4703: loss 0.3037077784538269
iteration 4704: loss 0.2525073289871216
iteration 4705: loss 0.3386407196521759
iteration 4706: loss 0.28079521656036377
iteration 4707: loss 0.23877933621406555
iteration 4708: loss 0.3234699070453644
iteration 4709: loss 0.30632275342941284
iteration 4710: loss 0.26424530148506165
iteration 4711: loss 0.32914260029792786
iteration 4712: loss 0.2429763525724411
iteration 4713: loss 0.2855246365070343
iteration 4714: loss 0.30387550592422485
iteration 4715: loss 0.28136181831359863
iteration 4716: loss 0.24100446701049805
iteration 4717: loss 0.32010161876678467
iteration 4718: loss 0.2590140700340271
iteration 4719: loss 0.2355777621269226
iteration 4720: loss 0.35590261220932007
iteration 4721: loss 0.2481440156698227
iteration 4722: loss 0.3236634433269501
iteration 4723: loss 0.27460893988609314
iteration 4724: loss 0.30002129077911377
iteration 4725: loss 0.2656722664833069
iteration 4726: loss 0.3072454035282135
iteration 4727: loss 0.29620054364204407
iteration 4728: loss 0.2532002627849579
iteration 4729: loss 0.26897069811820984
iteration 4730: loss 0.2908633351325989
iteration 4731: loss 0.27692604064941406
iteration 4732: loss 0.2538071572780609
iteration 4733: loss 0.3195386230945587
iteration 4734: loss 0.3222859501838684
iteration 4735: loss 0.3506665527820587
iteration 4736: loss 0.30315205454826355
iteration 4737: loss 0.23383714258670807
iteration 4738: loss 0.30523210763931274
iteration 4739: loss 0.26486146450042725
iteration 4740: loss 0.27785342931747437
iteration 4741: loss 0.2921386957168579
iteration 4742: loss 0.2782955467700958
iteration 4743: loss 0.2992396950721741
iteration 4744: loss 0.2851543724536896
iteration 4745: loss 0.3073165714740753
iteration 4746: loss 0.2800382375717163
iteration 4747: loss 0.24443630874156952
iteration 4748: loss 0.31395223736763
iteration 4749: loss 0.26512932777404785
iteration 4750: loss 0.2864236533641815
iteration 4751: loss 0.2681620121002197
iteration 4752: loss 0.3253260850906372
iteration 4753: loss 0.20530182123184204
iteration 4754: loss 0.23942723870277405
iteration 4755: loss 0.2734376788139343
iteration 4756: loss 0.28720876574516296
iteration 4757: loss 0.2984057664871216
iteration 4758: loss 0.2397831380367279
iteration 4759: loss 0.2330293357372284
iteration 4760: loss 0.2983652353286743
iteration 4761: loss 0.2595939636230469
iteration 4762: loss 0.2323037087917328
iteration 4763: loss 0.24872374534606934
iteration 4764: loss 0.26591113209724426
iteration 4765: loss 0.3200441300868988
iteration 4766: loss 0.2626020312309265
iteration 4767: loss 0.210606649518013
iteration 4768: loss 0.32092005014419556
iteration 4769: loss 0.21795643866062164
iteration 4770: loss 0.30618172883987427
iteration 4771: loss 0.2399251013994217
iteration 4772: loss 0.2939873933792114
iteration 4773: loss 0.2986801862716675
iteration 4774: loss 0.2899491488933563
iteration 4775: loss 0.2649315297603607
iteration 4776: loss 0.276877760887146
iteration 4777: loss 0.278417706489563
iteration 4778: loss 0.2665921151638031
iteration 4779: loss 0.3250541687011719
iteration 4780: loss 0.23969553411006927
iteration 4781: loss 0.2788810133934021
iteration 4782: loss 0.2514214515686035
iteration 4783: loss 0.27944180369377136
iteration 4784: loss 0.304691344499588
iteration 4785: loss 0.3444683253765106
iteration 4786: loss 0.2775525450706482
iteration 4787: loss 0.24724507331848145
iteration 4788: loss 0.3261469304561615
iteration 4789: loss 0.20379547774791718
iteration 4790: loss 0.3411537706851959
iteration 4791: loss 0.26595020294189453
iteration 4792: loss 0.26158809661865234
iteration 4793: loss 0.21952587366104126
iteration 4794: loss 0.25508931279182434
iteration 4795: loss 0.2983345091342926
iteration 4796: loss 0.23761138319969177
iteration 4797: loss 0.2663710415363312
iteration 4798: loss 0.30156445503234863
iteration 4799: loss 0.2900514602661133
iteration 4800: loss 0.3291493058204651
iteration 4801: loss 0.2460809201002121
iteration 4802: loss 0.31402093172073364
iteration 4803: loss 0.2957562804222107
iteration 4804: loss 0.23415842652320862
iteration 4805: loss 0.40135473012924194
iteration 4806: loss 0.32481852173805237
iteration 4807: loss 0.24268674850463867
iteration 4808: loss 0.2500784993171692
iteration 4809: loss 0.2929507791996002
iteration 4810: loss 0.23809893429279327
iteration 4811: loss 0.3454965353012085
iteration 4812: loss 0.23710879683494568
iteration 4813: loss 0.2627149820327759
iteration 4814: loss 0.23795798420906067
iteration 4815: loss 0.28703463077545166
iteration 4816: loss 0.30860501527786255
iteration 4817: loss 0.30322563648223877
iteration 4818: loss 0.22873981297016144
iteration 4819: loss 0.269992470741272
iteration 4820: loss 0.24105145037174225
iteration 4821: loss 0.294810026884079
iteration 4822: loss 0.2536954879760742
iteration 4823: loss 0.3312656283378601
iteration 4824: loss 0.26494839787483215
iteration 4825: loss 0.2913018465042114
iteration 4826: loss 0.31239914894104004
iteration 4827: loss 0.23798783123493195
iteration 4828: loss 0.25350555777549744
iteration 4829: loss 0.2926434278488159
iteration 4830: loss 0.2632110118865967
iteration 4831: loss 0.2571992576122284
iteration 4832: loss 0.28952041268348694
iteration 4833: loss 0.2592875361442566
iteration 4834: loss 0.28627926111221313
iteration 4835: loss 0.20599640905857086
iteration 4836: loss 0.23694269359111786
iteration 4837: loss 0.2936311662197113
iteration 4838: loss 0.26572221517562866
iteration 4839: loss 0.2744876444339752
iteration 4840: loss 0.3488573729991913
iteration 4841: loss 0.23089735209941864
iteration 4842: loss 0.2488282024860382
iteration 4843: loss 0.2917756736278534
iteration 4844: loss 0.26758748292922974
iteration 4845: loss 0.25064823031425476
iteration 4846: loss 0.21283751726150513
iteration 4847: loss 0.27079713344573975
iteration 4848: loss 0.32304733991622925
iteration 4849: loss 0.2429657280445099
iteration 4850: loss 0.2031373530626297
iteration 4851: loss 0.31721317768096924
iteration 4852: loss 0.3109810948371887
iteration 4853: loss 0.25118669867515564
iteration 4854: loss 0.23843754827976227
iteration 4855: loss 0.2712249457836151
iteration 4856: loss 0.30884942412376404
iteration 4857: loss 0.2752186357975006
iteration 4858: loss 0.2608533501625061
iteration 4859: loss 0.29476502537727356
iteration 4860: loss 0.2674359381198883
iteration 4861: loss 0.27742740511894226
iteration 4862: loss 0.3241024613380432
iteration 4863: loss 0.2847502827644348
iteration 4864: loss 0.24989034235477448
iteration 4865: loss 0.27256473898887634
iteration 4866: loss 0.22962743043899536
iteration 4867: loss 0.2499905377626419
iteration 4868: loss 0.3056117296218872
iteration 4869: loss 0.32371461391448975
iteration 4870: loss 0.28613990545272827
iteration 4871: loss 0.31085601449012756
iteration 4872: loss 0.31259721517562866
iteration 4873: loss 0.2216535061597824
iteration 4874: loss 0.23420532047748566
iteration 4875: loss 0.3288162648677826
iteration 4876: loss 0.22617007791996002
iteration 4877: loss 0.2827674150466919
iteration 4878: loss 0.2532024681568146
iteration 4879: loss 0.25266602635383606
iteration 4880: loss 0.24397099018096924
iteration 4881: loss 0.2511900067329407
iteration 4882: loss 0.2601580023765564
iteration 4883: loss 0.31100940704345703
iteration 4884: loss 0.3347061276435852
iteration 4885: loss 0.28399473428726196
iteration 4886: loss 0.26866984367370605
iteration 4887: loss 0.28906795382499695
iteration 4888: loss 0.2823602855205536
iteration 4889: loss 0.33286815881729126
iteration 4890: loss 0.23994329571723938
iteration 4891: loss 0.28424525260925293
iteration 4892: loss 0.25324130058288574
iteration 4893: loss 0.27766814827919006
iteration 4894: loss 0.2958312928676605
iteration 4895: loss 0.3126760721206665
iteration 4896: loss 0.3053984045982361
iteration 4897: loss 0.2926729917526245
iteration 4898: loss 0.23934237658977509
iteration 4899: loss 0.23714444041252136
iteration 4900: loss 0.3158152997493744
iteration 4901: loss 0.3058460056781769
iteration 4902: loss 0.22297470271587372
iteration 4903: loss 0.2549474239349365
iteration 4904: loss 0.2253977358341217
iteration 4905: loss 0.2893204092979431
iteration 4906: loss 0.30427730083465576
iteration 4907: loss 0.30479997396469116
iteration 4908: loss 0.2532036602497101
iteration 4909: loss 0.2797337770462036
iteration 4910: loss 0.31887656450271606
iteration 4911: loss 0.2812245488166809
iteration 4912: loss 0.2355833798646927
iteration 4913: loss 0.40776702761650085
iteration 4914: loss 0.3068930506706238
iteration 4915: loss 0.26626431941986084
iteration 4916: loss 0.26840004324913025
iteration 4917: loss 0.2781239449977875
iteration 4918: loss 0.27871784567832947
iteration 4919: loss 0.26936787366867065
iteration 4920: loss 0.2614222466945648
iteration 4921: loss 0.21434715390205383
iteration 4922: loss 0.3057640790939331
iteration 4923: loss 0.268365740776062
iteration 4924: loss 0.2466880828142166
iteration 4925: loss 0.25328385829925537
iteration 4926: loss 0.29257750511169434
iteration 4927: loss 0.2718721628189087
iteration 4928: loss 0.2724285423755646
iteration 4929: loss 0.31850919127464294
iteration 4930: loss 0.2461625337600708
iteration 4931: loss 0.2515178918838501
iteration 4932: loss 0.2666175663471222
iteration 4933: loss 0.26053062081336975
iteration 4934: loss 0.2711992859840393
iteration 4935: loss 0.2622097432613373
iteration 4936: loss 0.25918927788734436
iteration 4937: loss 0.2814943790435791
iteration 4938: loss 0.2691033184528351
iteration 4939: loss 0.27052757143974304
iteration 4940: loss 0.3076530992984772
iteration 4941: loss 0.2977275252342224
iteration 4942: loss 0.27489641308784485
iteration 4943: loss 0.27105259895324707
iteration 4944: loss 0.278770387172699
iteration 4945: loss 0.2699698209762573
iteration 4946: loss 0.3049618899822235
iteration 4947: loss 0.24939224123954773
iteration 4948: loss 0.2550952732563019
iteration 4949: loss 0.2128566950559616
iteration 4950: loss 0.20880241692066193
iteration 4951: loss 0.30978843569755554
iteration 4952: loss 0.25450390577316284
iteration 4953: loss 0.25625574588775635
iteration 4954: loss 0.24688388407230377
iteration 4955: loss 0.3689446747303009
iteration 4956: loss 0.2793826460838318
iteration 4957: loss 0.3109993636608124
iteration 4958: loss 0.32658839225769043
iteration 4959: loss 0.3309466242790222
iteration 4960: loss 0.30870360136032104
iteration 4961: loss 0.27311471104621887
iteration 4962: loss 0.25131943821907043
iteration 4963: loss 0.26017478108406067
iteration 4964: loss 0.2965254485607147
iteration 4965: loss 0.25315767526626587
iteration 4966: loss 0.2964482605457306
iteration 4967: loss 0.21818548440933228
iteration 4968: loss 0.3052660822868347
iteration 4969: loss 0.2571638822555542
iteration 4970: loss 0.2867909371852875
iteration 4971: loss 0.2474769651889801
iteration 4972: loss 0.26920849084854126
iteration 4973: loss 0.2766261696815491
iteration 4974: loss 0.28122594952583313
iteration 4975: loss 0.3040991723537445
iteration 4976: loss 0.310793936252594
iteration 4977: loss 0.35053592920303345
iteration 4978: loss 0.27293166518211365
iteration 4979: loss 0.2889270782470703
iteration 4980: loss 0.305448979139328
iteration 4981: loss 0.24817906320095062
iteration 4982: loss 0.27116265892982483
iteration 4983: loss 0.3344014883041382
iteration 4984: loss 0.23612503707408905
iteration 4985: loss 0.31191229820251465
iteration 4986: loss 0.2634720802307129
iteration 4987: loss 0.2695671319961548
iteration 4988: loss 0.25273948907852173
iteration 4989: loss 0.293770968914032
iteration 4990: loss 0.31752440333366394
iteration 4991: loss 0.2651553452014923
iteration 4992: loss 0.29901576042175293
iteration 4993: loss 0.2502765357494354
iteration 4994: loss 0.2936691343784332
iteration 4995: loss 0.2779945135116577
iteration 4996: loss 0.2313462197780609
iteration 4997: loss 0.2698730230331421
iteration 4998: loss 0.22041882574558258
iteration 4999: loss 0.2956106662750244
Accuracy:  0.9251 
Recall:  [(0.976530612244898, 0.9456521739130435), (0.9788546255506608, 0.9644097222222222), (0.8982558139534884, 0.926073926073926), (0.9089108910891089, 0.9107142857142857), (0.9409368635437881, 0.90234375), (0.8699551569506726, 0.9097303634232122), (0.9488517745302714, 0.9323076923076923), (0.9299610894941635, 0.9354207436399217), (0.8963039014373717, 0.8963039014373717), (0.8919722497522299, 0.9193054136874361)] 
Matrix:
 [[ 957    0    4    2    0    4    9    1    3    0]
 [   0 1111    2    2    0    1    4    2   13    0]
 [  11    8  927   14   15    2   10   12   29    4]
 [   2    0   24  918    0   28    1   15   16    6]
 [   1    2    4    0  924    0   12    2    5   32]
 [   9    3    3   37    8  776   17    6   25    8]
 [  12    3    4    0   13   13  909    1    3    0]
 [   3    9   27    6    6    0    0  956    2   19]
 [   8    8    5   20   11   19   12    8  873   10]
 [   9    8    1    9   47   10    1   19    5  900]]
iteration 0: loss 2.3023622035980225
iteration 1: loss 2.3012490272521973
iteration 2: loss 2.3001303672790527
iteration 3: loss 2.2990012168884277
iteration 4: loss 2.2978596687316895
iteration 5: loss 2.296699285507202
iteration 6: loss 2.295515298843384
iteration 7: loss 2.2943034172058105
iteration 8: loss 2.2930588722229004
iteration 9: loss 2.2917773723602295
iteration 10: loss 2.290456771850586
iteration 11: loss 2.2890944480895996
iteration 12: loss 2.2876880168914795
iteration 13: loss 2.2862343788146973
iteration 14: loss 2.284731864929199
iteration 15: loss 2.28318190574646
iteration 16: loss 2.281582832336426
iteration 17: loss 2.279934883117676
iteration 18: loss 2.278240442276001
iteration 19: loss 2.2764976024627686
iteration 20: loss 2.274707794189453
iteration 21: loss 2.272871732711792
iteration 22: loss 2.2709884643554688
iteration 23: loss 2.2690579891204834
iteration 24: loss 2.267078161239624
iteration 25: loss 2.265050172805786
iteration 26: loss 2.2629733085632324
iteration 27: loss 2.260845899581909
iteration 28: loss 2.258667230606079
iteration 29: loss 2.256437301635742
iteration 30: loss 2.2541563510894775
iteration 31: loss 2.2518224716186523
iteration 32: loss 2.249436855316162
iteration 33: loss 2.246999740600586
iteration 34: loss 2.244509696960449
iteration 35: loss 2.2419657707214355
iteration 36: loss 2.239368200302124
iteration 37: loss 2.23671555519104
iteration 38: loss 2.234008550643921
iteration 39: loss 2.2312474250793457
iteration 40: loss 2.2284305095672607
iteration 41: loss 2.2255592346191406
iteration 42: loss 2.222633123397827
iteration 43: loss 2.219650983810425
iteration 44: loss 2.2166125774383545
iteration 45: loss 2.213517904281616
iteration 46: loss 2.2103657722473145
iteration 47: loss 2.2071564197540283
iteration 48: loss 2.203890085220337
iteration 49: loss 2.2005648612976074
iteration 50: loss 2.1971821784973145
iteration 51: loss 2.1937406063079834
iteration 52: loss 2.190239667892456
iteration 53: loss 2.1866800785064697
iteration 54: loss 2.1830615997314453
iteration 55: loss 2.1793832778930664
iteration 56: loss 2.1756460666656494
iteration 57: loss 2.171849012374878
iteration 58: loss 2.1679930686950684
iteration 59: loss 2.1640772819519043
iteration 60: loss 2.160102367401123
iteration 61: loss 2.156067132949829
iteration 62: loss 2.151972770690918
iteration 63: loss 2.147820472717285
iteration 64: loss 2.143608331680298
iteration 65: loss 2.1393377780914307
iteration 66: loss 2.135009765625
iteration 67: loss 2.1306233406066895
iteration 68: loss 2.1261796951293945
iteration 69: loss 2.1216790676116943
iteration 70: loss 2.117121458053589
iteration 71: loss 2.1125071048736572
iteration 72: loss 2.107837200164795
iteration 73: loss 2.103111982345581
iteration 74: loss 2.0983314514160156
iteration 75: loss 2.093496799468994
iteration 76: loss 2.0886080265045166
iteration 77: loss 2.083665609359741
iteration 78: loss 2.0786709785461426
iteration 79: loss 2.0736238956451416
iteration 80: loss 2.068525552749634
iteration 81: loss 2.0633764266967773
iteration 82: loss 2.0581777095794678
iteration 83: loss 2.052929639816284
iteration 84: loss 2.047632932662964
iteration 85: loss 2.0422885417938232
iteration 86: loss 2.0368969440460205
iteration 87: loss 2.031458854675293
iteration 88: loss 2.025975465774536
iteration 89: loss 2.0204477310180664
iteration 90: loss 2.014875650405884
iteration 91: loss 2.009260654449463
iteration 92: loss 2.0036027431488037
iteration 93: loss 1.9979043006896973
iteration 94: loss 1.9921646118164062
iteration 95: loss 1.9863853454589844
iteration 96: loss 1.9805667400360107
iteration 97: loss 1.9747107028961182
iteration 98: loss 1.968817114830017
iteration 99: loss 1.962887167930603
iteration 100: loss 1.9569215774536133
iteration 101: loss 1.9509220123291016
iteration 102: loss 1.9448884725570679
iteration 103: loss 1.938821792602539
iteration 104: loss 1.9327239990234375
iteration 105: loss 1.9265942573547363
iteration 106: loss 1.9204350709915161
iteration 107: loss 1.9142465591430664
iteration 108: loss 1.9080300331115723
iteration 109: loss 1.9017853736877441
iteration 110: loss 1.8955152034759521
iteration 111: loss 1.8892189264297485
iteration 112: loss 1.882897973060608
iteration 113: loss 1.8765534162521362
iteration 114: loss 1.8701859712600708
iteration 115: loss 1.8637964725494385
iteration 116: loss 1.8573856353759766
iteration 117: loss 1.8509544134140015
iteration 118: loss 1.8445039987564087
iteration 119: loss 1.8380348682403564
iteration 120: loss 1.8315472602844238
iteration 121: loss 1.8250422477722168
iteration 122: loss 1.8185211420059204
iteration 123: loss 1.8119839429855347
iteration 124: loss 1.805431842803955
iteration 125: loss 1.7988650798797607
iteration 126: loss 1.7922838926315308
iteration 127: loss 1.7856895923614502
iteration 128: loss 1.7790827751159668
iteration 129: loss 1.7724632024765015
iteration 130: loss 1.7658329010009766
iteration 131: loss 1.7591907978057861
iteration 132: loss 1.7525391578674316
iteration 133: loss 1.7458792924880981
iteration 134: loss 1.7392115592956543
iteration 135: loss 1.732537865638733
iteration 136: loss 1.7258591651916504
iteration 137: loss 1.7191760540008545
iteration 138: loss 1.712489128112793
iteration 139: loss 1.70579993724823
iteration 140: loss 1.699108362197876
iteration 141: loss 1.6924159526824951
iteration 142: loss 1.685723066329956
iteration 143: loss 1.6790308952331543
iteration 144: loss 1.6723395586013794
iteration 145: loss 1.6656506061553955
iteration 146: loss 1.6589643955230713
iteration 147: loss 1.6522817611694336
iteration 148: loss 1.6456032991409302
iteration 149: loss 1.6389309167861938
iteration 150: loss 1.632263422012329
iteration 151: loss 1.6256028413772583
iteration 152: loss 1.6189500093460083
iteration 153: loss 1.6123050451278687
iteration 154: loss 1.605668306350708
iteration 155: loss 1.5990413427352905
iteration 156: loss 1.592423915863037
iteration 157: loss 1.5858174562454224
iteration 158: loss 1.5792227983474731
iteration 159: loss 1.5726391077041626
iteration 160: loss 1.5660680532455444
iteration 161: loss 1.5595107078552246
iteration 162: loss 1.5529661178588867
iteration 163: loss 1.5464364290237427
iteration 164: loss 1.5399214029312134
iteration 165: loss 1.533421277999878
iteration 166: loss 1.5269372463226318
iteration 167: loss 1.520469307899475
iteration 168: loss 1.5140180587768555
iteration 169: loss 1.507584810256958
iteration 170: loss 1.5011688470840454
iteration 171: loss 1.494770884513855
iteration 172: loss 1.4883918762207031
iteration 173: loss 1.482032060623169
iteration 174: loss 1.475691318511963
iteration 175: loss 1.469370722770691
iteration 176: loss 1.4630703926086426
iteration 177: loss 1.4567911624908447
iteration 178: loss 1.4505324363708496
iteration 179: loss 1.444295883178711
iteration 180: loss 1.4380806684494019
iteration 181: loss 1.4318877458572388
iteration 182: loss 1.4257168769836426
iteration 183: loss 1.4195692539215088
iteration 184: loss 1.4134443998336792
iteration 185: loss 1.4073429107666016
iteration 186: loss 1.401265263557434
iteration 187: loss 1.3952112197875977
iteration 188: loss 1.3891817331314087
iteration 189: loss 1.3831766843795776
iteration 190: loss 1.3771965503692627
iteration 191: loss 1.3712408542633057
iteration 192: loss 1.3653104305267334
iteration 193: loss 1.3594056367874146
iteration 194: loss 1.3535261154174805
iteration 195: loss 1.3476722240447998
iteration 196: loss 1.3418446779251099
iteration 197: loss 1.3360426425933838
iteration 198: loss 1.3302674293518066
iteration 199: loss 1.3245186805725098
iteration 200: loss 1.3187963962554932
iteration 201: loss 1.3131005764007568
iteration 202: loss 1.3074318170547485
iteration 203: loss 1.3017898797988892
iteration 204: loss 1.2961758375167847
iteration 205: loss 1.290588140487671
iteration 206: loss 1.2850282192230225
iteration 207: loss 1.2794957160949707
iteration 208: loss 1.2739909887313843
iteration 209: loss 1.268513798713684
iteration 210: loss 1.2630645036697388
iteration 211: loss 1.2576427459716797
iteration 212: loss 1.2522493600845337
iteration 213: loss 1.246883749961853
iteration 214: loss 1.2415457963943481
iteration 215: loss 1.2362364530563354
iteration 216: loss 1.2309552431106567
iteration 217: loss 1.225701928138733
iteration 218: loss 1.2204769849777222
iteration 219: loss 1.2152800559997559
iteration 220: loss 1.2101117372512817
iteration 221: loss 1.2049715518951416
iteration 222: loss 1.1998594999313354
iteration 223: loss 1.1947760581970215
iteration 224: loss 1.1897205114364624
iteration 225: loss 1.1846939325332642
iteration 226: loss 1.1796952486038208
iteration 227: loss 1.174724817276001
iteration 228: loss 1.1697828769683838
iteration 229: loss 1.1648691892623901
iteration 230: loss 1.1599836349487305
iteration 231: loss 1.1551260948181152
iteration 232: loss 1.1502968072891235
iteration 233: loss 1.145495891571045
iteration 234: loss 1.1407228708267212
iteration 235: loss 1.135978102684021
iteration 236: loss 1.1312613487243652
iteration 237: loss 1.1265718936920166
iteration 238: loss 1.1219106912612915
iteration 239: loss 1.1172771453857422
iteration 240: loss 1.1126710176467896
iteration 241: loss 1.1080927848815918
iteration 242: loss 1.103541374206543
iteration 243: loss 1.0990177392959595
iteration 244: loss 1.0945208072662354
iteration 245: loss 1.0900510549545288
iteration 246: loss 1.0856082439422607
iteration 247: loss 1.0811916589736938
iteration 248: loss 1.0768016576766968
iteration 249: loss 1.0724371671676636
iteration 250: loss 1.068098545074463
iteration 251: loss 1.0637843608856201
iteration 252: loss 1.059495210647583
iteration 253: loss 1.0552301406860352
iteration 254: loss 1.0509898662567139
iteration 255: loss 1.0467745065689087
iteration 256: loss 1.0425844192504883
iteration 257: loss 1.0384200811386108
iteration 258: loss 1.0342814922332764
iteration 259: loss 1.0301687717437744
iteration 260: loss 1.0260820388793945
iteration 261: loss 1.0220212936401367
iteration 262: loss 1.0179864168167114
iteration 263: loss 1.0139774084091187
iteration 264: loss 1.0099937915802002
iteration 265: loss 1.0060359239578247
iteration 266: loss 1.0021032094955444
iteration 267: loss 0.9981958866119385
iteration 268: loss 0.9943138957023621
iteration 269: loss 0.9904568791389465
iteration 270: loss 0.9866249561309814
iteration 271: loss 0.9828179478645325
iteration 272: loss 0.9790358543395996
iteration 273: loss 0.975278377532959
iteration 274: loss 0.9715456962585449
iteration 275: loss 0.9678373336791992
iteration 276: loss 0.9641534090042114
iteration 277: loss 0.9604939222335815
iteration 278: loss 0.9568585753440857
iteration 279: loss 0.9532469511032104
iteration 280: loss 0.9496592879295349
iteration 281: loss 0.9460956454277039
iteration 282: loss 0.9425554871559143
iteration 283: loss 0.9390391111373901
iteration 284: loss 0.9355456829071045
iteration 285: loss 0.9320758581161499
iteration 286: loss 0.9286289215087891
iteration 287: loss 0.9252051115036011
iteration 288: loss 0.9218040704727173
iteration 289: loss 0.9184256792068481
iteration 290: loss 0.9150698781013489
iteration 291: loss 0.9117366075515747
iteration 292: loss 0.9084254503250122
iteration 293: loss 0.9051364660263062
iteration 294: loss 0.9018693566322327
iteration 295: loss 0.8986243605613708
iteration 296: loss 0.8954009413719177
iteration 297: loss 0.8921989798545837
iteration 298: loss 0.8890187740325928
iteration 299: loss 0.8858594298362732
iteration 300: loss 0.8827213644981384
iteration 301: loss 0.8796043395996094
iteration 302: loss 0.8765078783035278
iteration 303: loss 0.8734322190284729
iteration 304: loss 0.8703768253326416
iteration 305: loss 0.867341935634613
iteration 306: loss 0.8643270134925842
iteration 307: loss 0.8613321185112
iteration 308: loss 0.8583571910858154
iteration 309: loss 0.8554018139839172
iteration 310: loss 0.8524657487869263
iteration 311: loss 0.8495492339134216
iteration 312: loss 0.8466517925262451
iteration 313: loss 0.843773603439331
iteration 314: loss 0.8409144878387451
iteration 315: loss 0.8380738496780396
iteration 316: loss 0.8352518677711487
iteration 317: loss 0.8324483633041382
iteration 318: loss 0.8296631574630737
iteration 319: loss 0.8268962502479553
iteration 320: loss 0.8241475820541382
iteration 321: loss 0.8214166760444641
iteration 322: loss 0.8187035322189331
iteration 323: loss 0.8160081505775452
iteration 324: loss 0.8133304715156555
iteration 325: loss 0.8106701374053955
iteration 326: loss 0.8080268502235413
iteration 327: loss 0.8054009079933167
iteration 328: loss 0.8027922511100769
iteration 329: loss 0.8002002835273743
iteration 330: loss 0.7976253032684326
iteration 331: loss 0.7950668334960938
iteration 332: loss 0.7925248146057129
iteration 333: loss 0.7899991869926453
iteration 334: loss 0.7874897122383118
iteration 335: loss 0.7849964499473572
iteration 336: loss 0.7825192213058472
iteration 337: loss 0.7800576090812683
iteration 338: loss 0.7776119709014893
iteration 339: loss 0.7751815915107727
iteration 340: loss 0.7727668881416321
iteration 341: loss 0.7703675031661987
iteration 342: loss 0.7679833173751831
iteration 343: loss 0.7656142711639404
iteration 344: loss 0.7632604837417603
iteration 345: loss 0.7609213590621948
iteration 346: loss 0.7585970759391785
iteration 347: loss 0.7562878131866455
iteration 348: loss 0.7539929747581482
iteration 349: loss 0.7517125606536865
iteration 350: loss 0.7494466304779053
iteration 351: loss 0.7471946477890015
iteration 352: loss 0.7449570894241333
iteration 353: loss 0.7427340745925903
iteration 354: loss 0.7405245304107666
iteration 355: loss 0.7383291125297546
iteration 356: loss 0.7361475229263306
iteration 357: loss 0.7339797019958496
iteration 358: loss 0.7318254113197327
iteration 359: loss 0.7296846508979797
iteration 360: loss 0.7275573015213013
iteration 361: loss 0.7254433035850525
iteration 362: loss 0.7233424186706543
iteration 363: loss 0.7212546467781067
iteration 364: loss 0.7191798090934753
iteration 365: loss 0.7171180248260498
iteration 366: loss 0.7150688171386719
iteration 367: loss 0.7130325436592102
iteration 368: loss 0.7110089063644409
iteration 369: loss 0.7089977860450745
iteration 370: loss 0.7069991827011108
iteration 371: loss 0.7050127983093262
iteration 372: loss 0.7030388712882996
iteration 373: loss 0.7010771036148071
iteration 374: loss 0.6991272568702698
iteration 375: loss 0.6971896290779114
iteration 376: loss 0.6952638626098633
iteration 377: loss 0.6933500170707703
iteration 378: loss 0.6914477348327637
iteration 379: loss 0.6895571351051331
iteration 380: loss 0.6876784563064575
iteration 381: loss 0.685810923576355
iteration 382: loss 0.6839545965194702
iteration 383: loss 0.6821098327636719
iteration 384: loss 0.6802765727043152
iteration 385: loss 0.6784539222717285
iteration 386: loss 0.6766424775123596
iteration 387: loss 0.674842119216919
iteration 388: loss 0.6730527877807617
iteration 389: loss 0.6712740659713745
iteration 390: loss 0.6695062518119812
iteration 391: loss 0.6677488684654236
iteration 392: loss 0.6660022139549255
iteration 393: loss 0.6642661690711975
iteration 394: loss 0.6625403761863708
iteration 395: loss 0.6608250141143799
iteration 396: loss 0.6591199636459351
iteration 397: loss 0.657425045967102
iteration 398: loss 0.6557402014732361
iteration 399: loss 0.6540656685829163
iteration 400: loss 0.6524010300636292
iteration 401: loss 0.6507463455200195
iteration 402: loss 0.6491013169288635
iteration 403: loss 0.647466242313385
iteration 404: loss 0.6458407640457153
iteration 405: loss 0.6442248225212097
iteration 406: loss 0.6426185369491577
iteration 407: loss 0.6410217881202698
iteration 408: loss 0.6394343376159668
iteration 409: loss 0.6378563046455383
iteration 410: loss 0.6362875699996948
iteration 411: loss 0.6347278356552124
iteration 412: loss 0.6331773996353149
iteration 413: loss 0.6316359043121338
iteration 414: loss 0.6301034688949585
iteration 415: loss 0.6285799741744995
iteration 416: loss 0.6270653605461121
iteration 417: loss 0.6255594491958618
iteration 418: loss 0.6240622401237488
iteration 419: loss 0.6225740313529968
iteration 420: loss 0.6210940480232239
iteration 421: loss 0.6196229457855225
iteration 422: loss 0.6181600689888
iteration 423: loss 0.6167054772377014
iteration 424: loss 0.6152596473693848
iteration 425: loss 0.6138218641281128
iteration 426: loss 0.6123923063278198
iteration 427: loss 0.6109709143638611
iteration 428: loss 0.6095577478408813
iteration 429: loss 0.608152449131012
iteration 430: loss 0.6067551970481873
iteration 431: loss 0.6053659915924072
iteration 432: loss 0.6039845943450928
iteration 433: loss 0.6026110053062439
iteration 434: loss 0.601245105266571
iteration 435: loss 0.5998870134353638
iteration 436: loss 0.5985364317893982
iteration 437: loss 0.5971934795379639
iteration 438: loss 0.5958579778671265
iteration 439: loss 0.5945301651954651
iteration 440: loss 0.593209445476532
iteration 441: loss 0.5918964743614197
iteration 442: loss 0.5905906558036804
iteration 443: loss 0.5892921686172485
iteration 444: loss 0.5880006551742554
iteration 445: loss 0.58671635389328
iteration 446: loss 0.5854393243789673
iteration 447: loss 0.5841692686080933
iteration 448: loss 0.582906186580658
iteration 449: loss 0.5816500186920166
iteration 450: loss 0.580400824546814
iteration 451: loss 0.5791584253311157
iteration 452: loss 0.577923059463501
iteration 453: loss 0.5766939520835876
iteration 454: loss 0.5754718780517578
iteration 455: loss 0.5742564797401428
iteration 456: loss 0.5730476975440979
iteration 457: loss 0.5718454122543335
iteration 458: loss 0.5706495642662048
iteration 459: loss 0.5694601535797119
iteration 460: loss 0.5682774782180786
iteration 461: loss 0.5671007037162781
iteration 462: loss 0.5659305453300476
iteration 463: loss 0.5647666454315186
iteration 464: loss 0.5636090040206909
iteration 465: loss 0.5624576807022095
iteration 466: loss 0.561312198638916
iteration 467: loss 0.560172975063324
iteration 468: loss 0.559039831161499
iteration 469: loss 0.5579124689102173
iteration 470: loss 0.5567915439605713
iteration 471: loss 0.5556761026382446
iteration 472: loss 0.5545667409896851
iteration 473: loss 0.5534632205963135
iteration 474: loss 0.5523653030395508
iteration 475: loss 0.5512734055519104
iteration 476: loss 0.5501871705055237
iteration 477: loss 0.5491066575050354
iteration 478: loss 0.5480315089225769
iteration 479: loss 0.5469622611999512
iteration 480: loss 0.5458984375
iteration 481: loss 0.5448401570320129
iteration 482: loss 0.54378741979599
iteration 483: loss 0.5427401661872864
iteration 484: loss 0.5416982173919678
iteration 485: loss 0.5406618118286133
iteration 486: loss 0.539630651473999
iteration 487: loss 0.5386047959327698
iteration 488: loss 0.537584125995636
iteration 489: loss 0.536568820476532
iteration 490: loss 0.5355586409568787
iteration 491: loss 0.534553587436676
iteration 492: loss 0.5335537195205688
iteration 493: loss 0.5325589179992676
iteration 494: loss 0.5315693020820618
iteration 495: loss 0.5305845141410828
iteration 496: loss 0.5296046733856201
iteration 497: loss 0.5286297798156738
iteration 498: loss 0.5276599526405334
iteration 499: loss 0.5266950726509094
iteration 500: loss 0.5257348418235779
iteration 501: loss 0.5247794985771179
iteration 502: loss 0.5238288640975952
iteration 503: loss 0.5228831171989441
iteration 504: loss 0.521942138671875
iteration 505: loss 0.5210056304931641
iteration 506: loss 0.5200739502906799
iteration 507: loss 0.519146740436554
iteration 508: loss 0.5182241797447205
iteration 509: loss 0.5173062682151794
iteration 510: loss 0.5163928866386414
iteration 511: loss 0.5154840350151062
iteration 512: loss 0.5145795345306396
iteration 513: loss 0.513679563999176
iteration 514: loss 0.5127840042114258
iteration 515: loss 0.5118926763534546
iteration 516: loss 0.5110058188438416
iteration 517: loss 0.5101231932640076
iteration 518: loss 0.5092448592185974
iteration 519: loss 0.5083708167076111
iteration 520: loss 0.5075010657310486
iteration 521: loss 0.5066354274749756
iteration 522: loss 0.5057740211486816
iteration 523: loss 0.5049167275428772
iteration 524: loss 0.5040636658668518
iteration 525: loss 0.5032146573066711
iteration 526: loss 0.5023696422576904
iteration 527: loss 0.5015286803245544
iteration 528: loss 0.5006918907165527
iteration 529: loss 0.49985891580581665
iteration 530: loss 0.49903008341789246
iteration 531: loss 0.4982049763202667
iteration 532: loss 0.4973840117454529
iteration 533: loss 0.49656686186790466
iteration 534: loss 0.49575355648994446
iteration 535: loss 0.4949440062046051
iteration 536: loss 0.4941383898258209
iteration 537: loss 0.4933364689350128
iteration 538: loss 0.4925384223461151
iteration 539: loss 0.49174410104751587
iteration 540: loss 0.49095335602760315
iteration 541: loss 0.49016639590263367
iteration 542: loss 0.48938319087028503
iteration 543: loss 0.4886035919189453
iteration 544: loss 0.4878275692462921
iteration 545: loss 0.4870551526546478
iteration 546: loss 0.48628637194633484
iteration 547: loss 0.48552098870277405
iteration 548: loss 0.4847594201564789
iteration 549: loss 0.4840012788772583
iteration 550: loss 0.4832465648651123
iteration 551: loss 0.48249533772468567
iteration 552: loss 0.4817475974559784
iteration 553: loss 0.4810032248497009
iteration 554: loss 0.48026230931282043
iteration 555: loss 0.4795246422290802
iteration 556: loss 0.4787905216217041
iteration 557: loss 0.47805964946746826
iteration 558: loss 0.47733208537101746
iteration 559: loss 0.4766077399253845
iteration 560: loss 0.4758867621421814
iteration 561: loss 0.47516903281211853
iteration 562: loss 0.474454402923584
iteration 563: loss 0.4737430810928345
iteration 564: loss 0.4730350077152252
iteration 565: loss 0.4723300039768219
iteration 566: loss 0.47162824869155884
iteration 567: loss 0.4709295630455017
iteration 568: loss 0.4702340364456177
iteration 569: loss 0.46954160928726196
iteration 570: loss 0.4688522219657898
iteration 571: loss 0.46816587448120117
iteration 572: loss 0.46748262643814087
iteration 573: loss 0.46680235862731934
iteration 574: loss 0.4661252200603485
iteration 575: loss 0.4654509127140045
iteration 576: loss 0.4647796154022217
iteration 577: loss 0.464111328125
iteration 578: loss 0.4634460508823395
iteration 579: loss 0.4627835154533386
iteration 580: loss 0.4621240198612213
iteration 581: loss 0.46146732568740845
iteration 582: loss 0.46081364154815674
iteration 583: loss 0.46016278862953186
iteration 584: loss 0.4595147371292114
iteration 585: loss 0.45886945724487305
iteration 586: loss 0.4582269489765167
iteration 587: loss 0.45758724212646484
iteration 588: loss 0.45695018768310547
iteration 589: loss 0.4563160538673401
iteration 590: loss 0.4556846022605896
iteration 591: loss 0.4550558924674988
iteration 592: loss 0.45442983508110046
iteration 593: loss 0.45380643010139465
iteration 594: loss 0.4531858265399933
iteration 595: loss 0.4525679051876068
iteration 596: loss 0.45195257663726807
iteration 597: loss 0.4513399004936218
iteration 598: loss 0.4507298171520233
iteration 599: loss 0.45012232661247253
iteration 600: loss 0.4495173692703247
iteration 601: loss 0.4489150643348694
iteration 602: loss 0.44831526279449463
iteration 603: loss 0.44771793484687805
iteration 604: loss 0.44712311029434204
iteration 605: loss 0.44653090834617615
iteration 606: loss 0.4459410607814789
iteration 607: loss 0.445353627204895
iteration 608: loss 0.4447687864303589
iteration 609: loss 0.44418618083000183
iteration 610: loss 0.4436061382293701
iteration 611: loss 0.4430284798145294
iteration 612: loss 0.44245317578315735
iteration 613: loss 0.4418801963329315
iteration 614: loss 0.4413096606731415
iteration 615: loss 0.4407414197921753
iteration 616: loss 0.4401755630970001
iteration 617: loss 0.43961191177368164
iteration 618: loss 0.4390506148338318
iteration 619: loss 0.4384915828704834
iteration 620: loss 0.4379348158836365
iteration 621: loss 0.4373803734779358
iteration 622: loss 0.43682822585105896
iteration 623: loss 0.4362783133983612
iteration 624: loss 0.4357306957244873
iteration 625: loss 0.4351850748062134
iteration 626: loss 0.4346417784690857
iteration 627: loss 0.43410077691078186
iteration 628: loss 0.4335618019104004
iteration 629: loss 0.4330250918865204
iteration 630: loss 0.4324905574321747
iteration 631: loss 0.4319581389427185
iteration 632: loss 0.4314277768135071
iteration 633: loss 0.4308997094631195
iteration 634: loss 0.43037354946136475
iteration 635: loss 0.4298495948314667
iteration 636: loss 0.42932769656181335
iteration 637: loss 0.4288078248500824
iteration 638: loss 0.4282902479171753
iteration 639: loss 0.42777445912361145
iteration 640: loss 0.4272608757019043
iteration 641: loss 0.42674922943115234
iteration 642: loss 0.42623960971832275
iteration 643: loss 0.4257321059703827
iteration 644: loss 0.42522644996643066
iteration 645: loss 0.4247228503227234
iteration 646: loss 0.4242211878299713
iteration 647: loss 0.42372140288352966
iteration 648: loss 0.4232236444950104
iteration 649: loss 0.4227278530597687
iteration 650: loss 0.4222339689731598
iteration 651: loss 0.42174190282821655
iteration 652: loss 0.42125174403190613
iteration 653: loss 0.4207635521888733
iteration 654: loss 0.42027729749679565
iteration 655: loss 0.41979295015335083
iteration 656: loss 0.4193103015422821
iteration 657: loss 0.41882964968681335
iteration 658: loss 0.4183509647846222
iteration 659: loss 0.41787394881248474
iteration 660: loss 0.41739872097969055
iteration 661: loss 0.41692546010017395
iteration 662: loss 0.41645392775535583
iteration 663: loss 0.41598430275917053
iteration 664: loss 0.41551637649536133
iteration 665: loss 0.41505032777786255
iteration 666: loss 0.4145859181880951
iteration 667: loss 0.41412344574928284
iteration 668: loss 0.41366270184516907
iteration 669: loss 0.4132036566734314
iteration 670: loss 0.4127464294433594
iteration 671: loss 0.4122908115386963
iteration 672: loss 0.41183704137802124
iteration 673: loss 0.4113849103450775
iteration 674: loss 0.4109344482421875
iteration 675: loss 0.4104858338832855
iteration 676: loss 0.4100387692451477
iteration 677: loss 0.40959346294403076
iteration 678: loss 0.409149706363678
iteration 679: loss 0.40870755910873413
iteration 680: loss 0.40826722979545593
iteration 681: loss 0.4078283905982971
iteration 682: loss 0.4073912501335144
iteration 683: loss 0.4069558382034302
iteration 684: loss 0.40652185678482056
iteration 685: loss 0.4060894846916199
iteration 686: loss 0.4056588113307953
iteration 687: loss 0.4052295386791229
iteration 688: loss 0.4048018455505371
iteration 689: loss 0.40437591075897217
iteration 690: loss 0.40395137667655945
iteration 691: loss 0.40352845191955566
iteration 692: loss 0.4031069874763489
iteration 693: loss 0.402687132358551
iteration 694: loss 0.4022686779499054
iteration 695: loss 0.4018518328666687
iteration 696: loss 0.40143659710884094
iteration 697: loss 0.40102267265319824
iteration 698: loss 0.4006102979183197
iteration 699: loss 0.40019944310188293
iteration 700: loss 0.3997899889945984
iteration 701: loss 0.3993821144104004
iteration 702: loss 0.398975670337677
iteration 703: loss 0.39857059717178345
iteration 704: loss 0.39816707372665405
iteration 705: loss 0.3977649509906769
iteration 706: loss 0.39736422896385193
iteration 707: loss 0.3969649076461792
iteration 708: loss 0.3965669870376587
iteration 709: loss 0.3961705267429352
iteration 710: loss 0.3957754969596863
iteration 711: loss 0.3953818678855896
iteration 712: loss 0.3949896991252899
iteration 713: loss 0.39459872245788574
iteration 714: loss 0.39420923590660095
iteration 715: loss 0.3938210904598236
iteration 716: loss 0.3934342861175537
iteration 717: loss 0.39304882287979126
iteration 718: loss 0.3926647901535034
iteration 719: loss 0.39228200912475586
iteration 720: loss 0.3919006288051605
iteration 721: loss 0.3915204405784607
iteration 722: loss 0.3911418616771698
iteration 723: loss 0.39076438546180725
iteration 724: loss 0.3903883099555969
iteration 725: loss 0.3900134563446045
iteration 726: loss 0.38963988423347473
iteration 727: loss 0.3892676532268524
iteration 728: loss 0.3888966143131256
iteration 729: loss 0.388526976108551
iteration 730: loss 0.3881584703922272
iteration 731: loss 0.38779136538505554
iteration 732: loss 0.3874254822731018
iteration 733: loss 0.3870607316493988
iteration 734: loss 0.38669726252555847
iteration 735: loss 0.3863351047039032
iteration 736: loss 0.38597404956817627
iteration 737: loss 0.3856142461299896
iteration 738: loss 0.3852557837963104
iteration 739: loss 0.3848983347415924
iteration 740: loss 0.38454222679138184
iteration 741: loss 0.3841872811317444
iteration 742: loss 0.38383355736732483
iteration 743: loss 0.38348090648651123
iteration 744: loss 0.3831295073032379
iteration 745: loss 0.38277924060821533
iteration 746: loss 0.3824301064014435
iteration 747: loss 0.3820821940898895
iteration 748: loss 0.38173550367355347
iteration 749: loss 0.38138991594314575
iteration 750: loss 0.38104528188705444
iteration 751: loss 0.3807018995285034
iteration 752: loss 0.3803597390651703
iteration 753: loss 0.38001856207847595
iteration 754: loss 0.3796786665916443
iteration 755: loss 0.37933969497680664
iteration 756: loss 0.3790018558502197
iteration 757: loss 0.37866514921188354
iteration 758: loss 0.3783296048641205
iteration 759: loss 0.37799495458602905
iteration 760: loss 0.3776615858078003
iteration 761: loss 0.3773292303085327
iteration 762: loss 0.37699800729751587
iteration 763: loss 0.37666773796081543
iteration 764: loss 0.37633857131004333
iteration 765: loss 0.3760104775428772
iteration 766: loss 0.37568339705467224
iteration 767: loss 0.37535741925239563
iteration 768: loss 0.3750324249267578
iteration 769: loss 0.37470847368240356
iteration 770: loss 0.3743855059146881
iteration 771: loss 0.37406373023986816
iteration 772: loss 0.3737427592277527
iteration 773: loss 0.3734230101108551
iteration 774: loss 0.3731040954589844
iteration 775: loss 0.3727862536907196
iteration 776: loss 0.372469425201416
iteration 777: loss 0.37215346097946167
iteration 778: loss 0.3718385398387909
iteration 779: loss 0.3715246319770813
iteration 780: loss 0.3712117671966553
iteration 781: loss 0.37089982628822327
iteration 782: loss 0.37058886885643005
iteration 783: loss 0.37027886509895325
iteration 784: loss 0.36996981501579285
iteration 785: loss 0.36966174840927124
iteration 786: loss 0.36935460567474365
iteration 787: loss 0.3690485656261444
iteration 788: loss 0.36874327063560486
iteration 789: loss 0.36843904852867126
iteration 790: loss 0.3681356906890869
iteration 791: loss 0.3678332269191742
iteration 792: loss 0.3675316572189331
iteration 793: loss 0.3672310411930084
iteration 794: loss 0.36693140864372253
iteration 795: loss 0.3666326105594635
iteration 796: loss 0.3663347065448761
iteration 797: loss 0.3660375773906708
iteration 798: loss 0.36574143171310425
iteration 799: loss 0.3654460906982422
iteration 800: loss 0.36515167355537415
iteration 801: loss 0.36485815048217773
iteration 802: loss 0.3645654320716858
iteration 803: loss 0.36427363753318787
iteration 804: loss 0.36398258805274963
iteration 805: loss 0.36369258165359497
iteration 806: loss 0.36340323090553284
iteration 807: loss 0.3631148934364319
iteration 808: loss 0.3628273606300354
iteration 809: loss 0.3625405728816986
iteration 810: loss 0.3622547686100006
iteration 811: loss 0.3619697690010071
iteration 812: loss 0.361685574054718
iteration 813: loss 0.3614022135734558
iteration 814: loss 0.36111971735954285
iteration 815: loss 0.3608380854129791
iteration 816: loss 0.36055710911750793
iteration 817: loss 0.360276997089386
iteration 818: loss 0.3599976599216461
iteration 819: loss 0.3597191572189331
iteration 820: loss 0.35944145917892456
iteration 821: loss 0.35916459560394287
iteration 822: loss 0.35888856649398804
iteration 823: loss 0.35861319303512573
iteration 824: loss 0.3583388328552246
iteration 825: loss 0.35806506872177124
iteration 826: loss 0.3577921986579895
iteration 827: loss 0.35752004384994507
iteration 828: loss 0.3572487235069275
iteration 829: loss 0.356978178024292
iteration 830: loss 0.3567083179950714
iteration 831: loss 0.3564392328262329
iteration 832: loss 0.35617098212242126
iteration 833: loss 0.3559034466743469
iteration 834: loss 0.3556366562843323
iteration 835: loss 0.35537058115005493
iteration 836: loss 0.35510510206222534
iteration 837: loss 0.35484057664871216
iteration 838: loss 0.3545767366886139
iteration 839: loss 0.35431355237960815
iteration 840: loss 0.3540510833263397
iteration 841: loss 0.353789359331131
iteration 842: loss 0.35352835059165955
iteration 843: loss 0.35326799750328064
iteration 844: loss 0.35300832986831665
iteration 845: loss 0.35274937748908997
iteration 846: loss 0.3524912893772125
iteration 847: loss 0.35223373770713806
iteration 848: loss 0.35197702050209045
iteration 849: loss 0.351720929145813
iteration 850: loss 0.3514655828475952
iteration 851: loss 0.3512107729911804
iteration 852: loss 0.35095688700675964
iteration 853: loss 0.350703626871109
iteration 854: loss 0.35045096278190613
iteration 855: loss 0.35019901394844055
iteration 856: loss 0.3499477505683899
iteration 857: loss 0.3496972322463989
iteration 858: loss 0.3494473397731781
iteration 859: loss 0.34919804334640503
iteration 860: loss 0.34894946217536926
iteration 861: loss 0.348701536655426
iteration 862: loss 0.3484542667865753
iteration 863: loss 0.3482077121734619
iteration 864: loss 0.34796178340911865
iteration 865: loss 0.34771648049354553
iteration 866: loss 0.34747186303138733
iteration 867: loss 0.34722796082496643
iteration 868: loss 0.3469845652580261
iteration 869: loss 0.34674185514450073
iteration 870: loss 0.34649986028671265
iteration 871: loss 0.346258282661438
iteration 872: loss 0.3460175693035126
iteration 873: loss 0.34577736258506775
iteration 874: loss 0.3455376923084259
iteration 875: loss 0.34529873728752136
iteration 876: loss 0.3450603783130646
iteration 877: loss 0.34482255578041077
iteration 878: loss 0.3445853292942047
iteration 879: loss 0.3443487286567688
iteration 880: loss 0.3441126048564911
iteration 881: loss 0.34387722611427307
iteration 882: loss 0.34364238381385803
iteration 883: loss 0.34340810775756836
iteration 884: loss 0.34317442774772644
iteration 885: loss 0.34294140338897705
iteration 886: loss 0.34270891547203064
iteration 887: loss 0.34247687458992004
iteration 888: loss 0.34224534034729004
iteration 889: loss 0.3420146107673645
iteration 890: loss 0.34178435802459717
iteration 891: loss 0.3415547013282776
iteration 892: loss 0.3413254916667938
iteration 893: loss 0.3410969376564026
iteration 894: loss 0.3408689498901367
iteration 895: loss 0.34064146876335144
iteration 896: loss 0.3404145836830139
iteration 897: loss 0.34018829464912415
iteration 898: loss 0.3399624824523926
iteration 899: loss 0.33973726630210876
iteration 900: loss 0.33951258659362793
iteration 901: loss 0.3392884433269501
iteration 902: loss 0.33906492590904236
iteration 903: loss 0.3388419449329376
iteration 904: loss 0.3386193811893463
iteration 905: loss 0.33839747309684753
iteration 906: loss 0.3381759822368622
iteration 907: loss 0.3379550874233246
iteration 908: loss 0.33773475885391235
iteration 909: loss 0.3375149369239807
iteration 910: loss 0.3372955322265625
iteration 911: loss 0.3370767831802368
iteration 912: loss 0.3368584215641022
iteration 913: loss 0.3366406559944153
iteration 914: loss 0.3364233672618866
iteration 915: loss 0.3362066149711609
iteration 916: loss 0.33599036931991577
iteration 917: loss 0.33577466011047363
iteration 918: loss 0.3355594277381897
iteration 919: loss 0.3353446125984192
iteration 920: loss 0.33513039350509644
iteration 921: loss 0.3349166214466095
iteration 922: loss 0.3347034156322479
iteration 923: loss 0.33449071645736694
iteration 924: loss 0.3342784643173218
iteration 925: loss 0.3340667486190796
iteration 926: loss 0.33385545015335083
iteration 927: loss 0.33364468812942505
iteration 928: loss 0.33343440294265747
iteration 929: loss 0.3332246243953705
iteration 930: loss 0.3330152928829193
iteration 931: loss 0.33280646800994873
iteration 932: loss 0.33259811997413635
iteration 933: loss 0.3323902487754822
iteration 934: loss 0.33218279480934143
iteration 935: loss 0.33197587728500366
iteration 936: loss 0.3317694067955017
iteration 937: loss 0.33156344294548035
iteration 938: loss 0.3313578963279724
iteration 939: loss 0.3311527371406555
iteration 940: loss 0.3309480845928192
iteration 941: loss 0.3307437598705292
iteration 942: loss 0.3305399715900421
iteration 943: loss 0.33033666014671326
iteration 944: loss 0.33013373613357544
iteration 945: loss 0.3299313187599182
iteration 946: loss 0.32972919940948486
iteration 947: loss 0.3295276165008545
iteration 948: loss 0.32932648062705994
iteration 949: loss 0.3291257619857788
iteration 950: loss 0.3289254307746887
iteration 951: loss 0.3287256360054016
iteration 952: loss 0.3285261392593384
iteration 953: loss 0.3283270597457886
iteration 954: loss 0.32812854647636414
iteration 955: loss 0.32793039083480835
iteration 956: loss 0.32773253321647644
iteration 957: loss 0.3275353014469147
iteration 958: loss 0.327338308095932
iteration 959: loss 0.32714176177978516
iteration 960: loss 0.3269456923007965
iteration 961: loss 0.3267499506473541
iteration 962: loss 0.32655465602874756
iteration 963: loss 0.32635971903800964
iteration 964: loss 0.32616516947746277
iteration 965: loss 0.32597097754478455
iteration 966: loss 0.3257773220539093
iteration 967: loss 0.3255838453769684
iteration 968: loss 0.32539084553718567
iteration 969: loss 0.32519835233688354
iteration 970: loss 0.32500624656677246
iteration 971: loss 0.3248143792152405
iteration 972: loss 0.32462310791015625
iteration 973: loss 0.3244321346282959
iteration 974: loss 0.3242415189743042
iteration 975: loss 0.3240513503551483
iteration 976: loss 0.3238615393638611
iteration 977: loss 0.3236720860004425
iteration 978: loss 0.3234829902648926
iteration 979: loss 0.3232942819595337
iteration 980: loss 0.32310596108436584
iteration 981: loss 0.3229178488254547
iteration 982: loss 0.3227303624153137
iteration 983: loss 0.32254308462142944
iteration 984: loss 0.32235631346702576
iteration 985: loss 0.3221697509288788
iteration 986: loss 0.3219836354255676
iteration 987: loss 0.3217979669570923
iteration 988: loss 0.32161250710487366
iteration 989: loss 0.32142749428749084
iteration 990: loss 0.32124271988868713
iteration 991: loss 0.3210584819316864
iteration 992: loss 0.3208744525909424
iteration 993: loss 0.3206908106803894
iteration 994: loss 0.32050755620002747
iteration 995: loss 0.32032454013824463
iteration 996: loss 0.3201419711112976
iteration 997: loss 0.31995970010757446
iteration 998: loss 0.3197776973247528
iteration 999: loss 0.31959614157676697
Accuracy:  0.9165 
Recall:  [(0.9755102040816327, 0.9345063538611925), (0.9823788546255506, 0.9628670120898101), (0.8827519379844961, 0.9295918367346939), (0.905940594059406, 0.8997050147492626), (0.9256619144602851, 0.9053784860557769), (0.8609865470852018, 0.9164677804295943), (0.9446764091858038, 0.9310699588477366), (0.9027237354085603, 0.9224652087475149), (0.8850102669404517, 0.8707070707070707), (0.88800792864222, 0.8853754940711462)] 
Matrix:
 [[ 956    0    3    3    0    6    7    2    3    0]
 [   0 1115    2    2    0    1    4    1   10    0]
 [  13    4  911   22   14    0   14   11   39    4]
 [   2    2   18  915    1   21    3   16   25    7]
 [   2    4    4    1  909    1   13    1    7   40]
 [  11    3    2   33   12  768   15    7   34    7]
 [  17    3    5    0    9   13  905    2    4    0]
 [   3   13   22    5   10    0    0  928    1   46]
 [   7    7   10   23    9   22   10   12  862   12]
 [  12    7    3   13   40    6    1   26    5  896]]
iteration 0: loss 2.3019626140594482
iteration 1: loss 2.300902843475342
iteration 2: loss 2.2998414039611816
iteration 3: loss 2.298772096633911
iteration 4: loss 2.2976884841918945
iteration 5: loss 2.2965826988220215
iteration 6: loss 2.295450210571289
iteration 7: loss 2.294285297393799
iteration 8: loss 2.2930855751037598
iteration 9: loss 2.2918455600738525
iteration 10: loss 2.2905638217926025
iteration 11: loss 2.289238691329956
iteration 12: loss 2.2878665924072266
iteration 13: loss 2.286449432373047
iteration 14: loss 2.2849841117858887
iteration 15: loss 2.2834694385528564
iteration 16: loss 2.281907320022583
iteration 17: loss 2.280296802520752
iteration 18: loss 2.2786364555358887
iteration 19: loss 2.2769248485565186
iteration 20: loss 2.2751624584198
iteration 21: loss 2.2733473777770996
iteration 22: loss 2.2714786529541016
iteration 23: loss 2.269556760787964
iteration 24: loss 2.2675797939300537
iteration 25: loss 2.265547752380371
iteration 26: loss 2.263461112976074
iteration 27: loss 2.2613179683685303
iteration 28: loss 2.259119749069214
iteration 29: loss 2.2568650245666504
iteration 30: loss 2.2545535564422607
iteration 31: loss 2.252185821533203
iteration 32: loss 2.249760866165161
iteration 33: loss 2.247279167175293
iteration 34: loss 2.2447400093078613
iteration 35: loss 2.2421436309814453
iteration 36: loss 2.2394909858703613
iteration 37: loss 2.2367806434631348
iteration 38: loss 2.2340126037597656
iteration 39: loss 2.2311854362487793
iteration 40: loss 2.228299617767334
iteration 41: loss 2.2253549098968506
iteration 42: loss 2.22235107421875
iteration 43: loss 2.219287633895874
iteration 44: loss 2.216163158416748
iteration 45: loss 2.212979555130005
iteration 46: loss 2.209733486175537
iteration 47: loss 2.206428289413452
iteration 48: loss 2.203059434890747
iteration 49: loss 2.19963002204895
iteration 50: loss 2.1961376667022705
iteration 51: loss 2.19258451461792
iteration 52: loss 2.1889681816101074
iteration 53: loss 2.1852896213531494
iteration 54: loss 2.181548595428467
iteration 55: loss 2.177745819091797
iteration 56: loss 2.1738803386688232
iteration 57: loss 2.169952392578125
iteration 58: loss 2.1659631729125977
iteration 59: loss 2.1619114875793457
iteration 60: loss 2.157799005508423
iteration 61: loss 2.1536240577697754
iteration 62: loss 2.149388313293457
iteration 63: loss 2.145092010498047
iteration 64: loss 2.140735387802124
iteration 65: loss 2.1363189220428467
iteration 66: loss 2.131843090057373
iteration 67: loss 2.1273086071014404
iteration 68: loss 2.1227145195007324
iteration 69: loss 2.1180622577667236
iteration 70: loss 2.113351583480835
iteration 71: loss 2.1085832118988037
iteration 72: loss 2.1037561893463135
iteration 73: loss 2.0988731384277344
iteration 74: loss 2.093933343887329
iteration 75: loss 2.088937282562256
iteration 76: loss 2.08388614654541
iteration 77: loss 2.0787806510925293
iteration 78: loss 2.0736207962036133
iteration 79: loss 2.0684077739715576
iteration 80: loss 2.0631420612335205
iteration 81: loss 2.0578243732452393
iteration 82: loss 2.052455186843872
iteration 83: loss 2.0470356941223145
iteration 84: loss 2.0415658950805664
iteration 85: loss 2.0360469818115234
iteration 86: loss 2.0304794311523438
iteration 87: loss 2.024864435195923
iteration 88: loss 2.019202709197998
iteration 89: loss 2.0134947299957275
iteration 90: loss 2.007741689682007
iteration 91: loss 2.001944065093994
iteration 92: loss 1.9961044788360596
iteration 93: loss 1.9902212619781494
iteration 94: loss 1.9842970371246338
iteration 95: loss 1.9783321619033813
iteration 96: loss 1.9723275899887085
iteration 97: loss 1.9662840366363525
iteration 98: loss 1.9602031707763672
iteration 99: loss 1.9540855884552002
iteration 100: loss 1.9479321241378784
iteration 101: loss 1.9417442083358765
iteration 102: loss 1.9355225563049316
iteration 103: loss 1.9292680025100708
iteration 104: loss 1.9229817390441895
iteration 105: loss 1.916664958000183
iteration 106: loss 1.9103182554244995
iteration 107: loss 1.9039435386657715
iteration 108: loss 1.8975410461425781
iteration 109: loss 1.8911116123199463
iteration 110: loss 1.8846569061279297
iteration 111: loss 1.8781776428222656
iteration 112: loss 1.8716756105422974
iteration 113: loss 1.8651504516601562
iteration 114: loss 1.858604073524475
iteration 115: loss 1.8520371913909912
iteration 116: loss 1.8454513549804688
iteration 117: loss 1.838847041130066
iteration 118: loss 1.8322255611419678
iteration 119: loss 1.825587511062622
iteration 120: loss 1.8189343214035034
iteration 121: loss 1.8122667074203491
iteration 122: loss 1.8055864572525024
iteration 123: loss 1.7988940477371216
iteration 124: loss 1.7921911478042603
iteration 125: loss 1.7854779958724976
iteration 126: loss 1.7787563800811768
iteration 127: loss 1.772026777267456
iteration 128: loss 1.7652907371520996
iteration 129: loss 1.7585492134094238
iteration 130: loss 1.751802921295166
iteration 131: loss 1.745052695274353
iteration 132: loss 1.7383003234863281
iteration 133: loss 1.7315455675125122
iteration 134: loss 1.7247905731201172
iteration 135: loss 1.7180355787277222
iteration 136: loss 1.7112808227539062
iteration 137: loss 1.7045284509658813
iteration 138: loss 1.6977788209915161
iteration 139: loss 1.6910322904586792
iteration 140: loss 1.684289574623108
iteration 141: loss 1.6775527000427246
iteration 142: loss 1.670821189880371
iteration 143: loss 1.6640963554382324
iteration 144: loss 1.6573781967163086
iteration 145: loss 1.6506692171096802
iteration 146: loss 1.6439682245254517
iteration 147: loss 1.6372768878936768
iteration 148: loss 1.6305958032608032
iteration 149: loss 1.623925805091858
iteration 150: loss 1.6172670125961304
iteration 151: loss 1.6106207370758057
iteration 152: loss 1.6039875745773315
iteration 153: loss 1.597367286682129
iteration 154: loss 1.5907611846923828
iteration 155: loss 1.5841703414916992
iteration 156: loss 1.577594518661499
iteration 157: loss 1.5710340738296509
iteration 158: loss 1.5644906759262085
iteration 159: loss 1.5579642057418823
iteration 160: loss 1.551455020904541
iteration 161: loss 1.544964075088501
iteration 162: loss 1.5384914875030518
iteration 163: loss 1.5320382118225098
iteration 164: loss 1.525604009628296
iteration 165: loss 1.5191898345947266
iteration 166: loss 1.51279616355896
iteration 167: loss 1.5064234733581543
iteration 168: loss 1.5000720024108887
iteration 169: loss 1.4937421083450317
iteration 170: loss 1.4874340295791626
iteration 171: loss 1.481148600578308
iteration 172: loss 1.4748859405517578
iteration 173: loss 1.4686462879180908
iteration 174: loss 1.4624302387237549
iteration 175: loss 1.456238031387329
iteration 176: loss 1.4500701427459717
iteration 177: loss 1.4439265727996826
iteration 178: loss 1.4378076791763306
iteration 179: loss 1.4317137002944946
iteration 180: loss 1.4256449937820435
iteration 181: loss 1.4196021556854248
iteration 182: loss 1.413584589958191
iteration 183: loss 1.4075932502746582
iteration 184: loss 1.4016281366348267
iteration 185: loss 1.395689606666565
iteration 186: loss 1.389777421951294
iteration 187: loss 1.3838921785354614
iteration 188: loss 1.378033995628357
iteration 189: loss 1.3722032308578491
iteration 190: loss 1.3663995265960693
iteration 191: loss 1.3606231212615967
iteration 192: loss 1.3548749685287476
iteration 193: loss 1.3491538763046265
iteration 194: loss 1.343461036682129
iteration 195: loss 1.3377958536148071
iteration 196: loss 1.3321590423583984
iteration 197: loss 1.3265504837036133
iteration 198: loss 1.3209705352783203
iteration 199: loss 1.3154186010360718
iteration 200: loss 1.3098950386047363
iteration 201: loss 1.304400086402893
iteration 202: loss 1.2989333868026733
iteration 203: loss 1.293495774269104
iteration 204: loss 1.2880865335464478
iteration 205: loss 1.2827064990997314
iteration 206: loss 1.2773545980453491
iteration 207: loss 1.2720319032669067
iteration 208: loss 1.2667373418807983
iteration 209: loss 1.2614721059799194
iteration 210: loss 1.2562352418899536
iteration 211: loss 1.251027226448059
iteration 212: loss 1.2458484172821045
iteration 213: loss 1.2406980991363525
iteration 214: loss 1.2355760335922241
iteration 215: loss 1.2304829359054565
iteration 216: loss 1.2254186868667603
iteration 217: loss 1.2203831672668457
iteration 218: loss 1.215375542640686
iteration 219: loss 1.210397720336914
iteration 220: loss 1.2054475545883179
iteration 221: loss 1.2005261182785034
iteration 222: loss 1.1956331729888916
iteration 223: loss 1.1907687187194824
iteration 224: loss 1.1859322786331177
iteration 225: loss 1.181123971939087
iteration 226: loss 1.1763441562652588
iteration 227: loss 1.1715925931930542
iteration 228: loss 1.1668686866760254
iteration 229: loss 1.1621726751327515
iteration 230: loss 1.1575044393539429
iteration 231: loss 1.1528644561767578
iteration 232: loss 1.148252010345459
iteration 233: loss 1.1436671018600464
iteration 234: loss 1.139109492301941
iteration 235: loss 1.134580135345459
iteration 236: loss 1.1300770044326782
iteration 237: loss 1.1256017684936523
iteration 238: loss 1.1211533546447754
iteration 239: loss 1.116731882095337
iteration 240: loss 1.1123374700546265
iteration 241: loss 1.107970118522644
iteration 242: loss 1.1036289930343628
iteration 243: loss 1.09931480884552
iteration 244: loss 1.0950270891189575
iteration 245: loss 1.0907654762268066
iteration 246: loss 1.086530327796936
iteration 247: loss 1.082321286201477
iteration 248: loss 1.0781382322311401
iteration 249: loss 1.0739810466766357
iteration 250: loss 1.0698494911193848
iteration 251: loss 1.0657440423965454
iteration 252: loss 1.0616638660430908
iteration 253: loss 1.057608962059021
iteration 254: loss 1.0535796880722046
iteration 255: loss 1.0495754480361938
iteration 256: loss 1.0455962419509888
iteration 257: loss 1.0416419506072998
iteration 258: loss 1.0377124547958374
iteration 259: loss 1.033807635307312
iteration 260: loss 1.0299272537231445
iteration 261: loss 1.0260714292526245
iteration 262: loss 1.0222399234771729
iteration 263: loss 1.018432378768921
iteration 264: loss 1.0146490335464478
iteration 265: loss 1.0108895301818848
iteration 266: loss 1.0071537494659424
iteration 267: loss 1.0034416913986206
iteration 268: loss 0.9997531771659851
iteration 269: loss 0.9960877299308777
iteration 270: loss 0.9924456477165222
iteration 271: loss 0.9888268113136292
iteration 272: loss 0.9852306842803955
iteration 273: loss 0.9816575646400452
iteration 274: loss 0.9781069755554199
iteration 275: loss 0.9745790958404541
iteration 276: loss 0.9710736274719238
iteration 277: loss 0.9675904512405396
iteration 278: loss 0.9641293883323669
iteration 279: loss 0.960690438747406
iteration 280: loss 0.9572734236717224
iteration 281: loss 0.9538779854774475
iteration 282: loss 0.9505042433738708
iteration 283: loss 0.9471521973609924
iteration 284: loss 0.9438214898109436
iteration 285: loss 0.9405117630958557
iteration 286: loss 0.9372232556343079
iteration 287: loss 0.9339559078216553
iteration 288: loss 0.9307093024253845
iteration 289: loss 0.9274832606315613
iteration 290: loss 0.9242780208587646
iteration 291: loss 0.9210929870605469
iteration 292: loss 0.9179285764694214
iteration 293: loss 0.9147841334342957
iteration 294: loss 0.9116598963737488
iteration 295: loss 0.9085555076599121
iteration 296: loss 0.9054709076881409
iteration 297: loss 0.9024060368537903
iteration 298: loss 0.8993604779243469
iteration 299: loss 0.8963345885276794
iteration 300: loss 0.8933276534080505
iteration 301: loss 0.8903403282165527
iteration 302: loss 0.8873716592788696
iteration 303: loss 0.8844219446182251
iteration 304: loss 0.8814908862113953
iteration 305: loss 0.8785789012908936
iteration 306: loss 0.8756850957870483
iteration 307: loss 0.8728097677230835
iteration 308: loss 0.8699524998664856
iteration 309: loss 0.8671136498451233
iteration 310: loss 0.864292562007904
iteration 311: loss 0.8614895939826965
iteration 312: loss 0.8587043285369873
iteration 313: loss 0.8559365272521973
iteration 314: loss 0.8531865477561951
iteration 315: loss 0.850453794002533
iteration 316: loss 0.8477383255958557
iteration 317: loss 0.8450402617454529
iteration 318: loss 0.8423590064048767
iteration 319: loss 0.8396948575973511
iteration 320: loss 0.8370471596717834
iteration 321: loss 0.8344164490699768
iteration 322: loss 0.831802248954773
iteration 323: loss 0.8292043209075928
iteration 324: loss 0.826622724533081
iteration 325: loss 0.8240575194358826
iteration 326: loss 0.8215083479881287
iteration 327: loss 0.8189750909805298
iteration 328: loss 0.8164575695991516
iteration 329: loss 0.8139559626579285
iteration 330: loss 0.8114702105522156
iteration 331: loss 0.8089996576309204
iteration 332: loss 0.8065447211265564
iteration 333: loss 0.8041050434112549
iteration 334: loss 0.8016806244850159
iteration 335: loss 0.7992711067199707
iteration 336: loss 0.7968767881393433
iteration 337: loss 0.7944972515106201
iteration 338: loss 0.7921324372291565
iteration 339: loss 0.7897822260856628
iteration 340: loss 0.7874467372894287
iteration 341: loss 0.7851256132125854
iteration 342: loss 0.7828189730644226
iteration 343: loss 0.7805262207984924
iteration 344: loss 0.7782479524612427
iteration 345: loss 0.7759835720062256
iteration 346: loss 0.7737331986427307
iteration 347: loss 0.7714964151382446
iteration 348: loss 0.7692737579345703
iteration 349: loss 0.7670643925666809
iteration 350: loss 0.7648687362670898
iteration 351: loss 0.7626863121986389
iteration 352: loss 0.7605174779891968
iteration 353: loss 0.7583617568016052
iteration 354: loss 0.756219208240509
iteration 355: loss 0.7540896534919739
iteration 356: loss 0.7519730925559998
iteration 357: loss 0.7498693466186523
iteration 358: loss 0.7477784156799316
iteration 359: loss 0.7457002401351929
iteration 360: loss 0.7436344027519226
iteration 361: loss 0.7415812015533447
iteration 362: loss 0.7395402789115906
iteration 363: loss 0.7375120520591736
iteration 364: loss 0.7354956269264221
iteration 365: loss 0.7334915399551392
iteration 366: loss 0.7314993143081665
iteration 367: loss 0.7295191287994385
iteration 368: loss 0.7275506854057312
iteration 369: loss 0.7255940437316895
iteration 370: loss 0.7236493229866028
iteration 371: loss 0.7217158079147339
iteration 372: loss 0.7197942137718201
iteration 373: loss 0.7178837656974792
iteration 374: loss 0.7159847021102905
iteration 375: loss 0.7140969038009644
iteration 376: loss 0.7122204303741455
iteration 377: loss 0.7103549838066101
iteration 378: loss 0.7085005044937134
iteration 379: loss 0.7066571116447449
iteration 380: loss 0.7048245668411255
iteration 381: loss 0.7030025720596313
iteration 382: loss 0.7011916041374207
iteration 383: loss 0.6993911266326904
iteration 384: loss 0.6976010799407959
iteration 385: loss 0.6958214640617371
iteration 386: loss 0.6940523386001587
iteration 387: loss 0.692293643951416
iteration 388: loss 0.6905449628829956
iteration 389: loss 0.6888065934181213
iteration 390: loss 0.6870781779289246
iteration 391: loss 0.6853598356246948
iteration 392: loss 0.6836514472961426
iteration 393: loss 0.6819530129432678
iteration 394: loss 0.6802643537521362
iteration 395: loss 0.6785855889320374
iteration 396: loss 0.6769160032272339
iteration 397: loss 0.6752563714981079
iteration 398: loss 0.6736062169075012
iteration 399: loss 0.6719654202461243
iteration 400: loss 0.6703340411186218
iteration 401: loss 0.6687118411064148
iteration 402: loss 0.6670989394187927
iteration 403: loss 0.6654952168464661
iteration 404: loss 0.6639006733894348
iteration 405: loss 0.6623151302337646
iteration 406: loss 0.6607385873794556
iteration 407: loss 0.659170925617218
iteration 408: loss 0.6576122045516968
iteration 409: loss 0.6560621857643127
iteration 410: loss 0.6545209884643555
iteration 411: loss 0.6529883146286011
iteration 412: loss 0.6514644622802734
iteration 413: loss 0.64994877576828
iteration 414: loss 0.6484418511390686
iteration 415: loss 0.6469432711601257
iteration 416: loss 0.6454529166221619
iteration 417: loss 0.6439712047576904
iteration 418: loss 0.6424973011016846
iteration 419: loss 0.6410319209098816
iteration 420: loss 0.639574408531189
iteration 421: loss 0.6381251811981201
iteration 422: loss 0.6366839408874512
iteration 423: loss 0.6352506279945374
iteration 424: loss 0.6338251829147339
iteration 425: loss 0.6324074268341064
iteration 426: loss 0.6309974789619446
iteration 427: loss 0.6295953392982483
iteration 428: loss 0.6282009482383728
iteration 429: loss 0.6268140077590942
iteration 430: loss 0.6254346966743469
iteration 431: loss 0.6240628361701965
iteration 432: loss 0.6226986050605774
iteration 433: loss 0.6213417053222656
iteration 434: loss 0.6199919581413269
iteration 435: loss 0.6186497211456299
iteration 436: loss 0.6173144578933716
iteration 437: loss 0.6159865856170654
iteration 438: loss 0.6146659255027771
iteration 439: loss 0.6133522987365723
iteration 440: loss 0.6120456457138062
iteration 441: loss 0.6107459664344788
iteration 442: loss 0.6094532012939453
iteration 443: loss 0.6081674098968506
iteration 444: loss 0.6068884134292603
iteration 445: loss 0.6056161522865295
iteration 446: loss 0.6043508648872375
iteration 447: loss 0.6030921339988708
iteration 448: loss 0.6018400192260742
iteration 449: loss 0.600594699382782
iteration 450: loss 0.5993556380271912
iteration 451: loss 0.5981231927871704
iteration 452: loss 0.5968971252441406
iteration 453: loss 0.5956776738166809
iteration 454: loss 0.5944644808769226
iteration 455: loss 0.5932576656341553
iteration 456: loss 0.5920570492744446
iteration 457: loss 0.5908628702163696
iteration 458: loss 0.589674711227417
iteration 459: loss 0.5884928107261658
iteration 460: loss 0.587317168712616
iteration 461: loss 0.5861472487449646
iteration 462: loss 0.5849835872650146
iteration 463: loss 0.5838258862495422
iteration 464: loss 0.5826741456985474
iteration 465: loss 0.5815283060073853
iteration 466: loss 0.5803883075714111
iteration 467: loss 0.5792542695999146
iteration 468: loss 0.5781258940696716
iteration 469: loss 0.5770030617713928
iteration 470: loss 0.5758861899375916
iteration 471: loss 0.5747750401496887
iteration 472: loss 0.57366943359375
iteration 473: loss 0.5725693106651306
iteration 474: loss 0.571475088596344
iteration 475: loss 0.5703859329223633
iteration 476: loss 0.569302499294281
iteration 477: loss 0.5682246088981628
iteration 478: loss 0.5671517848968506
iteration 479: loss 0.5660846829414368
iteration 480: loss 0.5650227069854736
iteration 481: loss 0.5639662146568298
iteration 482: loss 0.5629147887229919
iteration 483: loss 0.5618686676025391
iteration 484: loss 0.5608277916908264
iteration 485: loss 0.5597919821739197
iteration 486: loss 0.5587614178657532
iteration 487: loss 0.5577358603477478
iteration 488: loss 0.5567155480384827
iteration 489: loss 0.5557000637054443
iteration 490: loss 0.5546895265579224
iteration 491: loss 0.5536841750144958
iteration 492: loss 0.5526837110519409
iteration 493: loss 0.5516880750656128
iteration 494: loss 0.5506972074508667
iteration 495: loss 0.5497112274169922
iteration 496: loss 0.5487300157546997
iteration 497: loss 0.5477536916732788
iteration 498: loss 0.5467820167541504
iteration 499: loss 0.5458152294158936
iteration 500: loss 0.5448528528213501
iteration 501: loss 0.5438950061798096
iteration 502: loss 0.5429421067237854
iteration 503: loss 0.5419935584068298
iteration 504: loss 0.5410497784614563
iteration 505: loss 0.5401102900505066
iteration 506: loss 0.5391753911972046
iteration 507: loss 0.5382449626922607
iteration 508: loss 0.5373189449310303
iteration 509: loss 0.5363972783088684
iteration 510: loss 0.5354800224304199
iteration 511: loss 0.5345671772956848
iteration 512: loss 0.533658504486084
iteration 513: loss 0.5327541828155518
iteration 514: loss 0.5318542122840881
iteration 515: loss 0.5309584736824036
iteration 516: loss 0.5300668478012085
iteration 517: loss 0.5291794538497925
iteration 518: loss 0.528296172618866
iteration 519: loss 0.5274171233177185
iteration 520: loss 0.5265421271324158
iteration 521: loss 0.5256712436676025
iteration 522: loss 0.5248043537139893
iteration 523: loss 0.5239415168762207
iteration 524: loss 0.5230827331542969
iteration 525: loss 0.5222278237342834
iteration 526: loss 0.52137690782547
iteration 527: loss 0.5205299258232117
iteration 528: loss 0.5196868777275085
iteration 529: loss 0.5188475251197815
iteration 530: loss 0.5180123448371887
iteration 531: loss 0.5171808004379272
iteration 532: loss 0.5163530707359314
iteration 533: loss 0.5155290365219116
iteration 534: loss 0.514708936214447
iteration 535: loss 0.513892412185669
iteration 536: loss 0.5130797028541565
iteration 537: loss 0.5122706890106201
iteration 538: loss 0.5114652514457703
iteration 539: loss 0.5106635689735413
iteration 540: loss 0.5098655223846436
iteration 541: loss 0.5090708136558533
iteration 542: loss 0.5082799196243286
iteration 543: loss 0.5074925422668457
iteration 544: loss 0.5067088007926941
iteration 545: loss 0.5059284567832947
iteration 546: loss 0.5051516890525818
iteration 547: loss 0.5043782591819763
iteration 548: loss 0.5036083459854126
iteration 549: loss 0.5028420090675354
iteration 550: loss 0.5020788908004761
iteration 551: loss 0.5013193488121033
iteration 552: loss 0.5005631446838379
iteration 553: loss 0.49981018900871277
iteration 554: loss 0.49906066060066223
iteration 555: loss 0.4983142912387848
iteration 556: loss 0.4975713789463043
iteration 557: loss 0.4968315660953522
iteration 558: loss 0.4960951507091522
iteration 559: loss 0.49536192417144775
iteration 560: loss 0.494631826877594
iteration 561: loss 0.49390503764152527
iteration 562: loss 0.4931812882423401
iteration 563: loss 0.49246078729629517
iteration 564: loss 0.4917435050010681
iteration 565: loss 0.49102917313575745
iteration 566: loss 0.49031808972358704
iteration 567: loss 0.489609956741333
iteration 568: loss 0.4889049530029297
iteration 569: loss 0.48820289969444275
iteration 570: loss 0.4875039756298065
iteration 571: loss 0.48680806159973145
iteration 572: loss 0.486115038394928
iteration 573: loss 0.48542505502700806
iteration 574: loss 0.48473796248435974
iteration 575: loss 0.4840538799762726
iteration 576: loss 0.48337265849113464
iteration 577: loss 0.4826944172382355
iteration 578: loss 0.4820190966129303
iteration 579: loss 0.48134645819664
iteration 580: loss 0.4806768298149109
iteration 581: loss 0.4800100028514862
iteration 582: loss 0.47934606671333313
iteration 583: loss 0.47868481278419495
iteration 584: loss 0.47802630066871643
iteration 585: loss 0.47737058997154236
iteration 586: loss 0.4767177104949951
iteration 587: loss 0.47606754302978516
iteration 588: loss 0.475420206785202
iteration 589: loss 0.4747753143310547
iteration 590: loss 0.47413328289985657
iteration 591: loss 0.47349390387535095
iteration 592: loss 0.4728572368621826
iteration 593: loss 0.4722232520580292
iteration 594: loss 0.4715917706489563
iteration 595: loss 0.47096285223960876
iteration 596: loss 0.4703367352485657
iteration 597: loss 0.46971312165260315
iteration 598: loss 0.4690920114517212
iteration 599: loss 0.46847355365753174
iteration 600: loss 0.46785756945610046
iteration 601: loss 0.4672442674636841
iteration 602: loss 0.46663331985473633
iteration 603: loss 0.46602490544319153
iteration 604: loss 0.4654189944267273
iteration 605: loss 0.46481552720069885
iteration 606: loss 0.46421465277671814
iteration 607: loss 0.4636160731315613
iteration 608: loss 0.46301987767219543
iteration 609: loss 0.46242621541023254
iteration 610: loss 0.46183496713638306
iteration 611: loss 0.4612461030483246
iteration 612: loss 0.46065956354141235
iteration 613: loss 0.46007540822029114
iteration 614: loss 0.4594936668872833
iteration 615: loss 0.4589141607284546
iteration 616: loss 0.4583369791507721
iteration 617: loss 0.457762211561203
iteration 618: loss 0.4571895897388458
iteration 619: loss 0.45661935210227966
iteration 620: loss 0.45605140924453735
iteration 621: loss 0.4554857313632965
iteration 622: loss 0.45492228865623474
iteration 623: loss 0.45436108112335205
iteration 624: loss 0.4538021683692932
iteration 625: loss 0.45324528217315674
iteration 626: loss 0.4526907205581665
iteration 627: loss 0.45213842391967773
iteration 628: loss 0.45158806443214417
iteration 629: loss 0.4510400891304016
iteration 630: loss 0.45049411058425903
iteration 631: loss 0.44995027780532837
iteration 632: loss 0.44940871000289917
iteration 633: loss 0.44886913895606995
iteration 634: loss 0.4483317732810974
iteration 635: loss 0.44779640436172485
iteration 636: loss 0.4472631812095642
iteration 637: loss 0.4467320442199707
iteration 638: loss 0.44620293378829956
iteration 639: loss 0.4456758499145508
iteration 640: loss 0.44515079259872437
iteration 641: loss 0.4446277916431427
iteration 642: loss 0.4441067576408386
iteration 643: loss 0.4435878098011017
iteration 644: loss 0.4430709481239319
iteration 645: loss 0.44255584478378296
iteration 646: loss 0.44204285740852356
iteration 647: loss 0.441531777381897
iteration 648: loss 0.441022664308548
iteration 649: loss 0.44051551818847656
iteration 650: loss 0.4400103986263275
iteration 651: loss 0.4395069479942322
iteration 652: loss 0.4390057325363159
iteration 653: loss 0.43850621581077576
iteration 654: loss 0.43800872564315796
iteration 655: loss 0.43751299381256104
iteration 656: loss 0.4370191991329193
iteration 657: loss 0.4365273118019104
iteration 658: loss 0.4360370934009552
iteration 659: loss 0.43554896116256714
iteration 660: loss 0.4350624084472656
iteration 661: loss 0.4345778524875641
iteration 662: loss 0.4340951144695282
iteration 663: loss 0.4336141347885132
iteration 664: loss 0.4331349730491638
iteration 665: loss 0.43265751004219055
iteration 666: loss 0.43218180537223816
iteration 667: loss 0.4317080080509186
iteration 668: loss 0.4312358796596527
iteration 669: loss 0.4307655096054077
iteration 670: loss 0.43029698729515076
iteration 671: loss 0.42983004450798035
iteration 672: loss 0.42936477065086365
iteration 673: loss 0.4289012849330902
iteration 674: loss 0.4284396767616272
iteration 675: loss 0.42797955870628357
iteration 676: loss 0.4275212585926056
iteration 677: loss 0.4270646274089813
iteration 678: loss 0.4266095459461212
iteration 679: loss 0.426156222820282
iteration 680: loss 0.4257044494152069
iteration 681: loss 0.4252544343471527
iteration 682: loss 0.42480599880218506
iteration 683: loss 0.4243592917919159
iteration 684: loss 0.4239141047000885
iteration 685: loss 0.4234704077243805
iteration 686: loss 0.4230284094810486
iteration 687: loss 0.4225880205631256
iteration 688: loss 0.4221492409706116
iteration 689: loss 0.42171186208724976
iteration 690: loss 0.4212762117385864
iteration 691: loss 0.4208419919013977
iteration 692: loss 0.4204094409942627
iteration 693: loss 0.41997841000556946
iteration 694: loss 0.41954895853996277
iteration 695: loss 0.4191209375858307
iteration 696: loss 0.418694406747818
iteration 697: loss 0.41826945543289185
iteration 698: loss 0.41784587502479553
iteration 699: loss 0.4174240529537201
iteration 700: loss 0.4170035719871521
iteration 701: loss 0.41658446192741394
iteration 702: loss 0.4161669909954071
iteration 703: loss 0.41575101017951965
iteration 704: loss 0.4153362810611725
iteration 705: loss 0.4149231016635895
iteration 706: loss 0.41451141238212585
iteration 707: loss 0.4141010344028473
iteration 708: loss 0.41369226574897766
iteration 709: loss 0.4132848381996155
iteration 710: loss 0.41287869215011597
iteration 711: loss 0.4124741554260254
iteration 712: loss 0.4120709002017975
iteration 713: loss 0.4116691052913666
iteration 714: loss 0.4112686216831207
iteration 715: loss 0.41086962819099426
iteration 716: loss 0.41047194600105286
iteration 717: loss 0.4100756049156189
iteration 718: loss 0.40968063473701477
iteration 719: loss 0.4092869162559509
iteration 720: loss 0.40889468789100647
iteration 721: loss 0.4085037410259247
iteration 722: loss 0.40811416506767273
iteration 723: loss 0.40772584080696106
iteration 724: loss 0.40733879804611206
iteration 725: loss 0.4069531261920929
iteration 726: loss 0.4065687954425812
iteration 727: loss 0.4061858057975769
iteration 728: loss 0.4058041274547577
iteration 729: loss 0.4054235816001892
iteration 730: loss 0.4050443768501282
iteration 731: loss 0.4046664535999298
iteration 732: loss 0.40428978204727173
iteration 733: loss 0.4039144515991211
iteration 734: loss 0.4035402238368988
iteration 735: loss 0.4031673073768616
iteration 736: loss 0.4027957022190094
iteration 737: loss 0.4024253189563751
iteration 738: loss 0.4020562171936035
iteration 739: loss 0.40168821811676025
iteration 740: loss 0.40132153034210205
iteration 741: loss 0.40095600485801697
iteration 742: loss 0.40059173107147217
iteration 743: loss 0.40022870898246765
iteration 744: loss 0.39986687898635864
iteration 745: loss 0.3995061218738556
iteration 746: loss 0.39914652705192566
iteration 747: loss 0.3987882733345032
iteration 748: loss 0.39843109250068665
iteration 749: loss 0.39807504415512085
iteration 750: loss 0.3977202773094177
iteration 751: loss 0.39736664295196533
iteration 752: loss 0.39701399207115173
iteration 753: loss 0.3966626822948456
iteration 754: loss 0.396312415599823
iteration 755: loss 0.3959634602069855
iteration 756: loss 0.3956153094768524
iteration 757: loss 0.3952685296535492
iteration 758: loss 0.39492282271385193
iteration 759: loss 0.394578218460083
iteration 760: loss 0.3942347764968872
iteration 761: loss 0.39389246702194214
iteration 762: loss 0.39355114102363586
iteration 763: loss 0.39321109652519226
iteration 764: loss 0.39287200570106506
iteration 765: loss 0.392534077167511
iteration 766: loss 0.3921970725059509
iteration 767: loss 0.39186131954193115
iteration 768: loss 0.3915265202522278
iteration 769: loss 0.39119282364845276
iteration 770: loss 0.3908601701259613
iteration 771: loss 0.3905285894870758
iteration 772: loss 0.3901980221271515
iteration 773: loss 0.38986870646476746
iteration 774: loss 0.3895402252674103
iteration 775: loss 0.3892126977443695
iteration 776: loss 0.38888636231422424
iteration 777: loss 0.3885610103607178
iteration 778: loss 0.3882366120815277
iteration 779: loss 0.3879133462905884
iteration 780: loss 0.3875909149646759
iteration 781: loss 0.38726964592933655
iteration 782: loss 0.3869493305683136
iteration 783: loss 0.38662993907928467
iteration 784: loss 0.3863115906715393
iteration 785: loss 0.38599419593811035
iteration 786: loss 0.38567790389060974
iteration 787: loss 0.38536256551742554
iteration 788: loss 0.3850480020046234
iteration 789: loss 0.38473445177078247
iteration 790: loss 0.38442209362983704
iteration 791: loss 0.38411054015159607
iteration 792: loss 0.3837999403476715
iteration 793: loss 0.38349032402038574
iteration 794: loss 0.38318169116973877
iteration 795: loss 0.3828738331794739
iteration 796: loss 0.38256698846817017
iteration 797: loss 0.38226115703582764
iteration 798: loss 0.38195621967315674
iteration 799: loss 0.38165226578712463
iteration 800: loss 0.38134920597076416
iteration 801: loss 0.3810470700263977
iteration 802: loss 0.3807457387447357
iteration 803: loss 0.3804454207420349
iteration 804: loss 0.38014599680900574
iteration 805: loss 0.3798474669456482
iteration 806: loss 0.3795498311519623
iteration 807: loss 0.3792530298233032
iteration 808: loss 0.37895721197128296
iteration 809: loss 0.37866222858428955
iteration 810: loss 0.37836822867393494
iteration 811: loss 0.37807485461235046
iteration 812: loss 0.37778252363204956
iteration 813: loss 0.3774910271167755
iteration 814: loss 0.3772003650665283
iteration 815: loss 0.37691065669059753
iteration 816: loss 0.37662172317504883
iteration 817: loss 0.3763335645198822
iteration 818: loss 0.37604638934135437
iteration 819: loss 0.37575992941856384
iteration 820: loss 0.37547436356544495
iteration 821: loss 0.3751896619796753
iteration 822: loss 0.37490567564964294
iteration 823: loss 0.3746225833892822
iteration 824: loss 0.374340295791626
iteration 825: loss 0.3740588128566742
iteration 826: loss 0.37377816438674927
iteration 827: loss 0.37349826097488403
iteration 828: loss 0.37321922183036804
iteration 829: loss 0.3729410767555237
iteration 830: loss 0.3726634383201599
iteration 831: loss 0.3723869025707245
iteration 832: loss 0.3721109628677368
iteration 833: loss 0.371835857629776
iteration 834: loss 0.3715614974498749
iteration 835: loss 0.37128791213035583
iteration 836: loss 0.37101519107818604
iteration 837: loss 0.3707432150840759
iteration 838: loss 0.37047192454338074
iteration 839: loss 0.37020158767700195
iteration 840: loss 0.36993175745010376
iteration 841: loss 0.3696628510951996
iteration 842: loss 0.3693947494029999
iteration 843: loss 0.3691273331642151
iteration 844: loss 0.3688606917858124
iteration 845: loss 0.36859479546546936
iteration 846: loss 0.36832961440086365
iteration 847: loss 0.36806520819664
iteration 848: loss 0.36780136823654175
iteration 849: loss 0.3675383925437927
iteration 850: loss 0.36727604269981384
iteration 851: loss 0.36701449751853943
iteration 852: loss 0.36675360798835754
iteration 853: loss 0.3664933741092682
iteration 854: loss 0.3662340044975281
iteration 855: loss 0.36597520112991333
iteration 856: loss 0.3657170832157135
iteration 857: loss 0.36545971035957336
iteration 858: loss 0.365202933549881
iteration 859: loss 0.3649469316005707
iteration 860: loss 0.3646915555000305
iteration 861: loss 0.36443695425987244
iteration 862: loss 0.3641830086708069
iteration 863: loss 0.36392974853515625
iteration 864: loss 0.363677054643631
iteration 865: loss 0.3634253442287445
iteration 866: loss 0.36317408084869385
iteration 867: loss 0.36292362213134766
iteration 868: loss 0.3626737594604492
iteration 869: loss 0.3624246418476105
iteration 870: loss 0.3621761202812195
iteration 871: loss 0.3619283437728882
iteration 872: loss 0.3616812229156494
iteration 873: loss 0.3614347279071808
iteration 874: loss 0.3611888587474823
iteration 875: loss 0.36094385385513306
iteration 876: loss 0.36069929599761963
iteration 877: loss 0.3604555130004883
iteration 878: loss 0.3602122962474823
iteration 879: loss 0.3599697947502136
iteration 880: loss 0.35972779989242554
iteration 881: loss 0.35948655009269714
iteration 882: loss 0.35924598574638367
iteration 883: loss 0.35900598764419556
iteration 884: loss 0.3587666153907776
iteration 885: loss 0.35852789878845215
iteration 886: loss 0.3582897186279297
iteration 887: loss 0.3580522835254669
iteration 888: loss 0.35781529545783997
iteration 889: loss 0.3575790226459503
iteration 890: loss 0.3573433458805084
iteration 891: loss 0.35710832476615906
iteration 892: loss 0.3568739891052246
iteration 893: loss 0.356640100479126
iteration 894: loss 0.35640689730644226
iteration 895: loss 0.3561742901802063
iteration 896: loss 0.3559422492980957
iteration 897: loss 0.3557107746601105
iteration 898: loss 0.3554799556732178
iteration 899: loss 0.35524970293045044
iteration 900: loss 0.35502001643180847
iteration 901: loss 0.35479092597961426
iteration 902: loss 0.3545624315738678
iteration 903: loss 0.35433444380760193
iteration 904: loss 0.3541070818901062
iteration 905: loss 0.353880375623703
iteration 906: loss 0.3536541163921356
iteration 907: loss 0.3534284830093384
iteration 908: loss 0.3532034754753113
iteration 909: loss 0.35297891497612
iteration 910: loss 0.35275501012802124
iteration 911: loss 0.35253167152404785
iteration 912: loss 0.3523087799549103
iteration 913: loss 0.3520865738391876
iteration 914: loss 0.35186487436294556
iteration 915: loss 0.35164377093315125
iteration 916: loss 0.35142311453819275
iteration 917: loss 0.35120293498039246
iteration 918: loss 0.35098350048065186
iteration 919: loss 0.3507644832134247
iteration 920: loss 0.35054606199264526
iteration 921: loss 0.3503281772136688
iteration 922: loss 0.35011082887649536
iteration 923: loss 0.34989410638809204
iteration 924: loss 0.34967783093452454
iteration 925: loss 0.34946209192276
iteration 926: loss 0.34924688935279846
iteration 927: loss 0.34903231263160706
iteration 928: loss 0.3488181531429291
iteration 929: loss 0.34860455989837646
iteration 930: loss 0.34839147329330444
iteration 931: loss 0.348178893327713
iteration 932: loss 0.3479668200016022
iteration 933: loss 0.3477552831172943
iteration 934: loss 0.34754425287246704
iteration 935: loss 0.34733375906944275
iteration 936: loss 0.3471237123012543
iteration 937: loss 0.3469141721725464
iteration 938: loss 0.34670519828796387
iteration 939: loss 0.3464967608451843
iteration 940: loss 0.34628868103027344
iteration 941: loss 0.34608110785484314
iteration 942: loss 0.3458741307258606
iteration 943: loss 0.34566760063171387
iteration 944: loss 0.3454616069793701
iteration 945: loss 0.34525609016418457
iteration 946: loss 0.34505099058151245
iteration 947: loss 0.3448465168476105
iteration 948: loss 0.34464240074157715
iteration 949: loss 0.3444388210773468
iteration 950: loss 0.34423568844795227
iteration 951: loss 0.34403306245803833
iteration 952: loss 0.3438308537006378
iteration 953: loss 0.3436291515827179
iteration 954: loss 0.3434278964996338
iteration 955: loss 0.3432271182537079
iteration 956: loss 0.3430267870426178
iteration 957: loss 0.3428269326686859
iteration 958: loss 0.34262749552726746
iteration 959: loss 0.34242862462997437
iteration 960: loss 0.3422302007675171
iteration 961: loss 0.34203219413757324
iteration 962: loss 0.3418346643447876
iteration 963: loss 0.34163764119148254
iteration 964: loss 0.341440886259079
iteration 965: loss 0.34124475717544556
iteration 966: loss 0.34104910492897034
iteration 967: loss 0.34085381031036377
iteration 968: loss 0.340658962726593
iteration 969: loss 0.3404644727706909
iteration 970: loss 0.3402705788612366
iteration 971: loss 0.34007710218429565
iteration 972: loss 0.33988404273986816
iteration 973: loss 0.3396914005279541
iteration 974: loss 0.3394991457462311
iteration 975: loss 0.3393072783946991
iteration 976: loss 0.3391159176826477
iteration 977: loss 0.3389250338077545
iteration 978: loss 0.33873450756073
iteration 979: loss 0.3385445177555084
iteration 980: loss 0.33835482597351074
iteration 981: loss 0.33816567063331604
iteration 982: loss 0.3379768133163452
iteration 983: loss 0.3377884030342102
iteration 984: loss 0.337600439786911
iteration 985: loss 0.33741289377212524
iteration 986: loss 0.33722570538520813
iteration 987: loss 0.33703890442848206
iteration 988: loss 0.33685240149497986
iteration 989: loss 0.3366665244102478
iteration 990: loss 0.336480975151062
iteration 991: loss 0.33629584312438965
iteration 992: loss 0.33611103892326355
iteration 993: loss 0.33592674136161804
iteration 994: loss 0.3357427418231964
iteration 995: loss 0.33555907011032104
iteration 996: loss 0.33537593483924866
iteration 997: loss 0.33519312739372253
iteration 998: loss 0.33501073718070984
iteration 999: loss 0.3348287343978882
Accuracy:  0.9148 
Recall:  [(0.9734693877551021, 0.9362119725220804), (0.9814977973568282, 0.9611734253666955), (0.8788759689922481, 0.9264555669050051), (0.903960396039604, 0.8968565815324165), (0.9256619144602851, 0.9053784860557769), (0.8598654708520179, 0.9120095124851367), (0.94258872651357, 0.9280575539568345), (0.8988326848249028, 0.9221556886227545), (0.8829568788501027, 0.8669354838709677), (0.8889990089197225, 0.8854886475814413)] 
Matrix:
 [[ 954    0    3    2    0    5   11    2    3    0]
 [   0 1114    2    3    0    2    4    1    9    0]
 [  13    6  907   20   14    0   14   11   43    4]
 [   2    2   18  913    1   23    3   16   26    6]
 [   2    3    4    1  909    1   14    1    6   41]
 [  10    3    3   36   10  767   14    6   36    7]
 [  15    3    6    0   12   14  903    2    3    0]
 [   3   15   24    6   10    0    0  924    1   45]
 [   7    7    8   25    9   23   10   12  860   13]
 [  13    6    4   12   39    6    0   27    5  897]]
Accuracy:  0.0783 
Recall:  [(0.007142857142857143, 0.01511879049676026), (0.01145374449339207, 0.16455696202531644), (0.003875968992248062, 0.07692307692307693), (0.015841584158415842, 0.009211283822682787), (0.13034623217922606, 0.084155161078238), (0.011210762331838564, 0.06666666666666667), (0.11377870563674322, 0.06770186335403727), (0.0, 0.0), (0.5092402464065708, 0.11316449920146018), (0.0, 0.0)] 
Matrix:
 [[  7   6   2  15 102   4 528   1 314   1]
 [  2  13   8 327  65   0  22   0 698   0]
 [ 28  19   4 131 456   9  46   0 339   0]
 [ 44   5   2  16 247   8 208   0 478   2]
 [ 56   4   1 281 128  59 169   0 284   0]
 [ 36  15  19  23 110  10 164   0 514   1]
 [ 15  16   1 354 114   9 109   0 340   0]
 [ 85   1   3 169 125   1 143   0 501   0]
 [ 22   0   4 144 132  42 134   0 496   0]
 [168   0   8 277  42   8  87   0 419   0]]
Accuracy:  0.9404 
Recall:  [(0.976530612244898, 0.9522388059701492), (0.9885462555066079, 0.9697493517718236), (0.937015503875969, 0.9253588516746412), (0.9376237623762376, 0.9062200956937799), (0.9592668024439919, 0.9363817097415507), (0.9002242152466368, 0.9145785876993167), (0.9498956158663883, 0.9568874868559412), (0.9309338521400778, 0.9522388059701492), (0.9004106776180698, 0.936965811965812), (0.9137760158572844, 0.948559670781893)] 
Matrix:
 [[ 957    0    4    1    1    6    9    1    0    1]
 [   0 1122    3    2    0    1    2    1    4    0]
 [   8    6  967   11    3    3    7    8   17    2]
 [   4    3   16  947    1   16    0    9   12    2]
 [   1    1   10    1  942    2    4    2    3   16]
 [  10    4    3   36    6  803   13    1   14    2]
 [   9    2   13    1    5   16  910    1    1    0]
 [   1    8   21   10    8    1    0  957    3   19]
 [   8    4    6   25    7   26    6    7  877    8]
 [   7    7    2   11   33    4    0   18    5  922]]
Accuracy:  0.9446 
Recall:  [(0.986734693877551, 0.9593253968253969), (0.9876651982378855, 0.968048359240069), (0.9312015503875969, 0.937560975609756), (0.9415841584158415, 0.9287109375), (0.955193482688391, 0.9305555555555556), (0.9080717488789237, 0.9299655568312285), (0.9665970772442589, 0.9468302658486708), (0.9299610894941635, 0.956), (0.9158110882956879, 0.9409282700421941), (0.9157581764122894, 0.9428571428571428)] 
Matrix:
 [[ 967    0    2    0    0    5    4    1    1    0]
 [   0 1121    2    2    0    1    4    1    4    0]
 [   9    1  961    9   10    1   13    9   17    2]
 [   1    1   15  951    1   15    1   10   11    4]
 [   1    1    7    0  938    0    7    2    2   24]
 [   7    4    5   30    7  810   12    2   10    5]
 [   9    3    4    1    5    9  926    0    1    0]
 [   2   13   22    5    8    1    0  956    3   18]
 [   4    7    7   14    8   23   10    6  892    3]
 [   8    7    0   12   31    6    1   13    7  924]]
